{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding"
      ],
      "metadata": {
        "id": "ceW9vtP8qeKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xw9AoF6zHrD2",
        "outputId": "f5067150-b526-47eb-aaa8-42995249ea86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.9/dist-packages (4.3.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.9/dist-packages (from gensim) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.9/dist-packages (from gensim) (1.10.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.9/dist-packages (from gensim) (6.3.0)\n"
          ]
        }
      ],
      "source": [
        "pip install -U gensim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "7IQCuTf-g2FB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Import das Bibliotecas\n",
        "import warnings\n",
        "warnings.simplefilter('ignore')\n",
        "import pandas as pd\n",
        "import ast\n",
        "import json\n",
        "import nltk\n",
        "import numpy as np\n",
        "import gensim\n",
        "from pprint import pprint\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "import seaborn as sns\n",
        "from string import punctuation\n",
        "from nltk import word_tokenize\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "import gensim.corpora as corpora\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.probability import FreqDist\n",
        "from gensim.models import CoherenceModel\n",
        "from sklearn.decomposition import TruncatedSVD"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHjOM2FJH-nu",
        "outputId": "e51afda3-01e9-40ac-ea6b-284626a95876"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "!python -m spacy download pt_core_news_lg"
      ],
      "metadata": {
        "id": "lmJqU6BwaHjg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55f3858e-9284-4ab6-f176-87c77f7e33fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-04-19 12:38:33.843216: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pt-core-news-lg==3.5.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_lg-3.5.0/pt_core_news_lg-3.5.0-py3-none-any.whl (568.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m568.2/568.2 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.9/dist-packages (from pt-core-news-lg==3.5.0) (3.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (2.0.8)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (1.1.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (3.0.12)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (67.6.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (4.65.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (0.10.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (23.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (1.10.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (3.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (2.4.6)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (0.7.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (6.3.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (2.0.7)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (2.27.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (3.3.0)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (8.1.9)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (1.22.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (3.0.8)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.9/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.9/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (2.1.2)\n",
            "Installing collected packages: pt-core-news-lg\n",
            "Successfully installed pt-core-news-lg-3.5.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('pt_core_news_lg')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "S3F6ghBNbDWX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3a9fe50-f9d7-43a7-d520-8b38dabba989"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.corpora as corpora"
      ],
      "metadata": {
        "id": "LQ_GlqSobJ0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import LsiModel\n",
        "from gensim.models.coherencemodel import CoherenceModel"
      ],
      "metadata": {
        "id": "Qmx_bWlFbRXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Leitura do novo Dataset após tratamento inicial\n",
        "url = 'https://raw.githubusercontent.com/leonardoemerson/Fake-Dataset/main/dataset/treated_dataset3.csv'\n",
        "df = pd.read_csv(url, encoding = 'utf8', delimiter = ';', header = 0)"
      ],
      "metadata": {
        "id": "xYsTrsrfIFM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Eliminando coluna 'Unnamed: 0'\n",
        "df = df.drop(\"Unnamed: 0\",axis=1)"
      ],
      "metadata": {
        "id": "ZekUEYyuIFQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIL3kp3VIFUv",
        "outputId": "537a861b-ca5c-4318-cb9d-137a796001df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5201, 16)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualização inicial do Dataset\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 635
        },
        "id": "j6Xq_wlBIFYm",
        "outputId": "0210fddb-8cb5-4951-cf64-9444f58e1200"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               title  \\\n",
              "0  Pastor Sérgio Von Helder, que chutou santa, nã...   \n",
              "1  Lulinha, filho de Lula, comprou avião Gulfstre...   \n",
              "2  Lulinha não comprou Fazenda Fortaleza por 47 m...   \n",
              "3        Fanta Uva causa câncer e problema nos rins    \n",
              "4  Simpsons terá episódio sobre manifestações no ...   \n",
              "\n",
              "                                          title_norm  \\\n",
              "0  pastor sergio von helder, que chutou santa, na...   \n",
              "1  lulinha, filho de lula, comprou aviao gulfstre...   \n",
              "2  lulinha nao comprou fazenda fortaleza por 47 m...   \n",
              "3        fanta uva causa cancer e problema nos rins    \n",
              "4  simpsons tera episodio sobre manifestacoes no ...   \n",
              "\n",
              "                                             message  \\\n",
              "0                                                NaN   \n",
              "1                                                NaN   \n",
              "2  O Novo Mega Campeão do Brasil de enriqueciment...   \n",
              "3  ALERTA GERAL ATENÇÃO” FANTA UVA!!!! A propagan...   \n",
              "4                                                NaN   \n",
              "\n",
              "                                        message_norm  \\\n",
              "0                                                NaN   \n",
              "1                                                NaN   \n",
              "2  o novo mega campeao do brasil de enriqueciment...   \n",
              "3  alerta geral atencao fanta uva!!!!a propaganda...   \n",
              "4                                                NaN   \n",
              "\n",
              "                                              tokens  \\\n",
              "0  ['pastor', 'sergio', 'von', 'helder', 'que', '...   \n",
              "1  ['lulinha', 'filho', 'de', 'lula', 'comprou', ...   \n",
              "2  ['lulinha', 'nao', 'comprou', 'fazenda', 'fort...   \n",
              "3  ['fanta', 'uva', 'causa', 'cancer', 'e', 'prob...   \n",
              "4  ['simpsons', 'tera', 'episodio', 'sobre', 'man...   \n",
              "\n",
              "                                            features  \\\n",
              "0  {'len': 12, 1: {'token': 'pastor', 'lemma': 'p...   \n",
              "1  {'len': 14, 1: {'token': 'lulinha', 'lemma': '...   \n",
              "2  {'len': 8, 1: {'token': 'lulinha', 'lemma': 'l...   \n",
              "3  {'len': 8, 1: {'token': 'fanta', 'lemma': 'fan...   \n",
              "4  {'len': 7, 1: {'token': 'simpsons', 'lemma': '...   \n",
              "\n",
              "                     entities  type      source  \\\n",
              "0  [Pastor Sérgio Von Helder]  fake  boatos.org   \n",
              "1          [Lula, Gulfstream]  fake  boatos.org   \n",
              "2          [Lulinha, Fazenda]  fake  boatos.org   \n",
              "3                 [Fanta Uva]  fake  boatos.org   \n",
              "4          [Simpsons, Brasil]  fake  boatos.org   \n",
              "\n",
              "                                          url_review  \\\n",
              "0  https://www.boatos.org/religiao/pastor-sergio-...   \n",
              "1  https://www.boatos.org/politica/lulinha-aviao-...   \n",
              "2  https://www.boatos.org/politica/lulinha-fazend...   \n",
              "3  https://www.boatos.org/saude/fanta-uva-da-canc...   \n",
              "4  https://www.boatos.org/entretenimento/simpsons...   \n",
              "\n",
              "                              message_norm_treatment  \\\n",
              "0                                                NaN   \n",
              "1                                                NaN   \n",
              "2  o novo mega campeao do brasil de enriqueciment...   \n",
              "3  alerta geral atencao fanta uva!!!!a propaganda...   \n",
              "4                                                NaN   \n",
              "\n",
              "                    datetime  \\\n",
              "0  2013-08-07T19:27:23-03:00   \n",
              "1  2013-08-02T19:56:26-03:00   \n",
              "2  2013-07-29T14:13:21-03:00   \n",
              "3  2013-07-22T04:44:40-03:00   \n",
              "4  2013-07-08T17:37:40-03:00   \n",
              "\n",
              "                          message_norm_treatment_ssw  \\\n",
              "0                                                 []   \n",
              "1                                                 []   \n",
              "2  ['novo', 'mega', 'campeao', 'brasil', 'enrique...   \n",
              "3  ['alerta', 'geral', 'atencao', 'fanta', 'uva',...   \n",
              "4                                                 []   \n",
              "\n",
              "                         message_norm_treatment_ssw2  \\\n",
              "0                                                 []   \n",
              "1                                                 []   \n",
              "2  ['novo', 'mega', 'campeao', 'brasil', 'enrique...   \n",
              "3  ['alerta', 'geral', 'atencao', 'fanta', 'propa...   \n",
              "4                                                 []   \n",
              "\n",
              "                                           features2        category  \n",
              "0  {'len': 12, 1: {'token': 'pastor', 'lemma': 'p...        Religião  \n",
              "1  {'len': 14, 1: {'token': 'lulinha', 'lemma': '...        Política  \n",
              "2  {'len': 8, 1: {'token': 'lulinha', 'lemma': 'l...        Política  \n",
              "3  {'len': 8, 1: {'token': 'fanta', 'lemma': 'fan...           Saúde  \n",
              "4  {'len': 7, 1: {'token': 'simpsons', 'lemma': '...  Entretenimento  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-04be81fc-bd06-4b92-ab1c-2e32d8ae1184\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>title_norm</th>\n",
              "      <th>message</th>\n",
              "      <th>message_norm</th>\n",
              "      <th>tokens</th>\n",
              "      <th>features</th>\n",
              "      <th>entities</th>\n",
              "      <th>type</th>\n",
              "      <th>source</th>\n",
              "      <th>url_review</th>\n",
              "      <th>message_norm_treatment</th>\n",
              "      <th>datetime</th>\n",
              "      <th>message_norm_treatment_ssw</th>\n",
              "      <th>message_norm_treatment_ssw2</th>\n",
              "      <th>features2</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Pastor Sérgio Von Helder, que chutou santa, nã...</td>\n",
              "      <td>pastor sergio von helder, que chutou santa, na...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>['pastor', 'sergio', 'von', 'helder', 'que', '...</td>\n",
              "      <td>{'len': 12, 1: {'token': 'pastor', 'lemma': 'p...</td>\n",
              "      <td>[Pastor Sérgio Von Helder]</td>\n",
              "      <td>fake</td>\n",
              "      <td>boatos.org</td>\n",
              "      <td>https://www.boatos.org/religiao/pastor-sergio-...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2013-08-07T19:27:23-03:00</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>{'len': 12, 1: {'token': 'pastor', 'lemma': 'p...</td>\n",
              "      <td>Religião</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Lulinha, filho de Lula, comprou avião Gulfstre...</td>\n",
              "      <td>lulinha, filho de lula, comprou aviao gulfstre...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>['lulinha', 'filho', 'de', 'lula', 'comprou', ...</td>\n",
              "      <td>{'len': 14, 1: {'token': 'lulinha', 'lemma': '...</td>\n",
              "      <td>[Lula, Gulfstream]</td>\n",
              "      <td>fake</td>\n",
              "      <td>boatos.org</td>\n",
              "      <td>https://www.boatos.org/politica/lulinha-aviao-...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2013-08-02T19:56:26-03:00</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>{'len': 14, 1: {'token': 'lulinha', 'lemma': '...</td>\n",
              "      <td>Política</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Lulinha não comprou Fazenda Fortaleza por 47 m...</td>\n",
              "      <td>lulinha nao comprou fazenda fortaleza por 47 m...</td>\n",
              "      <td>O Novo Mega Campeão do Brasil de enriqueciment...</td>\n",
              "      <td>o novo mega campeao do brasil de enriqueciment...</td>\n",
              "      <td>['lulinha', 'nao', 'comprou', 'fazenda', 'fort...</td>\n",
              "      <td>{'len': 8, 1: {'token': 'lulinha', 'lemma': 'l...</td>\n",
              "      <td>[Lulinha, Fazenda]</td>\n",
              "      <td>fake</td>\n",
              "      <td>boatos.org</td>\n",
              "      <td>https://www.boatos.org/politica/lulinha-fazend...</td>\n",
              "      <td>o novo mega campeao do brasil de enriqueciment...</td>\n",
              "      <td>2013-07-29T14:13:21-03:00</td>\n",
              "      <td>['novo', 'mega', 'campeao', 'brasil', 'enrique...</td>\n",
              "      <td>['novo', 'mega', 'campeao', 'brasil', 'enrique...</td>\n",
              "      <td>{'len': 8, 1: {'token': 'lulinha', 'lemma': 'l...</td>\n",
              "      <td>Política</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Fanta Uva causa câncer e problema nos rins</td>\n",
              "      <td>fanta uva causa cancer e problema nos rins</td>\n",
              "      <td>ALERTA GERAL ATENÇÃO” FANTA UVA!!!! A propagan...</td>\n",
              "      <td>alerta geral atencao fanta uva!!!!a propaganda...</td>\n",
              "      <td>['fanta', 'uva', 'causa', 'cancer', 'e', 'prob...</td>\n",
              "      <td>{'len': 8, 1: {'token': 'fanta', 'lemma': 'fan...</td>\n",
              "      <td>[Fanta Uva]</td>\n",
              "      <td>fake</td>\n",
              "      <td>boatos.org</td>\n",
              "      <td>https://www.boatos.org/saude/fanta-uva-da-canc...</td>\n",
              "      <td>alerta geral atencao fanta uva!!!!a propaganda...</td>\n",
              "      <td>2013-07-22T04:44:40-03:00</td>\n",
              "      <td>['alerta', 'geral', 'atencao', 'fanta', 'uva',...</td>\n",
              "      <td>['alerta', 'geral', 'atencao', 'fanta', 'propa...</td>\n",
              "      <td>{'len': 8, 1: {'token': 'fanta', 'lemma': 'fan...</td>\n",
              "      <td>Saúde</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Simpsons terá episódio sobre manifestações no ...</td>\n",
              "      <td>simpsons tera episodio sobre manifestacoes no ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>['simpsons', 'tera', 'episodio', 'sobre', 'man...</td>\n",
              "      <td>{'len': 7, 1: {'token': 'simpsons', 'lemma': '...</td>\n",
              "      <td>[Simpsons, Brasil]</td>\n",
              "      <td>fake</td>\n",
              "      <td>boatos.org</td>\n",
              "      <td>https://www.boatos.org/entretenimento/simpsons...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2013-07-08T17:37:40-03:00</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>{'len': 7, 1: {'token': 'simpsons', 'lemma': '...</td>\n",
              "      <td>Entretenimento</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-04be81fc-bd06-4b92-ab1c-2e32d8ae1184')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-04be81fc-bd06-4b92-ab1c-2e32d8ae1184 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-04be81fc-bd06-4b92-ab1c-2e32d8ae1184');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Ordenando dataset de acordo com a coluna 'datetime'\n",
        "df = df.sort_values(by='datetime')"
      ],
      "metadata": {
        "id": "cauOgWspIFbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Resetando os índices do dataset\n",
        "df = df.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "2LfER8_AOjN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###COLOCANDO TÍTULO COMO TEXTO PARA NOTÍCIAS SEM TEXTO"
      ],
      "metadata": {
        "id": "cYFGxNkLXFJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import urlopen\n",
        "advanced_sw_list = []\n",
        "file = open(\"filename\",\"w\", encoding='utf-8')\n",
        "url = urlopen(\"https://gist.githubusercontent.com/alopes/5358189/raw/2107d809cca6b83ce3d8e04dbd9463283025284f/stopwords.txt\")\n",
        "for line in url:\n",
        "    file.write(str(line) +'\\n')\n",
        "    advanced_sw_list.append(line)\n",
        "file.close()"
      ],
      "metadata": {
        "id": "RiSf5TshXKeE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Removendo espaços e \\n dos elementos da lista avançada de strings(stopwords)\n",
        "advanced_sw_list1 = list(map(lambda x:x.strip(),advanced_sw_list))"
      ],
      "metadata": {
        "id": "tsgGtzO2XUoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Decode para remover o b frente das strings(stopwords) \n",
        "for i in range(0, len(advanced_sw_list1)):\n",
        "    advanced_sw_list1[i] = advanced_sw_list1[i].decode('utf-8')"
      ],
      "metadata": {
        "id": "RN-pO6pcXYuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unidecode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Si0Q2Ro8XaM1",
        "outputId": "8e3b5db6-a29b-4634-b3df-cdbfef8ecf50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.6-py3-none-any.whl (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.9/235.9 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.3.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Removendo acentuação das strings(stopwords)\n",
        "import unidecode \n",
        "for i in range(0, len(advanced_sw_list1)):\n",
        "    advanced_sw_list1[i] = unidecode.unidecode(advanced_sw_list1[i])"
      ],
      "metadata": {
        "id": "2kDeE7tmXcBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Adicionando novas stopwords manualmente\n",
        "advanced_sw_list1.append('ai')\n",
        "advanced_sw_list1.append('sobre')\n",
        "advanced_sw_list1.append('r')\n",
        "advanced_sw_list1.append('pra')\n",
        "advanced_sw_list1.append('entao')\n",
        "advanced_sw_list1.append('ta')\n",
        "advanced_sw_list1.append('porque')\n",
        "advanced_sw_list1.append('assim')\n",
        "advanced_sw_list1.append('onde')\n",
        "advanced_sw_list1.append('aqui')\n",
        "advanced_sw_list1.append('agora')\n",
        "advanced_sw_list1.append('vai')\n",
        "advanced_sw_list1.append('ainda')\n",
        "advanced_sw_list1.append('vamos')\n",
        "advanced_sw_list1.append('diz')\n",
        "advanced_sw_list1.append('pode')\n",
        "advanced_sw_list1.append('sendo')\n",
        "advanced_sw_list1.append('entao')\n",
        "advanced_sw_list1.append('aqui')\n",
        "advanced_sw_list1.append('1')"
      ],
      "metadata": {
        "id": "rAKfQH0fXeil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Mais stopwords\n",
        "mais_stopwords = \"a, agora, ainda, alguém, algum, alguma, algumas, alguns, ampla, amplas, amplo, amplos, ante, antes, ao, aos, após, aquela, aquelas, aquele, aqueles, aquilo, as, até, através, cada, coisa, coisas, com, como, contra, contudo, da, daquele, daqueles, das, de, dela, delas, dele, deles, depois, dessa, dessas, desse, desses, desta, destas, deste, deste, destes, deve, devem, devendo, dever, deverá, deverão, deveria, deveriam, devia, deviam, disse, disso, disto, dito, diz, dizem, do, dos, e, é, ela, elas, ele, eles, em, enquanto, entre, era, essa, essas, esse, esses, esta, está, estamos, estão, estas, estava, estavam, estávamos, este, estes, estou, eu, fazendo, fazer, feita, feitas, feito, feitos, foi, for, foram, fosse, fossem, grande, grandes, há, isso, isto, já, la, lá, lhe, lhes, lo, mas, me, mesma, mesmas, mesmo, mesmos, meu, meus, minha, minhas, muita, muitas, muito, muitos, na, não, nas, nem, nenhum, nessa, nessas, nesta, nestas, ninguém, no, nos, nós, nossa, nossas, nosso, nossos, num, numa, nunca, o, os, ou, outra, outras, outro, outros, para, pela, pelas, pelo, pelos, pequena, pequenas, pequeno, pequenos, per, perante, pode, pude, podendo, poder, poderia, poderiam, podia, podiam, pois, por, porém, porque, posso, pouca, poucas, pouco, poucos, primeiro, primeiros, própria, próprias, próprio, próprios, quais, qual, quando, quanto, quantos, que, quem, são, se, seja, sejam, sem, sempre, sendo, será, serão, seu, seus, si, sido, só, sob, sobre, sua, suas, talvez, também, tampouco, te, tem, tendo, tenha, ter, teu, teus, ti, tido, tinha, tinham, toda, todas, todavia, todo, todos, tu, tua, tuas, tudo, última, últimas, último, últimos, um, uma, umas, uns, vendo, ver, vez, vindo, vir, vos, vós\"\n",
        "mais_stopwords1 = [ 'a', 'à', 'adeus', 'agora', 'aí', 'ainda', 'além', 'algo', 'alguém', 'algum', 'alguma', 'algumas', 'alguns', 'ali', 'ampla', 'amplas', 'amplo', 'amplos', 'ano', 'anos', 'ante', 'antes', 'ao', 'aos', 'apenas', 'apoio', 'após', 'aquela', 'aquelas', 'aquele', 'aqueles', 'aqui', 'aquilo', 'área', 'as', 'às', 'assim', 'até', 'atrás', 'através', 'baixo', 'bastante', 'bem', 'boa', 'boas', 'bom', 'bons', 'breve', 'cá', 'cada', 'catorze', 'cedo', 'cento', 'certamente', 'certeza', 'cima', 'cinco', 'coisa', 'coisas', 'com', 'como', 'conselho', 'contra', 'contudo', 'custa', 'da', 'dá', 'dão', 'daquela', 'daquelas', 'daquele', 'daqueles', 'dar', 'das', 'de', 'debaixo', 'dela', 'delas', 'dele', 'deles', 'demais', 'dentro', 'depois', 'desde', 'dessa', 'dessas', 'desse', 'desses', 'desta', 'destas', 'deste', 'destes', 'deve', 'devem', 'devendo', 'dever', 'deverá', 'deverão', 'deveria', 'deveriam', 'devia', 'deviam', 'dez', 'dezanove', 'dezasseis', 'dezassete', 'dezoito', 'dia', 'diante', 'disse', 'disso', 'disto', 'dito', 'diz', 'dizem', 'dizer', 'do', 'dois', 'dos', 'doze', 'duas', 'dúvida', 'e', 'é', 'ela', 'elas', 'ele', 'eles', 'em', 'embora', 'enquanto', 'entre', 'era', 'eram', 'éramos', 'és', 'essa', 'essas', 'esse', 'esses', 'esta', 'está', 'estamos', 'estão', 'estar', 'estas', 'estás', 'estava', 'estavam', 'estávamos', 'este', 'esteja', 'estejam', 'estejamos', 'estes', 'esteve', 'estive', 'estivemos', 'estiver', 'estivera', 'estiveram', 'estivéramos', 'estiverem', 'estivermos', 'estivesse', 'estivessem', 'estivéssemos', 'estiveste', 'estivestes', 'estou', 'etc', 'eu', 'exemplo', 'faço', 'falta', 'favor', 'faz', 'fazeis', 'fazem', 'fazemos', 'fazendo', 'fazer', 'fazes', 'feita', 'feitas', 'feito', 'feitos', 'fez', 'fim', 'final', 'foi', 'fomos', 'for', 'fora', 'foram', 'fôramos', 'forem', 'forma', 'formos', 'fosse', 'fossem', 'fôssemos', 'foste', 'fostes', 'fui', 'geral', 'grande', 'grandes', 'grupo', 'há', 'haja', 'hajam', 'hajamos', 'hão', 'havemos', 'havia', 'hei', 'hoje', 'hora', 'horas', 'houve', 'houvemos', 'houver', 'houvera', 'houverá', 'houveram', 'houvéramos', 'houverão', 'houverei', 'houverem', 'houveremos', 'houveria', 'houveriam', 'houveríamos', 'houvermos', 'houvesse', 'houvessem', 'houvéssemos', 'isso', 'isto', 'já', 'la', 'lá', 'lado', 'lhe', 'lhes', 'lo', 'local', 'logo', 'longe', 'lugar', 'maior', 'maioria', 'mais', 'mal', 'mas', 'máximo', 'me', 'meio', 'menor', 'menos', 'mês', 'meses', 'mesma', 'mesmas', 'mesmo', 'mesmos', 'meu', 'meus', 'mil', 'minha', 'minhas', 'momento', 'muita', 'muitas', 'muito', 'muitos', 'na', 'nada', 'não', 'naquela', 'naquelas', 'naquele', 'naqueles', 'nas', 'nem', 'nenhum', 'nenhuma', 'nessa', 'nessas', 'nesse', 'nesses', 'nesta', 'nestas', 'neste', 'nestes', 'ninguém', 'nível', 'no', 'noite', 'nome', 'nos', 'nós', 'nossa', 'nossas', 'nosso', 'nossos', 'nova', 'novas', 'nove', 'novo', 'novos', 'num', 'numa', 'número', 'nunca', 'o', 'obra', 'obrigada', 'obrigado', 'oitava', 'oitavo', 'oito', 'onde', 'ontem', 'onze', 'os', 'ou', 'outra', 'outras', 'outro', 'outros', 'para', 'parece', 'parte', 'partir', 'paucas', 'pela', 'pelas', 'pelo', 'pelos', 'pequena', 'pequenas', 'pequeno', 'pequenos', 'per', 'perante', 'perto', 'pode', 'pude', 'pôde', 'podem', 'podendo', 'poder', 'poderia', 'poderiam', 'podia', 'podiam', 'põe', 'põem', 'pois', 'ponto', 'pontos', 'por', 'porém', 'porque', 'porquê', 'posição', 'possível', 'possivelmente', 'posso', 'pouca', 'poucas', 'pouco', 'poucos', 'primeira', 'primeiras', 'primeiro', 'primeiros', 'própria', 'próprias', 'próprio', 'próprios', 'próxima', 'próximas', 'próximo', 'próximos', 'pude', 'puderam', 'quais', 'quáis', 'qual', 'quando', 'quanto', 'quantos', 'quarta', 'quarto', 'quatro', 'que', 'quê', 'quem', 'quer', 'quereis', 'querem', 'queremas', 'queres', 'quero', 'questão', 'quinta', 'quinto', 'quinze', 'relação', 'sabe', 'sabem', 'são', 'se', 'segunda', 'segundo', 'sei', 'seis', 'seja', 'sejam', 'sejamos', 'sem', 'sempre', 'sendo', 'ser', 'será', 'serão', 'serei', 'seremos', 'seria', 'seriam', 'seríamos', 'sete', 'sétima', 'sétimo', 'seu', 'seus', 'sexta', 'sexto', 'si', 'sido', 'sim', 'sistema', 'só', 'sob', 'sobre', 'sois', 'somos', 'sou', 'sua', 'suas', 'tal', 'talvez', 'também', 'tampouco', 'tanta', 'tantas', 'tanto', 'tão', 'tarde', 'te', 'tem', 'tém', 'têm', 'temos', 'tendes', 'tendo', 'tenha', 'tenham', 'tenhamos', 'tenho', 'tens', 'ter', 'terá', 'terão', 'terceira', 'terceiro', 'terei', 'teremos', 'teria', 'teriam', 'teríamos', 'teu', 'teus', 'teve', 'ti', 'tido', 'tinha', 'tinham', 'tínhamos', 'tive', 'tivemos', 'tiver', 'tivera', 'tiveram', 'tivéramos', 'tiverem', 'tivermos', 'tivesse', 'tivessem', 'tivéssemos', 'tiveste', 'tivestes', 'toda', 'todas', 'todavia', 'todo', 'todos', 'trabalho', 'três', 'treze', 'tu', 'tua', 'tuas', 'tudo', 'última', 'últimas', 'último', 'últimos', 'um', 'uma', 'umas', 'uns', 'vai', 'vais', 'vão', 'vários', 'vem', 'vêm', 'vendo', 'vens', 'ver', 'vez', 'vezes', 'viagem', 'vindo', 'vinte', 'vir', 'você', 'vocês', 'vos', 'vós', 'vossa', 'vossas', 'vosso', 'vossos', 'zero', '1', '2', '3', '4', '5', '6', '7', '8', '9', '0', '_' ]\n",
        "mais_stopwords2 = str(mais_stopwords1).strip('[]')\n",
        "mais_stopwords3 = mais_stopwords + mais_stopwords2\n",
        "mais_stopwords3 = unidecode.unidecode(mais_stopwords3) \n",
        "lista_mais_stopwords = mais_stopwords3.split(', ')\n",
        "lista_final_stopwords = advanced_sw_list1 + lista_mais_stopwords"
      ],
      "metadata": {
        "id": "WhWfrbGgXgYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Definindo idioma para recuperação das stopwords\n",
        "stops = stopwords.words('portuguese')"
      ],
      "metadata": {
        "id": "sBrliMgTZNOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformando punctuation em uma lista, e inserindo na lista das stops_words\n",
        "punctuation = list(punctuation)\n",
        "# Juntando as duas listas em uma só\n",
        "stops.extend(punctuation)"
      ],
      "metadata": {
        "id": "gvuaV9itZPc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Colocando título como substituto do texto\n",
        "contador = 0\n",
        "indices=[]\n",
        "for row in df['message_norm_treatment_ssw2']:\n",
        "  if(len(row)==2):\n",
        "    indices.append(contador)\n",
        "    df['message_norm_treatment_ssw2'][contador] = df['title_norm'][contador]\n",
        "  contador+=1"
      ],
      "metadata": {
        "id": "IUnvhsd1aS5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenizando coluna para remover as stopwords\n",
        "contador3 = 0\n",
        "for row in df['message_norm_treatment_ssw2']:\n",
        "  if(contador3 in indices):\n",
        "    df['message_norm_treatment_ssw2'][contador3] = word_tokenize( df['message_norm_treatment_ssw2'][contador3])\n",
        "    print(df['message_norm_treatment_ssw2'][contador3])\n",
        "  contador3+=1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGRpjsbfZ4EK",
        "outputId": "b7121f2d-f965-418e-93de-5cd76ca49ae8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['simpsons', 'tera', 'episodio', 'sobre', 'manifestacoes', 'no', 'brasil']\n",
            "['lulinha', ',', 'filho', 'de', 'lula', ',', 'comprou', 'aviao', 'gulfstream', 'por', '100', 'milhoes', 'de', 'reais']\n",
            "['pastor', 'sergio', 'von', 'helder', ',', 'que', 'chutou', 'santa', ',', 'nao', 'virou', 'catolico']\n",
            "['mensagem', 'falsa', 'conta', 'briga', 'de', 'luiz', 'claudio', 'lula', 'da', 'silva', 'no', 'cirque', 'du', 'soleil']\n",
            "['vestidinho', 'branco', '-', 'historia', 'de', 'menina', 'que', 'aceitou', 'jesus', 'e', 'foi', 'morta', 'pelo', 'pai', 'e', 'ficticia']\n",
            "['natanael', 'bufalo', ',', 'que', 'matou', 'menina', 'de', '10', 'anos', ',', 'nao', 'esta', 'foragido']\n",
            "['mentira', '-', 'chineses', 'comem', 'bebes', 'para', 'aumentar', 'desempenho']\n",
            "['virus', ':', 'homem', 'ao', 'fazer', 'teste', 'de', 'fidelidade', 'descobre', 'que', 'mulher', 'e', 'lesbica']\n",
            "['mensagem', 'falsa', '-', 'rato', 'e', 'encontrado', 'no', 'acougue', 'do', 'supermercado', 'extra']\n",
            "['senha', 'invertida', 'avisa', 'policia', 'em', 'caso', 'de', 'assalto']\n",
            "['atencao', ':', 'garotas', 'filipinas', 'que', 'pisotearam', 'cao', 'ja', 'foram', 'identificadas']\n",
            "['inca', 'nao', 'precisa', 'de', 'doacoes', 'de', 'toucas', 'para', 'criancas', 'com', 'cancer']\n",
            "['facebook', 'nao', 'doara', '0,5', 'por', 'compartilhamento', 'para', 'tratamento', 'de', 'yasmin']\n",
            "['homem', 'filmado', 'agredindo', 'menino', 'de', '3', 'anos', 'na', 'malasia', 'ja', 'esta', 'preso']\n",
            "['alguem', 'explica', 'isso', ':', 'video', 'de', 'ovni', 'em', 'agudos-sp', 'faz', 'parte', 'de', 'comercial']\n",
            "['foto', 'de', 'dilma', 'rousseff', 'com', 'fuzil', 'e', 'montagem']\n",
            "['royal', 'canin', 'nao', 'e', 'dona', 'do', 'instituto', 'royal', ',', 'que', 'faz', 'teste', 'em', 'animais']\n",
            "['video', '``', 'carro', 'salva', 'aviao', 'de', 'uma', 'tragedia', \"''\", 'faz', 'parte', 'de', 'comercial', 'de', 'tv']\n",
            "['mito', ':', 'consumir', 'alimentos', 'de', 'microondas', 'faz', 'mal', 'a', 'saude']\n",
            "['ficcao', ':', 'jovem', 'humilhado', 'por', 'ser', 'cristao', 'faz', 'jipe', 'andar', 'sem', 'motor']\n",
            "['cores', 'dos', 'quadrados', 'em', 'pasta', 'de', 'dente', 'nao', 'indicam', 'se', 'produto', 'e', 'natural']\n",
            "['ashley', 'flores', 'nao', 'esta', 'desaparecida', 'ha', 'duas', 'semanas']\n",
            "['criatura', 'estranha', 'escala', 'predio', 'na', 'russia', ',', 'aponta', 'video']\n",
            "['montagem', ':', 'serpente', 'de', 'sete', 'cabecas', 'e', 'encontrada', 'em', 'honduras']\n",
            "['historia', 'do', 'rato', 'na', 'coca', 'cola', 'e', 'considerada', 'falsa', 'pela', 'justica']\n",
            "['ultra', 'slim', ':', 'virus', 'do', 'facebook', 'usa', 'produto', 'de', 'emagrecimento', 'como', 'isca']\n",
            "['virus', ':', 'mulher', 'descobre', 'traicao', 'do', 'marido', 'com', 'a', 'amiga', 'e', 'senta', 'a', 'porrada']\n",
            "['avril', 'lavigne', 'morreu', 'e', 'foi', 'substituida', 'por', 'sosia', 'patricinha']\n",
            "['mensagem', 'falsa', 'aponta', 'que', 'whatsapp', 'vai', 'cobrar', '0.37', 'centavos']\n",
            "['virus', ':', 'logotipo', 'da', 'copa', 'do', 'mundo', 'tem', 'mensagem', 'subliminar']\n",
            "['senado', 'aprova', 'bolsa-prostituicao', 'de', 'r', '$', '2', 'mil', 'por', 'mes']\n",
            "['nao', 'vai', 'ter', 'copa', ':', 'possibilidade', 'de', 'copa', 'no', 'brasil', 'ser', 'cancelada', 'e', 'nula']\n",
            "['foto', 'mostra', 'mortos', 'por', 'maduro', 'em', 'praia', 'na', 'venezuela']\n",
            "['materia', 'falsa', 'da', 'france', 'football', 'sobre', 'copa', 'no', 'brasil', 'circula', 'na', 'web']\n",
            "['whatsbook', ':', 'whatsapp', 'sera', 'cancelado']\n",
            "['mensagem', 'falsa', 'diz', 'que', 'colocar', 'limao', 'no', 'copo', 'de', 'cerveja', 'e', 'fatal']\n",
            "['marcola', 'concede', 'entrevista', 'bombastica', 'ao', 'jornal', 'o', 'globo']\n",
            "['foto', 'de', 'justin', 'bieber', 'beijando', 'outro', 'homem', 'e', 'montagem']\n",
            "['teoria', 'da', 'conspiracao', 'aponta', 'que', 'michelle', 'obama', 'e', 'um', 'homem']\n",
            "['camarao', 'com', 'vitamina', 'c', 'e', 'mistura', 'fatal', 'por', 'causa', 'de', 'arsenico']\n",
            "['pesquisa', 'do', 'ipea', 'traz', 'informacao', 'errada', 'sobre', 'estupro', 'e', 'roupas', 'curtas']\n",
            "['teoria', 'falsa', ':', 'masaru', 'emoto', 'diz', 'que', 'poder', 'das', 'palavras', 'pode', 'alterar', 'substancias']\n",
            "['quatro', 'luas', 'de', 'sangue', 'anunciam', 'fim', 'do', 'mundo', ',', 'diz', 'teoria', 'da', 'conspiracao']\n",
            "['virus', 'no', 'facebook', ':', 'ganhe', 'dinheiro', 'sem', 'sair', 'de', 'casa', 'saiba', 'como', 'aqui']\n",
            "['michael', 'jackson', 'esta', 'vivo', ':', 'teoria', 'diz', 'que', 'cantor', 'nao', 'morreu', 'em', '2009']\n",
            "['hello', 'kitty', 'e', 'do', 'diabo', ':', 'lenda', 'fala', 'de', 'pacto', 'de', 'personagem', 'com', 'demonio']\n",
            "['virus', ':', 'e-mail', 'diz', 'para', 'usuario', 'do', 'hotmail', 'recadastrar', 'conta', 'no', 'outlook']\n",
            "['dica', 'falsa', 'de', 'saude', 'diz', 'que', 'aspargos', 'podem', 'curar', 'cancer']\n",
            "['foto', 'falsa', 'mostra', 'papa', 'francisco', 'com', 'camiseta', 'do', 'black', 'sabbath']\n",
            "['lista', 'falsa', 'de', 'convocados', 'do', 'brasil', 'para', 'a', 'copa', 'do', 'mundo', 'vaza', 'na', 'web']\n",
            "['montagem', ':', 'nokia', 'lanca', 'celular', 'que', 'envia', 'mensagem', ',', 'diz', 'capa', 'de', 'jornal']\n",
            "['ilusao', 'de', 'otica', ':', 'carro', 'fantasma', 'aparece', 'em', 'cruzamento', 'na', 'russia']\n",
            "['capa', 'da', 'veja', ':', 'joaquim', 'barbosa', 'pede', 'para', 'populacao', 'nao', 'votar', 'no', 'pt']\n",
            "['selecao', 'japonesa', 'nao', 'viajou', 'para', 'copa', 'do', 'mundo', 'em', 'aviao', 'do', 'pokemon']\n",
            "['bloquinhos', 'de', 'concreto', 'sao', 'realmente', 'colocados', 'lado', 'a', 'lado', 'em', 'calcadas']\n",
            "['ator', 'sacha', 'baron', 'derruba', 'atriz', 'de', '87', 'anos', 'de', 'cima', 'do', 'palco']\n",
            "['os', 'simpsons', 'previu', 'lesao', 'de', 'neymar', 'na', 'copa', ',', 'diz', 'teoria', 'da', 'conspiracao']\n",
            "['propaganda', 'falsa', ':', 'iphone', 'fica', 'a', 'prova', 'dagua', 'se', 'voce', 'instalar', 'o', 'ios', '7']\n",
            "['foto', 'falsa', 'de', 'corpo', 'de', 'eduardo', 'campos', 'apos', 'acidente', 'circula', 'na', 'internet']\n",
            "['13', 'teorias', 'da', 'conspiracao', 'sobre', 'a', 'queda', 'do', 'aviao', 'de', 'eduardo', 'campos']\n",
            "['misterio', 'resolvido', ':', 'identificado', 'ovni', 'no', 'ceu', 'de', 'toronto']\n",
            "['cia', 'matou', 'eduardo', 'campos', 'para', 'ajudar', 'marina', ',', 'diz', 'teoria', 'da', 'conspiracao']\n",
            "['lista', 'falsa', 'de', 'telefones', 'de', 'garotas', 'de', 'programa', 'e', 'divulgada', 'na', 'internet']\n",
            "['mito', ':', 'pessoas', 'que', 'tem', 'letra', 'feia', 'sao', 'mais', 'inteligentes']\n",
            "['ovni', 'com', '450', 'km', 'de', 'diametro', 'e', 'registrado', 'por', 'satelite', 'no', 'chile']\n",
            "['ebola', 'no', 'brasil', ':', 'confira', 'cinco', 'noticias', 'falsas', 'sobre', 'casos', 'da', 'doenca']\n",
            "['aprenda', 'cinco', 'truques', 'para', 'detectar', 'mentiras', 'na', 'internet']\n",
            "['frase', 'atribuida', 'erradamente', 'a', 'george', 'orwell', 'circula', 'pela', 'web']\n",
            "['espanha', 'privatiza', 'o', 'sol', ',', 'diz', 'informacao', 'errada']\n",
            "['ilusao', 'de', 'otica', ':', 'foto', 'de', 'vestido', 'branco', 'e', 'dourado', 'circula', 'na', 'internet']\n",
            "['bracelete', 'cicret', ',', 'que', 'projeta', 'celular', ',', 'nao', 'existe']\n",
            "['site', 'pro-terceirizacao', 'da', 'fiesp', 'usa', 'fotos', 'estrangeiras', 'para', 'ilustrar', 'trabalhadores']\n",
            "['nao', 'beba', 'fanta', 'uva', ',', 'fanta', 'laranja', 'e', 'coca-cola']\n",
            "['comercial', 'da', 'sprite', 'foi', 'proibido']\n",
            "['saiba', 'em', 'que', 'cidade', 'ficam', 'o', 'bar', 'do', 'araujo', 'e', 'as', 'duas', 'igrejas', 'evangelicas']\n",
            "['brasil', 'perdeu', 'a', 'copa', 'para', 'dilma', 'perder', 'as', 'eleicoes']\n",
            "['globo', 'quer', 'proibir', 'que', 'neymar', 'use', 'faixa', '100', '%', 'jesus']\n",
            "['jean', 'wyllys', 'propoe', 'emenda', 'a', 'biblia', 'para', 'retirar', 'trechos', 'homofobicos']\n",
            "['video', 'que', 'circula', 'na', 'web', 'nao', 'e', 'de', 'monica', 'iozzi']\n",
            "['evo', 'morales', 'vai', 'invadir', 'o', 'brasil', '?', 'entenda', 'o', 'caso']\n",
            "['carne', 'processada', 'e', 'feita', 'de', 'animais', 'triturados', ',', 'diz', 'informacao', 'errada']\n",
            "['fique', 'ligado', ':', '7', 'mitos', 'sobre', 'cancer', 'que', 'sempre', 'circulam', 'na', 'web']\n",
            "['brasil', 'vai', 'construir', 'embaixada', 'para', 'terroristas', 'na', 'palestina', '?']\n",
            "['anitta', 'e', 'bi', '?', 'anitta', 'e', 'processada', 'por', 'beyonce', '?', 'anitta', 'e', 'rockeira', '?']\n",
            "['neymar', 'e', 'encontrado', 'morto', '?', 'preso', '?', 'humilhado', '?']\n",
            "['e', 'possivel', 'um', 'tsunami', 'atingir', 'o', 'brasil', '?']\n",
            "['evaristo', 'costa', 'e', 'gay', '?', 'e', 'casado', 'com', 'sandra', 'annemberg', '?']\n",
            "['michel', 'temer', 'e', 'satanista', '?', 'e', 'pai', 'de', 'daniel', 'mastral', '?', 'e', 'bruxo', '?']\n",
            "['molho', 'de', 'tomate', 'com', 'larvas', 'sera', 'verdade', 'mesmo', '?']\n",
            "['vacina', 'contra', 'o', 'cancer', 'de', 'pele', 'e', 'rins', 'e', 'desenvolvida', 'no', 'brasil']\n",
            "['mundo', 'vai', 'acabar', 'no', 'dia', '7', 'de', 'outubro', 'de', '2015', ',', 'diz', 'teoria', 'da', 'conspiracao']\n",
            "['marina', 'silva', 'e', 'evangelica', '?', 'e', 'comunista', '?', 'e', 'corrupta', '?']\n",
            "['brasil', 'tem', 'maior', 'carga', 'tributaria', 'do', 'mundo', 'e', 'ir', 'vai', 'subir', 'para', '35', '%', '?']\n",
            "['whatsapp', 'notificara', 'usuarios', 'quando', 'tela', 'for', 'printada']\n",
            "['novembro', 'azul', '2015', ':', 'o', 'exame', 'de', 'toque', 'retal', 'e', 'obrigatorio', 'para', 'todos', 'os', 'homens', '?']\n",
            "['atencao', ':', 'ser', 'muculmano', 'ou', 'refugiado', 'nao', 'significa', 'ser', 'terrorista']\n",
            "['crise', 'economica', 'prejudicou', 'mercado', 'imobiliario', ',', 'aponta', 'informacao', 'errada']\n",
            "['papai', 'noel', 'e', 'uma', 'invencao', 'da', 'coca-cola']\n",
            "['saiba', 'em', 'quais', 'dias', 'de', '2015', 'a', 'boataria', 'tomou', 'conta', 'da', 'web']\n",
            "['pai', 'que', 'bateu', 'na', 'filha', 'de', '3', 'anos', 'foi', 'morto', 'na', 'cadeia']\n",
            "['foto', 'de', 'aecio', 'neves', 'e', 'sergio', 'moro', 'juntos']\n",
            "['licenca-maternidade', 'de', '10', 'meses', 'e', 'aprovada']\n",
            "['morre', 'ex-goleiro', 'bruno', 'apos', 'brigar', 'com', 'detentos']\n",
            "['cotas', 'para', 'homossexuais', 'em', 'concursos', 'publicos']\n",
            "['novo', 'inseto', 'assassino', 'espalha', 'virus', 'mortal']\n",
            "['lula', 'disse', 'frase', 'sobre', 'jesus', 'e', 'presidencia']\n",
            "['pagina', 'do', 'facebook', 'sorteia', 'l200', '2017', ',', 'hilux', '2016', 'e', 'civic', '2017']\n",
            "['fabio', 'luis', ',', 'filho', 'de', 'lula', ',', 'e', 'o', 'novo', 'embaixador', 'do', 'brasil', 'na', 'russia']\n",
            "['fotos', 'da', 'filha', 'de', 'ratinho', 'caem', 'na', 'web']\n",
            "['claudia', 'leitte', 'ganhou', 'r', '$', '356', 'mil', 'para', 'lancar', 'livro', '(', 'agora', 'e', ')']\n",
            "['homem', 'toma', '24', 'latas', 'de', 'red', 'bull', 'e', 'seu', 'coracao', 'incha', 'para', 'fora', 'do', 'peito']\n",
            "['nunca', 'deixe', 'amem', 'em', 'posts', 'no', 'facebook', ',', 'pelo', 'amor', 'de', 'deus', '!']\n",
            "['50', 'cent', 'emagreceu', 'apos', 'fumar', 'maconha']\n",
            "['exercito', 'brasileiro', 'sai', 'as', 'ruas', 'para', 'conter', 'lula', 'e', 'dilma']\n",
            "['restaurante', 'chines', 'tritura', 'e', 'vende', 'bebes', 'em', 'pernambuco']\n",
            "['lula', 'diz', 'que', 'rico', 'quando', 'rouba', 'vira', 'ministro', '?', 'entenda']\n",
            "['stf', 'nega', 'suspensao', 'de', 'lula', 'como', 'ministro']\n",
            "['grampo', 'foi', 'no', 'telefone', 'de', 'dilma', 'e', 'nao', 'de', 'lula']\n",
            "['morre', 'luiz', 'fernando', 'pezao', ',', 'governador', 'do', 'rj']\n",
            "['pt', 'paga', 'r', '$', '30', 'para', 'manifestantes', ',', 'diz', 'carta']\n",
            "['para', 'curtir', 'e', 'nao', 'acreditar', '7', 'sites', 'de', 'noticias', 'falsas', 'que', 'enganam', 'incautos']\n",
            "['cigarro', 'faz', 'fumaca', 'colorida', 'como', 'um', 'arco-iris']\n",
            "['antonio', 'fagundes', 'critica', 'artistas', 'que', 'apoiam', 'o', 'governo']\n",
            "['voce', 'conhece', 'ninho', 'de', 'cobras', '?', 'saiba', 'o', 'que', 'e', 'no', 'texto', 'viral']\n",
            "['presidente', 'da', 'croacia', 'de', 'biquini', 'na', 'praia']\n",
            "['35', 'dos', '38', 'deputados', 'a', 'favor', 'do', 'impeachment', 'sao', 'corruptos']\n",
            "['integrante', 'do', 'mst', 'tira', 'foto', 'com', 'dilma', 'e', 'e', 'preso', 'com', 'r', '$', '55', 'mil']\n",
            "['7', 'coisas', 'que', 'naovao', 'acontecer', 'se', 'dilma', 'sofrer', 'o', 'impeachment']\n",
            "['dilma', 'sera', 'afastada', 'do', 'governo', 'em', '10', 'dias']\n",
            "['plano', 'temer', 'reduz', 'parlamentares', ',', 'ministerios', 'e', 'acaba', 'com', 'pt']\n",
            "['sergio', 'reis', 'doa', 'todo', 'salario', 'para', 'o', 'hospital', 'do', 'cancer']\n",
            "['senhor', 'morre', 'em', 'posto', 'de', 'saude', 'esperando', 'ser', 'atendido']\n",
            "['etec', 'martin', 'luther', 'king', 'vai', 'fechar', 'por', 'falta', 'de', 'alunos']\n",
            "['papa', 'elogia', 'mp', ',', 'pf', 'e', 'judiciario', 'e', 'surpreende', 'leticia', 'sabatella']\n",
            "['maioria', 'de', 'voto', 'nulo', 'cancela', 'eleicao', 'e', 'troca', 'candidatos']\n",
            "['lula', 'esta', 'muito', 'ruim', ',', 'foi', 'para', 'o', 'hospital', 'e', 'caso', 'e', 'grave']\n",
            "['450', 'homens', 'se', 'casam', 'com', 'criancas', 'em', 'gaza']\n",
            "['menina', 'morta', 'sera', 'enterrada', 'como', 'indigente']\n",
            "['michael', 'schumacher', 'morre', 'aos', '47', 'anos']\n",
            "['radar', 'movel', 'em', 'carro', 'da', 'prf', 'aplica', 'multas']\n",
            "['feijao', 'esta', 'caro', 'por', 'causa', 'de', 'doacao', 'para', 'cuba']\n",
            "['dilma', 'tem', 'conta', 'na', 'suica', 'de', 'r', '$', '700', 'milhoes']\n",
            "['tudo', 'que', 'voce', 'postou', 'no', 'facebook', 'se', 'torna', 'publico', 'amanha']\n",
            "['menino', 'samuel', 'ficou', 'dentro', 'do', 'carro', 'apos', 'assalto', 'em', 'rocha', 'miranda']\n",
            "['cientista', 'diz', 'que', 'criou', 'virus', 'da', 'aids', 'para', 'despovoar']\n",
            "['marina', 'joyce', 'foi', 'sequestrada', 'e', 'esta', 'em', 'perigo']\n",
            "['o', 'mundo', 'acaba', 'hoje', 'e', 'jesus', 'vai', 'voltar', 'para', 'salvar', 'apenas', 'cristaos']\n",
            "['senado', 'aprova', 'fim', 'das', 'ferias', ',', '13', 'salario', 'e', 'privatizacoes']\n",
            "['jesus', 'cristo', 'abre', 'olhos', 'em', 'igreja', 'do', 'mexico']\n",
            "['papinhas', 'de', 'banana', 'da', 'nestle', 'que', 'vencem', 'em', '2017', 'tem', 'vidro']\n",
            "['carla', 'bruni', 'fala', 'sobre', 'mulheres', 'de', '35', 'anos', 'para', 'a', 'veja']\n",
            "['jaden', 'smith', ',', 'filho', 'de', 'will', 'smith', ',', 'e', 'gay', 'e', 'beijou', 'garoto']\n",
            "['klm', 'esta', 'dando', 'passagens', 'gratis', 'para', 'comemorar', 'aniversario']\n",
            "['carro', 'foi', 'roubado', 'com', 'bebe', 'dentro', 'no', 'centro']\n",
            "['tempestade', 'solar', 'chega', 'hoje', 'a', 'terra', '.', 'atencao', '!']\n",
            "['estela', 'pereira', 'calheiros', ',', '14', 'anos', ',', 'esta', 'desaparecida', 'no', 'bairro', 'planalto']\n",
            "['fotos', 'do', 'corpo', 'de', 'domingos', 'montagner', 'vazam', 'na', 'web']\n",
            "['nao', 'temos', 'provas', ',', 'mas', 'temos', 'conviccao', ',', 'diz', 'procurador', 'da', 'lava-jato']\n",
            "['tico', 'santa', 'cruz', 'vai', 'parar', 'de', 'falar', 'em', 'politica', 'se', 'post', 'ganhar', 'curtidas']\n",
            "['freixo', 'vai', 'nomear', 'jean', 'wyllys', 'como', 'secretario', 'de', 'educacao']\n",
            "['alma', 'sai', 'do', 'corpo', 'de', 'mulher', 'apos', 'acidente', 'de', 'moto']\n",
            "['para', 'ler', ',', 'compartilhar', 'e', 'nao', 'cair', ':', '7', 'promocoes', 'falsas', 'que', 'circulam', 'na', 'web']\n",
            "['gilberto', 'gil', 'morreu', 'de', 'insuficiencia', 'renal', 'em', 'sao', 'paulo']\n",
            "['deputados', 'nao', 'votaram', 'projeto', 'de', 'lei', 'da', 'ficha', 'limpa']\n",
            "['fatura', 'do', 'cartao', 'de', 'credito', 'de', 'luciano', 'huck', 'e', 'divulgada', 'na', 'web']\n",
            "['maniaco', 'do', 'parque', 'vai', 'ser', 'solto', 'e', 'sera', 'bispo', 'na', 'igreja', 'universal']\n",
            "['temer', 'quer', 'proibir', 'a', 'biblia', 'no', 'brasil', 'em', '2017']\n",
            "['bilionario', 'africano', 'matou', '20', 'elefantes', 'em', 'seu', 'aniversario']\n",
            "['donald', 'trump', 'menciona', 'jair', 'bolsonaro', 'em', 'discurso']\n",
            "['caio', 'castro', 'morreu', 'em', 'acidente', 'de', 'carro', 'hoje']\n",
            "['corpo', 'do', 'papa', 'joao', 'paulo', 'ii', 'e', 'retirado', 'do', 'tumulo', 'e', 'esta', 'intacto']\n",
            "['ative', 'videochamadas', 'no', 'whatsapp', 'e', 'wi-fi', 'gratis', 'por', 'convite']\n",
            "['blue', 'beam', ':', 'nasa', 'vai', 'simular', '2', 'vinda', 'de', 'jesus', 'cristo']\n",
            "['alteracoes', 'da', 'ldb', 'sairam', 'hoje', 'e', 'a', 'nova', 'legislacao', 'da', 'educacao']\n",
            "['mulher', 'que', 'agrediu', 'bebe', 'em', 'video', 'no', 'sofa', 'foi', 'assassinada']\n",
            "['myriam', 'rios', 'lanca', 'campanha', 'todos', 'contra', 'o', 'sexo', 'anal']\n",
            "['fotos', 'do', 'corpo', 'de', 'fidel', 'castro', 'vazam', 'na', 'internet']\n",
            "['7', 'historias', 'falsas', 'sobre', 'leis', 'de', 'transito', 'que', 'sempre', 'enganam', 'os', 'motoristas']\n",
            "['7', 'historias', '(', 'falsas', ')', 'atribuidas', 'a', 'nasa', 'que', 'as', 'pessoas', 'sempre', 'acreditam']\n",
            "['anuidade', 'para', 'portadores', 'de', 'cnh', 'e', 'lei', 'e', 'sera', 'cobrada', 'por', 'governo']\n",
            "['usp', 'lista', '10', 'maiores', 'sites', 'de', 'noticias', 'falsas', 'do', 'brasil', 'em', 'pesquisa']\n",
            "['piloto', 'de', 'eduardo', 'campos', 'foi', 'envenenado', ',', 'diz', 'laudo']\n",
            "['papa', 'diz', 'que', 'quem', 'desejou', 'morte', 'de', 'marisa', 'nao', 'precisa', 'mais', 'ir', 'na', 'missa']\n",
            "['vazou', 'a', 'lista', 'das', 'delacoes', 'da', 'odebrecht', 'homologadas', '30/01']\n",
            "['policia', 'militar', 'do', 'rio', 'de', 'janeiro', 'vai', 'entrar', 'em', 'greve']\n",
            "['pastora', 'patricia', 'vende', 'oracoes', 'por', 'r', '$', '1.500', 'no', 'whatsapp']\n",
            "['pmdb', 'e', 'psdb', 'aprovam', 'aposentadoria', 'para', 'deputado', 'com', 'um', 'ano', 'de', 'mandato']\n",
            "['juiza', 'solta', 'bandido', 'e', 'e', 'assaltada', 'na', 'saida', 'do', 'forum', 'por', 'ele']\n",
            "['26', 'corpos', 'sao', 'encontrados', 'nos', 'armazens', 'do', 'burger', 'king']\n",
            "['mulher', 'colocou', 'bebe', 'na', 'maquina', 'de', 'lavar', 'e', 'depois', 'foi', 'linchada']\n",
            "['mulher', 'finge', 'pedir', 'ajuda', 'de', 'madrugada', 'no', 'portao', 'para', 'realizar', 'assaltos']\n",
            "['idade', 'de', 'aposentadoria', 'em', 'outros', 'paises', 'e', 'muito', 'menor', 'do', 'que', 'no', 'brasil']\n",
            "['boi', 'sequestro', ',', 'morto', 'na', 'estrada', ',', 'e', 'a', 'causa', 'da', 'carne', 'podre', 'no', 'brasil']\n",
            "['cerveja', 'com', 'urina', 'de', 'cavalo', 'e', 'encontrada', 'em', 'operacao', 'da', 'pf', 'na', 'bahia']\n",
            "['tapioca', 'e', 'um', 'veneno', 'por', 'causa', 'do', 'cianeto', 'e', 'pode', 'matar']\n",
            "['como', 'se', '1', 'de', 'abril', 'fosse', 'todo', 'dia']\n",
            "['assassino', 'de', 'criancas', 'e', 'procurado', 'e', 'fez', 'promessa', 'de', 'matar', 'mais']\n",
            "['kopenhagen', 'esta', 'dando', '1000', 'ovos', 'de', 'pascoa', 'lingua', 'de', 'gato']\n",
            "['nao', 'instale', 'o', 'jogo', 'baleia', 'azul', 'no', 'whatsapp', 'e', 'facebook']\n",
            "['2', 'mi', 'de', 'assinaturas', 'para', 'mandado', 'de', 'seguranca', 'contra', 'reforma', 'da', 'previdencia']\n",
            "['trump', 'diz', 'que', 'brasil', 'deveria', 'sumir', 'do', 'mapa', 'e', 'e', 'um', 'pais', 'de', 'vergonha']\n",
            "['fabrica', 'de', 'mendigos', 'e', 'descoberta', 'no', 'rio', 'de', 'janeiro']\n",
            "['balas', 'envenenadas', 'sao', 'entregues', 'em', 'escolas', 'por', 'causa', 'do', 'jogo', 'baleia', 'azul']\n",
            "['15', 'perguntas', 'e', 'respostas', 'sobre', 'o', 'jogo', 'da', 'baleia', 'azul']\n",
            "['bb', 'informa', 'por', 'sms', 'sobre', 'agendamento', 'de', 'saque', 'e', 'bloqueio', 'do', 'cartao']\n",
            "['7', 'historias', 'falsas', 'sobre', 'crimes', 'cometidos', 'por', 'maes', 'que', 'viralizaram', 'na', 'web']\n",
            "['cemig', 'avisa', 'sobre', 'roubo', 'de', 'uniformes', 'da', 'empresa']\n",
            "['explodiram', 'carro-forte', 'com', 'vigilantes', 'dentro', 'e', 'fotos', 'vazaram']\n",
            "['sao', 'januario', ',', 'estadio', 'do', 'vasco', ',', 'vai', 'ser', 'leiloado', 'pela', 'justica']\n",
            "['delacao', 'da', 'jbs', 'foi', 'divulgada', 'para', 'eleger', 'lula', 'e', 'o', 'pt']\n",
            "['capa', 'da', 'veja', 'aponta', 'que', 'sergio', 'moro', 'enganou', 'povo', 'brasileiro']\n",
            "['stf', 'decide', 'que', 'quem', 'rouba', 'celular', 'de', 'ate', 'r', '$', '500', 'nao', 'e', 'preso']\n",
            "['ponte', 'rio-niteroi', 'e', 'tragedia', 'anunciada', 'por', 'causa', 'de', 'rachadura']\n",
            "['carro', 'do', 'juiz', 'sergio', 'moro', 'acaba', 'de', 'ser', 'metralhado', 'em', 'curitiba']\n",
            "['video', 'mostra', 'tiroteio', 'no', 'tunel', 'marcello', 'alencar', ',', 'no', 'rio', ',', 'ontem']\n",
            "['bomba', 'vai', 'explodir', 'metro', 'da', 'pavuna', 'e', 'central', 'do', 'brasil', ',', 'no', 'rio']\n",
            "['policiais', 'sao', 'filmados', 'quebrando', 'vidracas', 'em', 'brasilia']\n",
            "['globo', 'e', 'a', 'primeira', 'da', 'lista', 'de', 'maiores', 'devedores', 'do', 'bndes']\n",
            "['consulta', 'publica', 'para', 'reducao', 'de', 'salario', 'de', 'politicos', 'acaba', 'hoje']\n",
            "['budweiser', 'falsificada', 'e', 'fabricada', 'por', 'chineses', 'e', 'vendida', 'no', 'brasil']\n",
            "['fabrica', 'clandestina', 'de', 'heineken', 'falsificada', 'e', 'descoberta']\n",
            "['lula', 'e', 'levado', 'as', 'pressas', 'ao', 'hospital', 'sirio-libanes']\n",
            "['engenheiros', ',', 'medicos', 'e', 'policiais', 'ganham', 'direitos', 'iguais', 'a', 'taxistas']\n",
            "['mcdonald', \"'s\", 'da', 'cupom', 'de', 'r', '$', '70', 'gratis', 'em', 'promocao']\n",
            "['tiririca', 'diz', 'que', 'o', 'brasil', 'esta', 'assim', 'porque', 'se', 'esqueceu', 'de', 'deus']\n",
            "['mulher', 'que', 'pisou', 'em', 'cao', 'e', 'espancada', 'na', 'prisao', 'e', 'quase', 'morre']\n",
            "['agua', 'gelada', 'faz', 'mal', 'para', 'voce', 'causa', 'cancer', 'e', 'infarto']\n",
            "['e', 'perigoso', 'cortar', 'uma', 'cebola', 'e', 'comer', 'so', 'no', 'dia', 'seguinte']\n",
            "['whatsapp', 'esta', 'completando', '3', 'anos', 'e', 'voce', 'recebera', 'creditos', 'no', 'celular']\n",
            "['nao', 'aceitem', 'pedidos', 'de', 'crhisopher', 'davies', 'ou', 'jessica', 'davies', 'no', 'facebook']\n",
            "['nao', 'cliquem', 'no', 'video', 'dance', 'of', 'the', 'pope', 'e', 'um', 'virus', 'mortal']\n",
            "['muculmanos', 'do', 'estado', 'islamico', 'chegam', 'com', 'forca', 'a', 'trindade']\n",
            "['juiz', 'pego', 'em', 'blitz', 'mandou', 'rebocar', 'viaturas', 'e', 'motos', 'da', 'policia']\n",
            "['quadrilha', 'internacional', 'esta', 'roubando', 'criancas', 'no', 'brasil', ',', 'diz', 'policia']\n",
            "['bolsonaro', 'ganhou', 'r', '$', '18', 'milhoes', 'em', '2017', 'para', 'proteger', 'temer']\n",
            "['morcegos', 'vampiros', 'estao', 'atacando', 'pessoas', 'dormindo', 'no', 'rio']\n",
            "['sereia', 'e', 'encontrada', 'na', 'praia', 'do', 'algodoal', ',', 'no', 'para']\n",
            "['cantora', 'simaria', 'morreu', 'agora', 'pouco', 'em', 'acidente', 'de', 'carro']\n",
            "['nataline', ',', 'filha', 'de', 'cristine', ',', 'vai', 'ganhar', 'doacao', 'por', 'mensagem', 'no', 'whatsapp']\n",
            "['video', 'mostra', 'momento', 'em', 'que', 'raio', 'atinge', 'rio', 'e', 'causa', 'explosao']\n",
            "['edital', 'do', 'mec', 'preve', 'distribuicao', 'de', '500', 'mil', 'vibradores', 'em', 'escolas']\n",
            "['sergio', 'moro', 'diz', 'que', 'voto', 'nulo', 'e', 'a', 'unica', 'forma', 'de', 'acabar', 'com', 'corruptos']\n",
            "['delegada', 'que', 'arquivou', 'caso', 'de', 'adriano', 'e', 'vista', 'com', 'ele', 'em', 'festa']\n",
            "['bolsonaro', 'esta', 'fora', 'das', 'eleicoes', '2018', 'por', 'causa', 'de', 'condenacao']\n",
            "['silas', 'malafaia', 'disse', 'porque', 'vota', 'em', 'jair', 'bolsonaro', 'para', 'presidente']\n",
            "['fabrizio', 'brambilla', 'e', 'um', 'hacker', 'do', 'facebook', '!', 'nao', 'aceite', 'amizade']\n",
            "['bruno', 'guimaraes', 'buhler', '(', 'core', ')', 'foi', 'morto', 'por', 'causa', 'de', 'denuncia', 'em', 'audio']\n",
            "['noiva', 'traida', 'surpreende', 'convidados', 'em', 'seu', 'proprio', 'casamento']\n",
            "['dipirona', 's/500', 'da', 'venezuela', 'contem', 'virus', 'marburg', ',', 'que', 'e', 'mortal']\n",
            "['comerciante', 'de', 'caldas', 'novas', 'indenizara', 'familia', 'de', 'assaltantes']\n",
            "['american', 'airlines', 'faz', '70', 'anos', 'e', 'esta', 'dando', 'duas', 'passagens', 'aereas']\n",
            "['stf', 'autoriza', 'eua', 'fazer', 'monitoramento', 'do', 'whatsapp', 'de', 'brasileiros']\n",
            "['garoto', 'do', 'vietna', 'deu', 'origem', 'a', 'musica', 'he', 'ai', \"n't\", 'heavy', ',', 'he', \"'s\", 'my', 'brother']\n",
            "['juiz', 'erick', 'bretas', 'faz', 'texto', 'sobre', 'o', 'que', 'se', 'passa', 'no', 'brasil', 'corrupto']\n",
            "['aviao', 'do', 'cantor', 'leonardo', 'cai', 'na', 'praia', 'de', 'pajucara', ',', 'alagoas']\n",
            "['palocci', 'disse', 'que', 'lula', 'e', 'gleisi', 'sao', 'amantes', 'ha', 'anos']\n",
            "['bolas', 'de', 'fogo', 'caem', 'do', 'ceu', 'em', 'israel', ',', 'mostra', 'video']\n",
            "['produtos', 'da', 'pepsi', 'foram', 'contaminados', 'com', 'o', 'virus', 'hiv', ',', 'da', 'aids']\n",
            "['jean', 'wyllys', 'vai', 'lancar', 'o', 'filme', 'jesus', 'a', 'diva', 'da', 'mentira']\n",
            "['senhor', 'esta', 'perdido', 'ha', 'cinco', 'dias', 'na', 'rodoviaria', 'de', 'campinas', '(', 'sp', ')']\n",
            "['santander', 'perdeu', 'mais', 'de', '20', 'mil', 'clientes', 'apos', 'exposicao', 'no', 'rs']\n",
            "['fatima', 'bernardes', 'diz', 'que', 'exposicao', 'do', 'santander', 'deve', 'continuar']\n",
            "['fim', 'do', 'mundo', 'sera', 'em', '23', 'de', 'setembro', 'de', '2017']\n",
            "['exame', 'da', 'oab', 'tera', '3', 'fase', ',', 'que', 'sera', 'prova', 'oral']\n",
            "['talysson', 'de', 'almeida', 'da', 'pf', 'diz', 'que', 'lula', 'se', 'encontrou', 'com', 'delegado', 'da', 'lava', 'jato']\n",
            "['jean', 'wyllys', 'vai', 'lancar', 'filme', 'corpus', 'christi', ',', 'parodia', 'de', 'jesus', 'gay']\n",
            "['jair', 'bolsonaro', 'criou', 'projeto', 'que', 'preve', 'o', 'fim', 'do', 'ipva']\n",
            "['ivete', 'sangalo', 'diz', 'que', 'sergio', 'moro', 'e', 'um', 'mentiroso']\n",
            "['novo', 'numero', 'do', 'samu', 'e', '3254', 'telefone', '192', 'esta', 'com', 'problemas']\n",
            "['ultima', 'mensagem', 'de', 'marcelo', 'rezende', 'para', 'o', 'brasil', 'e', 'divulgada']\n",
            "['adega', 'de', 'joesley', 'batista', 'e', 'descoberta', 'pela', 'policia', 'federal']\n",
            "['bolsonaro', 'alerta', 'sobre', 'roubo', 'de', 'temer', 'na', 'gasolina', 'e', 'energia', 'eletrica']\n",
            "['flor', 'do', 'baoba', 'so', 'brota', 'de', '50', 'em', '50', 'anos', 'e', 'e', 'gigante']\n",
            "['jean', 'wyllys', 'e', 'pabllo', 'vittar', 'farao', 'turne', 'lgbt', 'em', 'escolas', 'do', 'brasil']\n",
            "['silas', 'malafaia', 'diz', 'que', 'jean', 'wyllys', 'quer', 'legalizar', 'sexo', 'com', 'criancas', 'de', '8', 'anos']\n",
            "['video', 'mostra', 'drones', 'da', 'prf', 'fazendo', 'monitoramento', 'e', 'aplicando', 'multas']\n",
            "['o', 'boticario', 'esta', 'dando', 'vale-presente', 'gratis', 'no', 'inicio', 'de', 'primavera']\n",
            "['video', 'mostra', 'lula', 'falando', 'que', 'o', 'pt', 'e', 'fascismo', ',', 'nazismo', ',', 'menos', 'democracia']\n",
            "['mpf', 'proibe', 'a', 'vacina', 'contra', 'o', 'hpv', 'em', 'todo', 'brasil']\n",
            "['wagner', 'schwartz', ',', 'coreografo', 'do', 'la', 'bete', ',', 'e', 'morto', 'a', 'pauladas']\n",
            "['pilula', 'zumbi', 'chegou', 'ao', 'brasil']\n",
            "['urgente', ':', 'comboio', 'do', '61', 'batalhao', 'esta', 'indo', 'para', 'base', 'de', 'santa', 'maria']\n",
            "['wagner', 'schwartz', 'e', 'preso', 'apos', 'pedido', 'de', 'marco', 'feliciano', 'na', 'justica']\n",
            "['quem', 'trabalhou', 'entre', '1998', 'e', '2017', 'tem', 'direito', 'a', '2', 'salarios', 'em', 'lista']\n",
            "['carol', 'barcellos', 'e', 'flagrada', 'em', 'video', 'no', 'rock', 'in', 'rio']\n",
            "['crianca', 'ferida', 'em', 'janauba', 'precisa', 'de', 'sangue', 'ab', 'negativo']\n",
            "['carlos', 'massa', ',', 'ratinho', ',', 'denuncia', 'filme', 'corpus', 'christi', 'de', 'jean', 'wyllys']\n",
            "['cnh', 'gratuita', 'com', 'programa', 'cnh', 'social', 'tem', '38', 'mil', 'vagas']\n",
            "['menina', 'que', 'morreu', 'em', 'janauba', 'gravou', 'video', 'tocando', 'piano']\n",
            "['ministro', 'da', 'china', 'diz', 'o', 'que', 'se', 'deve', 'fazer', 'no', 'brasil']\n",
            "['abertura', 'da', 'nova', 'novela', 'da', 'globo', 'tem', 'beijo', 'gay', 'entre', 'dois', 'homens']\n",
            "['governo', 'liberou', 'lote', 'do', '14', 'salario', 'para', 'quem', 'nasceu', 'de', 'janeiro', 'a', 'julho']\n",
            "['temer', 'revoga', 'decreto', 'imperial', 'que', 'dizia', 'que', 'advogado', 'e', 'doutor']\n",
            "['chico', 'lang', 'e', 'mamma', 'bruschetta', 'sao', 'a', 'mesma', 'pessoa', ',', 'aponta', 'video']\n",
            "['doacao', 'de', 'dois', 'shitzus', 'de', 'tres', 'anos', 'em', 'adocao', 'conjunta']\n",
            "['mae', ',', 'filho', 'e', 'neto', 'morrem', 'em', 'descarga', 'eletrica', 'com', 'celular', 'conectado', 'ao', 'carregador']\n",
            "['fabio', 'de', 'melo', 'faz', 'texto', 'criticando', 'esquizofrenia', 'social', 'e', 'movimento', 'gay']\n",
            "['crivella', 'cobra', 'taxa', 'para', 'suipa', 'realizar', 'evento', 'de', 'adocao', 'de', 'animais']\n",
            "['pabllo', 'vittar', 'vai', 'apresentar', 'programa', 'infantil', 'na', 'globo']\n",
            "['deputado', 'jean', 'wyllys', 'quer', 'legalizar', 'traicao', 'em', 'casamento']\n",
            "['mulher', 'esta', 'perdida', 'e', 'chorando', 'ha', '4', 'dias', 'no', 'instituto', 'da', 'mulher']\n",
            "['diretor', 'do', 'fantastico', 'critica', 'apoio', 'da', 'globo', 'a', 'pedofilia', 'na', 'arte']\n",
            "['morgan', 'freeman', 'morre', 'aos', '80', 'anos', 'no', 'estados', 'unidos']\n",
            "['decalogo', 'de', 'lenin', 'foi', 'escrito', 'em', '1913', 'pelo', 'pai', 'do', 'comunismo']\n",
            "['decreto', 'aumenta', 'aliquota', 'do', 'imposto', 'de', 'renda', 'de', '27,5', '%', 'para', '35', '%']\n",
            "['coca-cola', 'foi', 'contaminada', 'com', 'virus', 'hiv', ',', 'da', 'aids', ',', 'por', 'funcionario']\n",
            "['menina', 'cega', 'ganha', '0,10', 'por', 'video', 'postado', 'para', 'cirurgia', 'na', 'medula']\n",
            "['filhote', 'de', 'cachorro', 'nasce', 'com', 'cara', 'e', 'corpo', 'de', 'gente', ',', 'mostra', 'foto']\n",
            "['temer', 'tem', 'doenca', 'mortal', ',', 'chora', 'e', 'vai', 'deixar', 'o', 'governo']\n",
            "['governo', 'temer', 'aprovou', 'aumento', 'do', 'salario', 'presidiario']\n",
            "['hackers', 'chineses', 'usam', 'mensagens', 'de', 'bom', 'dia', 'em', 'gif', 'para', 'phishing']\n",
            "['padre', 'interrompe', 'missa', ',', 'expulsa', 'lula', 'da', 'igreja', 'e', 'o', 'chama', 'de', 'bandido']\n",
            "['agua', 'de', 'rio', 'em', 'kasese', '(', 'congo', ')', ',', 'na', 'africa', ',', 'virou', 'sangue', 'desde', 'ontem']\n",
            "['pai', 'de', 'bruna', 'marquezine', 'cometeu', 'suicidio', 'quando', 'ela', 'tinha', '5', 'anos']\n",
            "['ana', 'maria', 'braga', 'esta', 'com', 'cancer', ',', 'e', 'afastada', 'do', 'mais', 'voce', 'e', 'chora']\n",
            "['sargento', 'da', 'policia', 'militar', 'tem', 'prisao', 'decretada', 'apos', 'prender', 'filho', 'de', 'juiz']\n",
            "['maria', 'do', 'rosario', 'e', 'jean', 'wyllys', 'querem', 'descriminalizar', 'a', 'pedofilia']\n",
            "['carro', 'explode', 'em', 'posto', 'de', 'gasolina', 'por', 'causa', 'de', 'telefone', 'celular']\n",
            "['primo', 'de', 'marcelo', 'freixo', ',', 'gustavo', ',', 'deu', 'drogas', 'e', 'estuprou', 'alunos']\n",
            "['novo', 'clipe', 'de', 'gabriel', 'o', 'pensador', 'vai', 'ser', 'retirado', 'do', 'ar', 'pela', 'presidencia']\n",
            "['menino', 'gustavo,5', 'anos', ',', 'foi', 'sequestrado', 'no', 'santa', 'monica', 'de', 'madureira']\n",
            "['video', 'do', 'deputado', 'wladimir', 'costa', 'com', 'loira', 'vaza', 'na', 'internet']\n",
            "['livro', 'de', 'roberto', 'bolanos', 'revela', 'que', 'seu', 'madruga', 'e', 'o', 'pai', 'de', 'chaves']\n",
            "['palmeiras', 'tem', 'mundial', 'interclubes', 'reconhecido', 'pela', 'fifa']\n",
            "['rosto', 'de', 'jesus', 'aparece', 'nas', 'nuvens', 'em', 'dianopolis', ',', 'tocantins']\n",
            "['orgia', 'entre', 'idosos', 'acaba', 'com', 'sete', 'mortos', 'na', 'belgica']\n",
            "['bonecas', 'trans', ',', 'como', 'a', 'niny', ',', 'comecam', 'a', 'ser', 'vendidas', 'no', 'brasil']\n",
            "['silas', 'malafaia', 'pede', 'boicote', 'a', 'novela', '``', 'gay', \"''\", 'da', 'globo', '``', 'demoniaca', \"''\"]\n",
            "['comentarista', 'da', 'tv', 'alema', 'frederick', 'von', 'mullir', 'critica', 'os', 'brasileiros']\n",
            "['papa', 'apela', 'para', 'trump', 'aprovar', 'lei', 'do', 'descanso', 'dominical']\n",
            "['hitler', '(', 'ou', 'stalin', ')', 'e', 'a', 'historia', 'da', 'galinha', 'depenada']\n",
            "['7', 'recomendacoes', 'medicas', 'falsas', 'que', 'sempre', 'enganam', 'internautas']\n",
            "['corinthians', 'vendeu', 'o', 'brasileirao', 'para', 'a', 'crefisa', 'e', 'para', 'o', 'palmeiras']\n",
            "['diretor', 'carlos', 'henrique', 'schraeder', 'se', 'demitiu', 'da', 'globo', 'e', 'fez', 'gravacao']\n",
            "['foto', 'que', 'ensina', 'como', 'se', 'enforcar', 'esta', 'se', 'espalhando', 'entre', 'criancas', 'e', 'site', 'infantis']\n",
            "['policia', 'ira', 'averiguar', 'conteudo', 'do', 'telefone', 'celular', 'em', 'abordagens']\n",
            "['governo', 'quer', 'cortar', 'bpc', 'de', '4,7', 'milhoes', 'de', 'pessoas', 'com', 'cadunico']\n",
            "['mst', 'esta', 'destruindo', 'torres', 'de', 'energia', 'para', 'fazendas', 'nao', 'produzirem']\n",
            "['burundanga', 'esta', 'sendo', 'usada', 'por', 'bandidos', 'em', 'trafico', 'de', 'orgaos']\n",
            "['significado', 'do', 'emoji', 'de', 'maos', 'juntas', 'e', 'oracao', 'para', 'deuses', 'estranhos']\n",
            "['clipe', 'de', 'lorena', 'e', 'rafaela', 'foi', 'retirado', 'do', 'youtube', 'pelo', 'congresso']\n",
            "['menina', 'chega', 'atrasada', 'no', 'enem', 'por', 'dois', 'anos', 'seguidos']\n",
            "['venezuelanos', 'estao', 'ha', 'um', 'mes', 'protestando', 'nas', 'ruas', ',', 'mostra', 'video']\n",
            "['atirador', 'da', 'igreja', 'do', 'texas', 'era', 'do', 'grupo', 'esquerdista-comunista', 'antifas']\n",
            "['whatsapp', 'esta', 'dando', 'recargas', 'gratis', 'por', 'causa', 'da', 'black', 'friday']\n",
            "['o', 'boticario', 'esta', 'dando', 'lapis', 'delineador', 'make', 'b', 'gratis', 'em', 'site']\n",
            "['hospital', 'san', 'juan', 'de', 'dios', 'ganhara', '5', 'centavos', 'por', 'visualizacao', 'de', 'video', 'com', 'macaco']\n",
            "['caminhao', 'do', 'deputado', 'romanelli', 'foi', 'apreendido', 'com', 'drogas']\n",
            "['quem', 'e', 'beneficiario', 'do', 'loas', 'tem', 'que', 'se', 'recadastrar', 'no', 'inss']\n",
            "['latam', 'esta', 'dando', '2', 'ingressos', 'gratis', 'para', 'todos', 'no', '41', 'aniversario']\n",
            "['rodrigo', 'maia', 'diz', 'que', 'congresso', 'nao', 'e', 'obrigado', 'a', 'ouvir', 'o', 'povo']\n",
            "['lula', 'contratou', '15', 'advogados', 'para', 'tirar', 'video', 'a', 'verdade', 'oculta', 'do', 'ar']\n",
            "['gol', 'esta', 'dando', 'passagens', 'gratis', 'de', 'final', 'de', 'ano', 'no', 'site', 'voe', 'de', 'gol']\n",
            "['o', 'boticario', 'esta', 'presenteando', 'voce', 'com', 'kit', 'natal', ',', 'perfume', 'e', 'creme']\n",
            "['tiririca', 'faz', 'revelacao', 'chocante', 'sobre', 'eleicoes', 'de', '2018', 'em', 'carta']\n",
            "['cobra', 'venenosa', 'e', 'achada', 'em', 'vaso', 'sanitario', 'de', 'sao', 'joao', 'del', 'rei']\n",
            "['nova', 'especie', 'de', 'cobra', 'que', 'se', 'esconde', 'na', 'areia', 'aparece', 'em', 'itaparica']\n",
            "['pessoas', 'abandonam', 'carros', 'por', 'causa', 'de', 'aumento', 'da', 'gasolina', 'na', 'alemanha']\n",
            "['papa', 'francisco', 'e', 'flagrado', 'dancando', 'merengue', 'com', 'mulher', 'em', 'festa']\n",
            "['sardinhas', 'estao', 'vindo', 'com', 'bolinhas', 'brancas', 'que', 'causam', 'parasita', 'mortal']\n",
            "['anitta', 'errou', 'a', 'letra', 'do', 'hino', 'nacional', 'no', 'gp', 'brasil', 'de', 'f-1']\n",
            "['cacau', 'show', 'esta', 'presenteando', 'com', 'kit', 'natal', 'em', 'site', 'oficial']\n",
            "['carne', 'de', 'cavalo', 'em', 'embalagem', 'friboi', 'e', 'vendida', 'em', 'acougues', 'e', 'feiras']\n",
            "['pisca', 'piscas', 'chineses', 'fazem', 'arvore', 'de', 'natal', 'pegar', 'fogo', ',', 'mostra', 'video']\n",
            "['governo', 'lancou', 'lei', 'para', 'cobrar', 'multa', 'r', '$', '150', 'de', 'quem', 'nao', 'fez', 'biometria']\n",
            "['smartmatic', ',', 'responsavel', 'pelas', 'urnas', 'eletronicas', ',', 'fraudou', 'eleicoes', 'do', 'brasil']\n",
            "['fernando', 'segovia', ',', 'chefe', 'da', 'pf', ',', 'foi', 'a', 'protesto', 'com', 'nariz', 'de', 'palhaco']\n",
            "['professora', 'yokasta', 'm', 'obriga', 'alunos', 'a', 'transar', 'com', 'ela', 'em', 'troca', 'de', 'notas', 'altas']\n",
            "['anjo', 'caiu', 'do', 'ceu', 'hoje', 'de', 'manha', 'em', 'londres', 'e', 'na', 'china']\n",
            "['general', 'aureliano', 'fala', ',', 'em', 'audio', 'e', 'video', ',', 'que', 'intervencao', 'militar', 'e', 'inevitavel']\n",
            "['fausto', 'silva', ',', 'o', 'faustao', ',', 'morre', 'de', 'infarto', 'em', 'sao', 'paulo']\n",
            "['bolsonaro', 'diz', 'que', 'acredita', 'na', 'teoria', 'da', 'terra', 'plana']\n",
            "['mst', 'rouba', 'ovos', 'de', 'tartaruga', 'no', 'rio', 'solimoes', 'para', 'vender']\n",
            "['bandidos', 'colocam', 'garrafa', 'pet', 'em', 'pneu', 'de', 'carros', 'para', 'assaltar']\n",
            "['cuidado', 'com', 'a', 'lei', 'do', '1', 'minuto', 'e', 'meio', 'ao', 'acordar', 'para', 'nao', 'ter', 'um', 'avc']\n",
            "['pcc', 'oferece', 'r', '$', '50', 'mil', 'pela', 'morte', 'da', 'socialite', 'day', 'mccarthy']\n",
            "['eliana', 'acaba', 'de', 'anunciar', 'a', 'morte', 'de', 'silvio', 'santos']\n",
            "['enfermeira', 'alerta', 'que', 'bacteria', 'sem', 'cura', 'esta', 'nas', 'notas', 'de', 'dinheiro']\n",
            "['recarga', 'facil', 'esta', 'dando', 'r', '$', '20', 'em', 'creditos', 'gratis', 'em', 'promocao', 'de', 'natal']\n",
            "['banco', 'central', 'anuncia', 'que', 'vai', 'bloquear', 'contas', 'com', 'mais', 'de', 'r', '$', '10', 'mil']\n",
            "['moacyr', 'franco', 'foi', 'demitido', 'do', 'sbt', 'por', 'causa', 'de', 'musica', 'proibida', 'pela', 'justica']\n",
            "['perfume', 'acqua', 'di', 'gioia', 'esta', 'sendo', 'usado', 'por', 'bandidos', 'em', 'assaltos']\n",
            "['restaurante', 'que', 'vende', 'carne', 'humana', 'e', 'inaugurado', 'em', 'toquio']\n",
            "['medium', 'que', 'previu', 'acidente', 'da', 'chapecoense', 'preve', 'tsunami', 'em', '2018']\n",
            "['silvio', 'santos', 'chama', 'bolsonaro', 'de', 'corrupto', 'e', 'torturador', 'no', 'roda', 'a', 'roda']\n",
            "['celular', 'carregando', 'explodiu', 'no', 'rosto', 'de', 'um', 'rapaz', 'e', 'ele', 'morreu']\n",
            "['nao', 'atenda', 'ao', 'celular', 'carregando', ',', 'alerta', 'tecnico', 'da', 'samsung']\n",
            "['coca-cola', 'tem', 'prejuizo', 'bilionario', 'por', 'causa', 'de', 'foto', 'de', 'pabllo', 'vittar']\n",
            "['tiririca', 'pede', 'que', 'seus', 'eleitores', 'apoiem', 'jair', 'bolsonaro']\n",
            "['alunos', 'tiram', 'foto', 'com', 'prato', 'de', 'merenda', 'e', 'devolvem', 'em', 'escola', 'no', 'brasil']\n",
            "['voce', 'ganhou', 'um', 'premio', 'atraves', 'do', 'messenger', 'do', 'facebook']\n",
            "['homem', 'com', 'penis', 'pequeno', 'ganha', 'direito', 'a', 'bolsa', 'familia', 'no', 'rio']\n",
            "['aracy', 'balabanian', 'morre', 'de', 'cancer', 'aos', '76', 'anos', 'no', 'rio', 'de', 'janeiro']\n",
            "['caixa', 'libera', 'bonus', 'de', 'r', '$', '326,00', 'do', '13', 'salario', 'confira', 'lista', 'em', 'site']\n",
            "['bolsonaro', 'lidera', 'pesquisa', 'datafolha', 'em', 'todos', 'os', 'estados', 'do', 'brasil']\n",
            "['bebe', 'tem', 'cancer', 'no', 'sangue', 'e', 'whatsapp', 'paga', 'um', 'real', 'por', 'compartilhamento']\n",
            "['pandora', 'esta', 'dando', 'braceletes', 'e', 'charms', 'de', 'brinde', 'em', 'site', 'oficial']\n",
            "['tiririca', 'renuncia', 'ao', 'mandato', 'de', 'deputado', 'federal', 'em', 'desabafo']\n",
            "['ex-goleiro', 'bruno', 'morreu', 'hoje', 'espancado', 'em', 'sua', 'cela']\n",
            "['jose', 'genoino', ',', 'do', 'pt', ',', 'e', 'o', 'primeiro', 'suplente', 'de', 'tiririca', 'na', 'camara']\n",
            "['arlindo', 'cruz', 'morreu', ':', 'mestre', 'do', 'samba', 'acaba', 'de', 'falecer']\n",
            "['fbi', 'divulga', 'que', 'brasil', 'vendeu', 'o', 'jogo', 'para', 'a', 'alemanha', 'na', 'copa', 'de', '2014']\n",
            "['cheirar', 'pum', 'do', 'parceiro', 'prolonga', 'vida', 'e', 'evita', 'doencas']\n",
            "['cancer', 'e', 'apenas', 'uma', 'deficiencia', 'de', 'vitamina', 'b17', 'e', 'nao', 'uma', 'doenca']\n",
            "['benjamin', 'netanyahu', 'faz', 'discurso', 'sobre', '70', 'anos', 'de', 'israel', 'e', 'judeus']\n",
            "['rodrigo', 'santoro', 'posou', 'com', 'cartaz', 'pedindo', 'fora', 'bolsonaro']\n",
            "['milhares', 'de', 'passaros', 'sao', 'libertados', 'por', 'ativistas', 'do', 'greenpeace', 'em', 'video']\n",
            "['tiririca', 'fez', 'discurso', 'so', 'para', 'ganhar', 'aposentadoria', 'integral', 'de', 'parlamentar']\n",
            "['pescadores', 'encontram', 'peixe', '(', 'sereia', ')', 'com', 'quatro', 'peitos', 'e', 'corpo', 'de', 'gente']\n",
            "['contracheque', 'prova', 'que', 'lula', 'ganha', 'r', '$', '45', 'mil', 'como', 'anistiado', 'politico']\n",
            "['general', 'mourao', 'foi', 'promovido', 'no', 'exercito', 'apos', 'pedir', 'intervencao']\n",
            "['mulheres', 'abusam', 'de', 'homem', 'que', 'estava', 'dormindo', 'e', 'sao', 'presas']\n",
            "['recarga', 'realizada', 'com', 'sucesso', '!', 'faca', 'agora', 'recarga', 'gratis']\n",
            "['sergio', 'moro', 'faz', 'declaracao', 'pedindo', 'para', 'voce', 'nao', 'reeleger', 'ninguem']\n",
            "['o', 'boticario', 'esta', 'dando', 'amostra', 'gratis', 'do', 'floratta', 'flores', 'secretas', 'em', 'site']\n",
            "['tecido', 'que', 'torna', 'invisivel', 'e', 'inventado', 'por', 'cientista', 'na', 'china']\n",
            "['milionaria', 'de', '83', 'anos', 'se', 'casou', 'com', 'cuidador', 'de', 'idosos', 'de', '24', 'anos', 'em', 'goias']\n",
            "['embraer', 'faz', 'propaganda', 'criticando', 'impostos', 'e', 'politicos', 'do', 'brasil']\n",
            "['pastor', 'julio', 'cesar', 'gravou', 'audio', 'no', 'whatsapp', 'antes', 'de', 'se', 'suicidar']\n",
            "['globo', 'e', 'dona', 'da', 'mapfre', ',', 'maior', 'empresa', 'de', 'previdencia', 'privada', 'do', 'brasil']\n",
            "['flamengo', 'e', 'suspenso', 'por', 'dois', 'anos', 'pela', 'conmebol', 'e', 'esta', 'fora', 'da', 'libertadores']\n",
            "['artista', 'dos', 'eua', 'ganha', 'r', '$', '9', 'milhoes', 'da', 'lei', 'rouanet', 'com', 'arte', 'invisivel']\n",
            "['argentinos', 'gritam', '``', 'aqui', 'nao', 'e', 'brasil', \"''\", 'e', 'derrubam', 'reforma', 'da', 'previdencia']\n",
            "['bono', 'vox', 'vira', 'ao', 'brasil', 'para', 'o', 'julgamento', 'de', 'lula', 'em', 'janeiro']\n",
            "['caiu', 'a', 'casa', 'da', 'mega', 'sena', ':', 'policia', 'federal', 'descobre', 'fraude', 'em', 'loterias']\n",
            "['maria', 'do', 'rosario', 'pede', 'que', 'familias', 'recebam', 'presos', 'em', 'ceia', 'de', 'natal']\n",
            "['whatsapp', 'liberou', 'retrospectiva', '2017', 'para', 'quem', 'compartilhar', 'no', 'app']\n",
            "['carrefour', 'tem', 'vaga', 'de', 'emprego', 'para', 'quem', 'compartilhar', 'mensagem']\n",
            "['ganhe', 'bilhete', 'da', 'mega', 'da', 'virada', 'gratis', 'acessando', 'site', 'no', 'whatsapp']\n",
            "['ganhe', 'bitcoins', 'gratis', 'participando', 'de', 'questionario', 'em', 'site']\n",
            "['ative', 'o', 'whatsapp', 'multicolorido', 'com', 'novos', 'emoticons', 'em', 'site']\n",
            "['dirigir', 'alcoolizado', 'e', 'crime', 'com', 'pena', 'de', '5', 'anos', 'de', 'prisao', ',', 'diz', 'lei', '13.546']\n",
            "['marco', 'feliciano', 'cria', 'projeto', 'que', 'criminaliza', 'sexo', 'anal']\n",
            "['pabllo', 'vittar', 'vai', 'ganhar', 'r', '$', '5', 'milhoes', 'da', 'lei', 'rouanet', 'em', '2018']\n",
            "['vagas', 'de', 'emprego', 'na', 'cacau', 'show', 'para', 'todo', 'brasil', 'em', 'site']\n",
            "['juiza', 'do', 'forum', 'de', 'duque', 'de', 'caxias', 'bate', 'carro', 'e', 'agride', 'policiais']\n",
            "['austria', 'e', 'o', '1', 'pais', 'com', 'governo', '100', '%', 'de', 'direita', 'e', 'proibe', 'comunismo']\n",
            "['walmart', 'da', 'cupons', 'e', 'vouchers', 'de', 'r', '$', '40', 'mil', 'em', 'premios', 'na', 'roleta', 'da', 'sorte']\n",
            "['fbi', 'vai', 'pedir', 'a', 'prisao', 'de', 'lula', 'e', 'dilma', 'nos', 'eua']\n",
            "['shakespeare', 'disse', 'eu', 'sempre', 'me', 'sinto', 'feliz', 'porque', 'nao', 'espero', 'nada', 'de', 'ninguem']\n",
            "['cartao', 'de', 'credito', 'para', 'quem', 'tem', 'nome', 'sujo', 'esta', 'em', 'site', 'no', 'whatsapp']\n",
            "['carlos', ',', 'diretor', 'do', 'facebook', ',', 'diz', 'que', 'servico', 'vai', 'custar', 'dinheiro']\n",
            "['policia', 'federal', 'descobre', 'fraude', 'na', 'mega', 'sena', 'da', 'virada']\n",
            "['doria', 'cancela', 'homenagem', 'a', 'dona', 'marisa', 'e', 'viaduto', 'tera', 'nome', 'de', 'professora', 'heley']\n",
            "['burger', 'king', 'esta', 'dando', 'cupom', 'de', 'r', '$', '50', 'em', 'site', 'burguer-king']\n",
            "['virada', 'de', 'ano', 'na', 'tijuca', '(', 'rj', ')', 'tem', 'queima', 'de', 'fogos', 'com', 'tiroteio']\n",
            "['assai', 'atacadista', 'faz', 'sorteio', 'de', 'ate', 'r', '$', '40', 'mil', 'em', 'cupons', 'em', 'site', 'de', 'vouchers']\n",
            "['video', 'mostra', 'pabllo', 'vittar', 'antes', 'da', 'fama', 'cantando', 'no', 'idolos']\n",
            "['azul', 'esta', 'dando', 'passagens', 'gratis', 'em', 'promocao', 'para', 'quem', 'compartilhar']\n",
            "['carmen', 'lucia', 'publicou', 'carta', 'no', 'o', 'globo', 'falando', 'de', 'revolucao', 'sem', 'armas']\n",
            "['novas', 'multas', 'estao', 'valendo', 'a', 'partir', 'de', 'hoje', ',', '2018', ',', 'no', 'detran']\n",
            "['cursos', 'gratuitos', 'no', 'senai', 'e', 'senac', 'para', 'quem', 'compartilhar', 'no', 'whatsapp']\n",
            "['abono', 'salarial', '2018', 'foi', 'liberado', 'para', 'quem', 'compartilhar', 'no', 'whatsapp']\n",
            "['cassio', 'e', 'fagner', ',', 'do', 'corinthians', ',', 'anunciam', 'que', 'vao', 'se', 'casar']\n",
            "['padre', 'reginaldo', 'manzotti', 'engravidou', 'garota', 'de', '21', 'anos', 'em', 'mg']\n",
            "['radar', 'movel', 'e', 'colocado', 'em', 'carro', 'para', 'aplicar', 'multas', 'a', 'noite', 'em', 'sao', 'paulo']\n",
            "['muculmanos', 'destroem', 'estatua', 'de', 'jesus', 'e', 'virgem', 'maria', 'na', 'italia']\n",
            "['bebe', 'com', 'cancer', 'no', 'sangue', 'vai', 'ganhar', 'r', '$', '1', 'por', 'compartilhamento', 'no', 'whatsapp']\n",
            "['fugitivos', 'de', 'goias', 'estao', 'na', 'regiao', 'pedindo', 'agua', 'e', 'sao', 'perigosos']\n",
            "['brasileiro', 'que', 'vendia', 'pao', 'de', 'queijo', 'nos', 'eua', 'foi', 'denunciado', 'por', 'brasileiros']\n",
            "['video', 'intimo', 'de', 'anitta', 'com', 'bandidos', 'apos', 'baile', 'funk', 'vaza', 'na', 'web']\n",
            "['interpol', 'apreende', 'caixas', 'da', 'cruz', 'vermelha', 'com', 'dinheiro', 'do', 'brasil', 'para', 'comunistas']\n",
            "['policia', 'encontra', 'ossada', 'de', 'eliza', 'samudio', 'em', '2018']\n",
            "['video', 'mostra', 'ondas', 'gigantes', 'na', 'plataforma', 'de', 'mongagua', ',', 'no', 'brasil']\n",
            "['policia', 'da', 'bolivia', 'invade', 'catedral', 'de', 'la', 'paz', 'e', 'tranca', 'cristaos', 'dentro']\n",
            "['jornalista', 'da', 'globo', 'diz', 'que', 'placar', 'do', 'trf-4', 'sera', '2', 'a', '1', 'a', 'favor', 'de', 'lula']\n",
            "['alvaro', 'dias', 'diz', 'que', 'afastamento', 'de', 'diretores', 'da', 'caixa', 'foi', 'por', 'causa', 'de', 'fraude', 'em', 'loterias']\n",
            "['spotify', 'esta', 'dando', 'contas', 'premium', 'por', '1', 'ano', 'em', 'site', 'no', 'whatsapp']\n",
            "['video', 'mostra', 'ataque', 'em', 'igreja', 'na', 'bolivia', 'e', 'morte', 'de', 'cristaos']\n",
            "['sergio', 'moro', 'manda', 'carta', 'ao', 'povo', 'brasileiro', 'sobre', 'fazer', 'a', 'revolucao']\n",
            "['medica', 'funcionaria', 'do', 'instituto', 'butantan', 'fala', 'do', 'perigo', 'da', 'vacina', 'da', 'febre', 'amarela']\n",
            "['pf', 'prende', 'bruxo', 'que', 'sacrificava', 'criancas', 'em', 'nome', 'da', 'globo', 'e', 'politicos']\n",
            "['motoristas', 'do', 'uber', 'usam', 'bala', 'com', 'sonifero', 'para', 'dopar', 'passageiras']\n",
            "['cantor', 'leonardo', 'comemora', 'condenacao', 'de', 'lula', 'cantando', 'a', 'casa', 'caiu']\n",
            "['vacina', 'contra', 'febre', 'amarela', 'paralisa', 'o', 'figado', ',', 'diz', 'medico', 'de', 'sorocaba']\n",
            "['macacos', 'transmitem', 'febre', 'amarela', 'para', 'humanos', 'e', 'a', 'solucao', 'e', 'matar']\n",
            "['unimed', 'esta', 'dando', 'planos', 'de', 'saude', 'gratis', 'por', 'um', 'ano', 'em', 'site']\n",
            "['mais', 'de', '100', 'bandidos', 'e', 'traficantes', 'roubaram', 'onibus', 'brt', 'no', 'rio']\n",
            "['lutadora', 'de', 'vale', 'tudo', 'morre', 'apos', 'enfrentar', 'adversaria', 'trans', 'na', 'malasia']\n",
            "['video', 'mostra', 'rato', 'tomando', 'banho', 'com', 'sabonete', 'em', 'pia']\n",
            "['caseiro', 'do', 'sitio', 'de', 'atibaia', 'diz', 'que', 'lula', 'tem', 'cofre', 'de', 'concreto', 'no', 'local']\n",
            "['maduro', 'vai', 'soltar', '1500', 'presos', 'crueis', 'que', 'podem', 'vir', 'da', 'venezuela', 'para', 'o', 'brasil']\n",
            "['lula', 'inventou', 'reuniao', 'da', 'fao', 'na', 'etiopia', 'para', 'fugir', 'do', 'brasil']\n",
            "['video', 'de', 'jaqueline', 'no', 'bbb', '18', 'ao', 'vivo', 'circula', 'na', 'internet']\n",
            "['oms', 'muda', 'classificacao', 'de', 'idade', 'para', 'jovens', 'e', 'idosos']\n",
            "['compartilhe', 'video', 'de', 'pai', 'agredindo', 'criancas', 'para', 'que', 'ele', 'seja', 'preso']\n",
            "['tubarao', 'ataca', 'surfista', 'em', 'praia', 'e', 'video', 'flagra', 'tudo']\n",
            "['fotos', 'de', 'cristiane', 'brasil', 'e', 'namorada', 'sao', 'publicadas', 'na', 'internet']\n",
            "['juiza', 'federal', 'bebada', 'arma', 'barraco', 'em', 'balneario', 'camboriu', '(', 'sc', ')']\n",
            "['ministra', 'carmen', 'lucia', 'e', 'prima', 'de', 'sepulveda', 'pertence', ',', 'advogado', 'de', 'lula']\n",
            "['eclipse', 'da', 'lua', 'no', 'tibet', ',', 'em', 'kailas', 'manaasa', 'sarova', ',', 'e', 'registrado', 'em', 'fotos', 'incriveis']\n",
            "['pai', 'e', 'filha', 'praticam', 'incesto', 'no', 'bbb', '18', 'e', 'policia', 'invade', 'programa']\n",
            "['lufthansa', 'esta', 'dando', 'passagens', 'aereas', 'gratis', 'em', 'aniversario']\n",
            "['bolsonaro', 'diz', 'que', 'vai', 'metralhar', 'favela', 'da', 'rocinha', 'se', 'for', 'eleito']\n",
            "['video', 'mostra', 'deputados', 'marcando', 'presenca', 'de', 'colegas', 'durante', 'o', 'carnaval', 'no', 'brasil']\n",
            "['xuxa', 'declara', 'apoio', 'a', 'jair', 'bolsonaro', 'em', 'post', 'no', 'twitter']\n",
            "['air', 'france', 'oferece', 'duas', 'passagens', 'gratis', 'em', 'comemoracao', 'ao', '85', 'aniversario']\n",
            "['lula', 'vai', 'pagar', 'r', '$', '50', 'milhoes', 'de', 'honorarios', 'para', 'sepulveda', 'pertence']\n",
            "['traficante', 'lambari', ',', 'do', 'jacarezinho', ',', 'vai', 'expulsar', 'bandidos', 'do', 'chapadao']\n",
            "['foto', 'mostra', 'atirador', 'da', 'florida', 'usando', 'camiseta', 'comunista']\n",
            "['eduardo', ',', 'filho', 'de', 'jair', 'bolsonaro', ',', 'e', 'flagrado', 'marcando', 'a', 'presenca', 'de', 'colegas', 'na', 'camara']\n",
            "['general', 'gramoza', 'faz', 'discurso', 'maravilhoso', 'sobre', 'intervencao', 'militar']\n",
            "['app', 'taxi.rio', 'foi', 'criado', 'para', 'universal', 'cobrar', 'dizimo', 'de', 'taxistas']\n",
            "['geddel', 'esta', 'com', 'cancer', 'e', 'denuncia', 'fazendas', 'de', 'lula', 'e', 'dilma', 'em', 'video']\n",
            "['boletim', 'interno', 'spoex', 'do', 'exercito', 'proibe', 'pessoas', 'nas', 'ruas', 'depois', 'das', '22:00']\n",
            "['video', 'mostra', 'tanques', 'do', 'exercito', 'na', 'via', 'dutra', 'para', 'intervencao', 'militar', 'no', 'rio']\n",
            "['sylvester', 'stallone', 'morreu', 'nesta', 'madrugada', 'de', 'cancer', 'de', 'prostata']\n",
            "['detalhes', 'da', 'intervencao', 'no', 'rio', '(', 'plano', 'de', 'acao', 'redentor', ')', 'vazaram', 'na', 'web']\n",
            "['policial', 'vive', 'dia', 'de', 'chuck', 'norris', 'e', 'mata', '10', 'suspeitos', 'em', 'sao', 'paulo']\n",
            "['balas', 'popping', 'candy', 'distribuidas', 'em', 'escolas', 'sao', 'comprimidos', 'de', 'ecstasy']\n",
            "['renato', 'aragao', ',', 'o', 'didi', ',', 'morreu', 'hoje', 'de', 'manha']\n",
            "['globo', 'rural', 'disponibiliza', 'mudas', 'de', 'rosas', 'do', 'deserto', 'gratis', 'em', 'site']\n",
            "['crivella', 'diz', 'que', 'rio', 'foi', 'castigado', 'com', 'diluvio', 'por', 'causa', 'do', 'carnaval']\n",
            "['pabllo', 'vittar', 'leva', 'tombo', 'durante', 'show', 'e', 'paga', 'mico', ',', 'mostra', 'video']\n",
            "['globo', 'news', 'se', 'arrependeu', 'de', 'convidar', 'general', 'heleno', 'para', 'debate']\n",
            "['claudia', 'rodrigues', 'perde', 'todo', 'cabelo', 'por', 'causa', 'de', 'doenca', 'grave', ',', 'mostra', 'foto']\n",
            "['mulher', 'solta', 'pum', 'em', 'voo', 'e', 'aviao', 'e', 'obrigado', 'a', 'fazer', 'pouso', 'de', 'emergencia']\n",
            "['soldado', 'da', 'policia', 'do', 'exercito', 'e', 'afastado', 'apos', 'agredir', 'traficante', 'a', 'pedido', 'da', 'oab']\n",
            "['video', 'mostra', 'chuva', 'filmada', 'do', 'observatorio', 'de', 'belo', 'horizonte']\n",
            "['virus', 'da', 'febre', 'amarela', 'sofreu', 'uma', 'mutacao', 'e', 'vacina', 'nao', 'protege', 'mais']\n",
            "['existem', '9', 'tipos', 'de', 'peixe', 'que', 'voce', 'nao', 'pode', 'comer', 'de', 'jeito', 'nenhum']\n",
            "['pabllo', 'vittar', 'vai', 'apresentar', 'programa', 'tv', 'crianca', 'gay', 'na', 'tv', 'globo']\n",
            "['trump', 'briga', 'com', 'governador', 'da', 'cidade', 'do', 'mexico', 'durante', 'evento', ',', 'mostra', 'video']\n",
            "['rita', 'lee', 'fez', 'homenagem', 'ao', 'rio', 'com', 'camiseta', '``', 'eu', 'sou', 'o', 'rio', \"''\", 'em', 'show', 'de', '2018']\n",
            "['proxima', 'novela', 'da', 'globo', 'tera', 'casamento', 'entre', 'avo', '(', 'jose', 'de', 'abreu', ')', 'e', 'neta']\n",
            "['biblia', 'previu', 'guerra', 'da', 'siria', 'do', 'seculo', 'xxi', 'no', 'livro', 'isaias', '17']\n",
            "['canais', 'de', 'veneza', 'ficam', 'congelados', 'por', 'causa', 'do', 'frio', ',', 'mostra', 'foto']\n",
            "['day', 'mccarthy', 'sofreu', 'acidente', 'de', 'carro', 'e', 'esta', 'entre', 'a', 'vida', 'e', 'a', 'morte']\n",
            "['estao', 'dando', 'kits', 'de', 'maquiagem', 'gratis', 'no', 'dia', 'internacional', 'da', 'mulher']\n",
            "['suzane', 'von', 'richthofen', 'vai', 'se', 'candidatar', 'a', 'deputada', 'federal', 'pelo', 'pt']\n",
            "['menina', 'siria', 'fecha', 'olhos', 'de', 'boneca', 'para', 'ela', 'nao', 'ver', 'guerra', ',', 'mostra', 'foto']\n",
            "['garota', 'siria', 'de', 'olhos', 'azuis', 'e', 'encontrada', 'sozinha', 'no', 'meio', 'da', 'guerra']\n",
            "['geddel', 'vieira', 'teve', 'um', 'infarto', 'e', 'gravou', 'video', 'denunciando', 'lula']\n",
            "['a', 'forma', 'da', 'agua', 'e', 'um', 'plagio', 'do', 'filme', 'o', 'pirarucu', 'tarado', 'e', 'a', 'mudinha']\n",
            "['pabllo', 'vittar', 'e', 'homenageada', 'pelo', 'mec', 'no', 'dia', 'internacional', 'da', 'mulher']\n",
            "['cristiano', 'ronaldo', 'leiloou', 'bolas', 'de', 'ouro', 'para', 'ajudar', 'criancas', 'na', 'siria']\n",
            "['casamento', 'da', 'filha', 'de', 'chico', 'pinheiro', 'teve', 'hino', 'do', 'atletico', '(', 'mg', ')', ',', 'mostra', 'video']\n",
            "['garota', 'de', '22', 'anos', 'vai', 'se', 'casar', 'com', 'fazendeiro', 'de', '72', 'em', 'manaus', '(', 'am', ')']\n",
            "['pabllo', 'vittar', 'assume', 'namoro', 'com', 'rene', 'junior', ',', 'jogador', 'do', 'corinthians']\n",
            "['marielle', 'franco', 'era', 'ex-mulher', 'de', 'marcinho', 'vp', 'e', 'engravidou', 'dele', 'aos', '16']\n",
            "['vereadora', 'marielle', 'franco', 'foi', 'eleita', 'pelo', 'comando', 'vermelho']\n",
            "['gisele', 'palhares', 'gouveia', 'foi', 'assassinada', 'ontem', ',', 'alerta', 'pastor', 'claudio', 'duarte']\n",
            "['foto', 'de', 'marielle', 'franco', 'sentada', 'no', 'colo', 'de', 'marcinho', 'vp', 'vazou', 'na', 'web']\n",
            "['generais', 'fecham', 'o', 'congresso', ',', 'acuam', 'o', 'stf', 'e', 'nao', 'havera', 'eleicoes']\n",
            "['renovacao', 'de', 'cnh', 'vai', 'exigir', 'curso', 'de', '10h', 'e', 'prova', 'teorica']\n",
            "['neymar', 'tem', 'divida', 'de', 'r', '$', '200', 'milhoes', 'com', 'a', 'receita', 'federal', 'perdoada', 'por', 'temer']\n",
            "['juiz', 'da', 'operacao', 'carne', 'fraca', 'e', 'encontrado', 'morto', 'no', 'parana']\n",
            "['7', 'capas', 'falsas', 'de', 'revistas', 'e', 'jornais', 'que', 'viralizaram', 'na', 'internet']\n",
            "['homem', 'e', 'condenado', 'a', 'morte', '15', 'minutos', 'apos', 'estuprar', 'menina', 'em', 'dubai']\n",
            "['foto', 'mostra', 'lula', 'com', 'o', 'rosto', 'sujo', 'apos', 'ser', 'atingido', 'por', 'ovos', 'em', 'comicio']\n",
            "['medico', 'alerta', 'para', 'bacteria', 'de', 'conjuntivite', 'viral', 'nivel', '3', 'sem', 'tratamento']\n",
            "['marcelo', 'freixo', 'posa', 'para', 'foto', 'com', 'traficantes', 'aliados']\n",
            "['tap', 'air', 'portugal', 'esta', 'dando', '2', 'ingressos', 'gratuitos', 'de', '75', 'aniversario']\n",
            "['diretor', 'mark', 'alerta', 'que', 'facebook', 'sera', 'fechado', 'e', 'pago', 'para', 'quem', 'nao', 'compartilhar', 'texto']\n",
            "['ricardo', 'almeida', 'diz', 'que', 'lula', 'foi', 'o', 'presidente', 'que', 'mais', 'ostentou', 'nos', 'ternos']\n",
            "['inri', 'cristo', 'morreu', ':', 'corpo', 'de', 'alvaro', 'thais', 'foi', 'encontrado', 'na', 'colombia']\n",
            "['rolex', 'esta', 'dando', 'relogios', 'de', 'graca', 'por', 'causa', 'de', '93', 'aniversario']\n",
            "['moca', 'perdeu', 'a', 'memoria', 'e', 'esta', 'perdida', 'no', 'posto', 'br', 'do', 'jacare', ',', 'mostra', 'video']\n",
            "['menina', 'sofia', 'vai', 'ganhar', 'r', '$', '1', 'por', 'compartilhamento', 'de', 'video', 'no', 'whatsapp']\n",
            "['temer', 'vai', 'apresentar', 'carta', 'renuncia', 'em', 'pronunciamento', 'da', 'tv']\n",
            "['ratinho', 'revela', 'quem', 'e', 'o', 'culpado', 'pela', 'morte', 'de', 'marielle', 'franco']\n",
            "['pastor', 'atira', 'mulher', 'no', 'chao', 'durante', 'culto', 'e', 'ela', 'fica', 'tetraplegica']\n",
            "['gleisi', 'hoffmann', 'colocou', 'miguelitos', 'nos', 'pneus', 'do', 'onibus', 'de', 'lula']\n",
            "['papa', 'francisco', 'disse', 'que', 'maior', 'crime', 'de', 'lula', 'foi', 'lutar', 'contra', 'a', 'fome']\n",
            "['cha', 'de', 'erva-doce', 'cura', 'a', 'gripe', 'h1n1', 'e', 'e', 'a', 'formula', 'do', 'tamiflu']\n",
            "['7', 'informacoes', 'falsas', '(', 'e', 'perigosas', ')', 'sobre', 'a', 'gripe', 'que', 'sempre', 'circulam', 'online']\n",
            "['nostradamus', 'previu', 'lula', 'no', 'livro', 'visao', 'das', 'trevas', ',', 'grandes', 'catastrofes', 'da', 'humanidade']\n",
            "['corinthians', 'faz', 'homenagem', 'a', 'lula', 'na', 'final', 'do', 'campeonato', 'paulista', '2018']\n",
            "['william', 'bonner', 'debocha', 'da', 'prisao', 'de', 'lula', 'junto', 'com', 'renata', 'vasconcellos', 'no', 'jn']\n",
            "['alface', 'do', 'mcdonalds', 'e', 'de', 'plastico', 'e', 'fabricada', 'artificialmente', ',', 'mostra', 'video']\n",
            "['furar', 'o', 'dedo', 'com', 'uma', 'agulha', 'salva', 'a', 'vida', 'de', 'quem', 'esta', 'tendo', 'um', 'avc', '(', 'derrame', ')']\n",
            "['roberto', 'carlos', 'diz', 'que', 'o', 'lugar', 'de', 'lula', 'e', 'na', 'presidencia', 'do', 'brasil']\n",
            "['7', 'textos', 'sobre', 'a', 'prisao', 'de', 'lula', 'que', 'foram', 'falsamente', 'atribuidos', 'a', 'famosos']\n",
            "['papa', 'francisco', 'cancela', 'a', 'biblia', 'e', 'propoe', 'a', 'criacao', 'de', 'um', 'novo', 'livro']\n",
            "['fotos', 'da', 'senadora', 'gleisi', 'hoffmann', ',', 'do', 'pt', ',', 'vazaram', 'na', 'internet']\n",
            "['crianca', 'de', '5', 'anos', 'morreu', 'por', 'causa', 'da', 'planta', 'amoena', '(', 'comigo-ninguem-pode', ')']\n",
            "['china', 'executou', '26', 'politicos', 'condenados', 'a', 'morte', 'por', 'corrupcao', ',', 'mostra', 'foto']\n",
            "['arnold', 'schwarzenegger', 'dormiu', 'embaixo', 'de', 'sua', 'estatua', 'apos', 'ser', 'recusado', 'em', 'hotel']\n",
            "['mark', 'zuckerberg', 'inventou', 'palavra', 'bff', 'para', 'verificar', 'conta', 'no', 'facebook']\n",
            "['mundo', 'arabe', 'da', 'resposta', 'ao', 'pedido', 'de', 'gleisi', 'hoffmann', 'em', 'video', 'da', 'al-jazira']\n",
            "['foto', 'mostra', 'lula', 'e', 'gleisi', 'hoffmann', 'se', 'beijando', 'em', 'publico']\n",
            "['atacadao', 'mercado', 'esta', 'dando', 'vale-compra', 'para', 'seguidores', 'no', 'instagram']\n",
            "['cuba', 'envia', 'medicos', ',', 'enfermeiros', 'e', 'medicamentos', 'para', 'siria', 'em', '2018']\n",
            "['jangadeiro', 'desmascara', 'lazaro', 'ramos', 'em', 'entrevista', 'censurada', 'pela', 'globo']\n",
            "['medica', 'de', 'unimed', 'diz', 'que', '100', '%', 'da', 'populacao', 'acima', 'dos', '30', 'anos', 'sera', 'atingida', 'pela', 'gripe', 'h2n3']\n",
            "['jornal', 'a', 'tarde', 'divulga', 'valor', 'das', 'novas', 'multas', 'valendo', 'a', 'partir', 'de', 'hoje']\n",
            "['brasil', 'tera', 'o', 'inverno', 'mais', 'frio', 'dos', 'ultimos', '100', 'anos', ',', 'diz', 'previsao']\n",
            "['turistas', 'sofrem', 'assalto', 'no', 'ceu', 'enquanto', 'voavam', 'de', 'parapente', 'no', 'rio']\n",
            "['mulher', 'da', 'a', 'luz', 'a', 'um', 'cachorro', '(', 'ou', 'cabra', ')', ',', 'mostra', 'video']\n",
            "['triplex', 'do', 'guaruja', 'nao', 'tem', 'elevador', 'privativo', 'e', 'nao', 'foi', 'reformado']\n",
            "['fantasma', 'aparece', 'em', 'cemiterio', 'e', 'se', 'esconde', 'atras', 'de', 'arvore', ',', 'mostra', 'video']\n",
            "['temer', 'aprova', 'o', 'fim', 'do', '13', 'e', 'do', 'fgts', 'de', 'todos', 'os', 'brasileiros']\n",
            "['lula', 'lanca', 'seu', 'filho', ',', 'lulinha', ',', 'como', 'candidato', 'a', 'presidente', 'em', '2018']\n",
            "['papa', 'diz', 'que', 'paredes', 'de', 'hospitais', 'ja', 'ouviram', 'preces', 'mais', 'honestas', 'do', 'que', 'igrejas']\n",
            "['joaquim', 'barbosa', 'e', 'contra', 'prisao', 'em', '2', 'instancia', 'e', 'fa', 'de', 'lula', 'e', 'dilma']\n",
            "['chaves', 'pode', 'deixar', 'de', 'ser', 'exibido', 'no', 'brasil', 'por', 'causa', 'de', 'bullying']\n",
            "['mst', 'incendeia', 'torre', 'de', 'internet', 'rural', ',', 'mostra', 'video']\n",
            "['emirates', 'coloca', 'desenho', 'de', 'lula', 'preso', 'na', 'parte', 'frontal', 'de', 'aviao']\n",
            "['ponto', 'vernal', 'e', 'mostrado', 'em', 'fotos', 'da', 'fronteira', 'china/tibet', 'a', '5.580', 'metros']\n",
            "['quem', 'ja', 'recebeu', 'bolsa', 'familia', 'e', 'tem', 'o', 'cartao', 'pode', 'ganhar', 'r', '$', '954', 'neste', 'mes']\n",
            "['bandido', 'ganha', 'r', '$', '4', 'mil', 'de', 'auxilio-reclusao', 'e', 'comete', 'assalto', ',', 'mostra', 'video']\n",
            "['saiu', 'o', 'mandado', 'de', 'prisao', 'de', 'jair', 'bolsonaro', 'por', 'crime', 'de', 'racismo']\n",
            "['lobisomem', 'ataca', 'na', 'regiao', 'norte', 'do', 'parana', 'e', 'e', 'flagrado']\n",
            "['garoto', 'atira', 'e', 'mata', 'outra', 'crianca', 'em', 'bicicleta', ',', 'mostra', 'video']\n",
            "['cha', 'de', 'folhas', 'de', 'graviola', 'cura', 'cancer', ',', 'afirma', 'medico', 'oncologista']\n",
            "['brigadeiro', 'suspende', 'voos', 'de', 'gilmar', 'mendes', 'em', 'avioes', 'da', 'fab']\n",
            "['ladroes', 'sao', 'atacados', 'por', 'marimbondos', 'apos', 'roubo', 'a', 'mao', 'armada']\n",
            "['tribunal', 'de', 'haia', 'acaba', 'de', 'reconhecer', 'lula', 'como', 'preso', 'politico']\n",
            "['video', 'mostra', 'encontro', 'de', 'oceano', 'pacifico', 'e', 'oceano', 'atlantico', ',', 'que', 'nao', 'se', 'misturam']\n",
            "['mataram', 'dinho', 'kapp', ',', 'o', 'papa', 'capim', ',', 'a', 'tiros', ',', 'mostram', 'fotos']\n",
            "['maquina', 'separa', 'bolinhas', 'por', 'cores', 'por', 'causa', 'da', 'fisica', 'quantica']\n",
            "['pastor', 'do', 'juiz', 'sergio', 'moro', 'diz', 'que', 'ele', 'esta', 'sendo', 'ameacado', 'de', 'morte']\n",
            "['palocci', 'divulga', ',', 'em', 'delacao', 'premiada', ',', 'video', 'de', 'lula', 'que', 'estava', 'guardado', 'a', 'sete', 'chaves']\n",
            "['banco', 'central', 'proibe', 'rede', 'bancaria', 'de', 'receber', 'notas', 'com', 'carimbo', 'lula', 'livre']\n",
            "['bebedouro', 'tem', 'sistema', 'que', 'retorna', '35', '%', 'da', 'agua', 'desperdicada', 'para', 'ser', 'bebida', 'de', 'novo']\n",
            "['video', 'mostra', 'parque', 'de', 'diversoes', 'na', 'china', 'com', 'brinquedos', 'surreais']\n",
            "['boulos', 'e', 'o', 'mtst', 'cobravam', 'aluguel', 'em', 'predio', 'que', 'desabou', 'em', 'sao', 'paulo']\n",
            "['inglaterra', 'coloca', 'rosto', 'de', 'ayrton', 'senna', 'nas', 'notas', 'de', '10', 'libras']\n",
            "['el', 'pais', 'diz', 'que', 'o', 'corinthians', 'e', 'o', 'time', 'mais', 'ajudado', 'pela', 'arbitragem', 'no', 'mundo']\n",
            "['leite', 'elege', 'e', 'retirado', 'do', 'mercado', 'por', 'excesso', 'de', 'sangue', ',', 'pus', 'e', 'toxinas']\n",
            "['virus', 'h3n2', 'nao', 'tem', 'vacina', ',', 'nao', 'tem', 'cura', 'e', 'ja', 'matou', 'medico', 'em', 'recife']\n",
            "['quem', 'baixar', 'a', 'cnh', 'digital', 'e', 'atingir', '20', 'pontos', 'perde', 'carteira', 'automaticamente']\n",
            "['mcdonalds', 'esta', 'dando', 'um', 'cupom', 'de', 'r', '$', '50', 'para', 'quem', 'entrar', 'em', 'site']\n",
            "['cantor', 'gospel', 'feliciano', 'amaral', 'morreu', 'aos', '98', 'anos', 'apos', 'ser', 'internado']\n",
            "['ricardo', ',', 'que', 'morreu', 'em', 'predio', 'que', 'desabou', ',', 'era', 'do', 'pcc', 'e', 'matador', 'de', 'policiais']\n",
            "['assaltantes', 'matadores', 'de', 'motoristas', 'de', 'uber', 'postam', 'video', 'ostentando', 'armas']\n",
            "['lei', 'marielle', ',', 'que', 'preve', 'cota', 'de', '20', '%', 'para', 'menores', 'infratores', 'em', 'concursos', ',', 'e', 'aprovada']\n",
            "['zibia', 'gasparetto', ',', 'escritora', 'espiritualista', ',', 'morreu', 'aos', '91', 'anos']\n",
            "['lula', 'teve', 'crise', 'de', 'abstinencia', 'de', 'alcool', 'na', 'sede', 'da', 'pf', 'e', 'quase', 'morreu']\n",
            "['manuela', 'davila', 'diz', 'que', 'abortar', 'e', 'a', 'saida', 'para', 'nao', 'criar', 'filho', 'de', 'vagabundo']\n",
            "['briga', 'de', 'transito', 'em', 'catanduva', 'resulta', 'na', 'morte', 'de', 'homens', ',', 'mostra', 'video']\n",
            "['marvel', 'esta', 'dando', 'ingressos', 'gratis', 'para', 'vingadores', ':', 'guerra', 'infinita', 'em', 'site']\n",
            "['cerveja', 'heineken', 'patrocina', 'briga', 'de', 'caes', 'em', 'rinha', 'na', 'mongolia']\n",
            "['andar', 'em', 'escada', 'rolante', 'parada', 'e', 'um', 'perigo', ',', 'mostra', 'video']\n",
            "['servico', 'medico', 'no', 'iaserj/maracana', 'vai', 'fechar', 'por', 'falta', 'de', 'pacientes']\n",
            "['alexandre', 'frota', 'virou', 'motorista', 'da', 'uber', 'por', 'nao', 'ter', 'emprego', 'na', 'tv']\n",
            "['mulher', 'e', 'presa', 'por', 'vender', 'espetinho', 'de', 'carne', 'de', 'cachorro', 'em', 'cidade', 'no', 'brasil']\n",
            "['programa', 'habilitacao', 'social', '2018', 'da', 'carteira', 'de', 'motorista', 'gratis', 'no', 'whatsapp']\n",
            "['anatep', 'mandou', 'ascenty', 'diminuir', 'intensidade', 'de', 'giga-hertz', 'e', 'cortar', 'internet', 'no', 'brasil']\n",
            "['bruno', 'maranhao', 'do', 'mst', 'foi', 'preso', 'em', 'flagrante', 'com', 'armas', 'e', 'drogas']\n",
            "['oculos', 'de', 'sol', '(', 'de', 'qualquer', 'modelo', ')', 'ajuda', 'a', 'dirigir', 'sob', 'chuva', 'forte', 'e', 'a', 'noite']\n",
            "['direitos', 'humanos', 'pedem', 'prisao', 'de', 'mae', 'pm', 'que', 'matou', 'ladrao', 'em', 'escola', 'de', 'sp']\n",
            "['desafio', 'das', '72', 'horas', '(', 'game', 'of', '72', ')', 'e', 'um', 'novo', 'jogo', 'que', 'faz', 'adolescentes', 'desaparecerem']\n",
            "['lulinha', 'e', 'genro', 'de', 'daniel', 'dantas', ',', 'dono', 'da', 'fazenda', 'santa', 'barbara']\n",
            "['antonio', 'fagundes', 'apanha', 'de', 'policial', 'em', 'posto', 'de', 'gasolina', ',', 'mostra', 'video']\n",
            "['taurus', 'spectrum', '380', 'e', 'uma', 'mini', 'pistola', 'utilizada', 'por', 'meninos', 'em', 'assaltos', 'no', 'brasil']\n",
            "['senadora', 'regina', 'souza', '(', 'celia', ')', 'defende', 'aborto', 'para', 'crianca', 'nao', 'nascer', 'com', 'rinite', 'ou', 'sinusite']\n",
            "['filha', 'briga', 'para', 'ganhar', 'fortuna', 'de', 'bilhoes', 'de', 'fidel', 'castro', 'e', 'tem', 'mansao', 'de', 'us', '$', '17', 'mi']\n",
            "['limao', 'na', 'agua', 'quente', 'mata', 'celulas', 'cancerigenas', ',', 'diz', 'tchen', 'horin']\n",
            "['maduro', 'acena', 'para', 'ninguem', 'em', 'praca', 'vazia', 'na', 'venezuela', ',', 'mostra', 'video']\n",
            "['arnaldo', 'jabor', 'diz', 'que', 'o', 'brasileiro', 'e', 'babaca', 'em', 'cronica', 'inteligente']\n",
            "['botijao', 'de', 'gas', 'explode', 'apos', 'dois', 'homens', 'baterem', 'nele']\n",
            "['video', 'mostra', 'chuva', 'de', 'granizo', 'com', 'tamanho', 'de', 'uma', 'bola', 'de', 'tenis', 'ontem', 'na', 'australia']\n",
            "['bill', 'gates', 'diz', 'que', 'vacinas', 'servem', 'para', 'esterilizar', ',', 'matar', 'e', 'reduzir', 'populacao', 'mundial']\n",
            "['pauta', 'dos', 'caminhoneiros', 'pede', 'renuncia', 'de', 'temer', 'e', 'eleicoes', 'antecipadas']\n",
            "['caminhoes', 'estao', 'cheios', 'de', 'carne', 'descongeladas', '!', 'nao', 'compre', 'apos', 'a', 'greve']\n",
            "['governo', 'vai', 'lancar', 'plano', 'michel', 'e', 'congelar', 'poupancas', 'e', 'contas-correntes']\n",
            "['pcc', 'anuncia', 'apoio', 'a', 'greve', 'dos', 'caminhoneiros', 'e', 'ameaca', 'quem', 'sair', 'de', 'casa']\n",
            "['deputado', 'edinho', 'bez', 'gravou', 'audio', 'sobre', 'greve', 'e', 'renuncia', 'de', 'temer']\n",
            "['lulu', 'santos', 'escreve', 'carta', 'ao', 'diario', 'de', 'sao', 'paulo', 'sobre', 'o', 'inss', 'e', 'deputados']\n",
            "['temer', ',', 'a', 'pedido', 'de', 'rodrigo', 'maia', ',', 'decreta', 'estado', 'de', 'sitio', 'por', 'causa', 'de', 'greve']\n",
            "['emilio', 'dalcoquio', 'diz', 'que', 'greve', 'e', 'para', 'o', 'povo', 'e', 'nao', 'para', 'os', 'caminhoneiros', 'em', 'video']\n",
            "['policia', 'federal', 'esta', 'atirando', 'nos', 'caminhoneiros', 'durante', 'greve', 'de', '2018', ',', 'mostra', 'video']\n",
            "['crianca', 'morre', 'apos', 'comer', 'ovo', 'de', 'cobra', 'achando', 'que', 'era', 'ovo', 'de', 'codorna']\n",
            "['video', 'mostra', 'assalto', 'a', 'motoristas', 'durante', 'arrastao', 'em', 'campinas', '(', 'sp', ')']\n",
            "['caminhoneiro', 'e', 'morto', 'por', 'tiro', 'de', 'bala', 'de', 'borracha', 'da', 'prf', 'na', 'br-101', '(', 'bahia', ')']\n",
            "['general', 'villas', 'boas', 'anuncia', 'intervencao', 'militar', 'e', 'convoca', 'povo', 'do', 'brasil']\n",
            "['juizes', 'e', 'desembargadores', 'do', 'tjdft', 'recebem', 'auxilio-moradia', 'de', 'r', '$', '437', 'mil']\n",
            "['arnaldo', 'jabor', 'escreve', 'texto', '``', 'militar', 'e', 'incompetente', 'demais', '!', 'militares', 'nunca', 'mais', \"''\"]\n",
            "['caminhoneiros', 'invadem', 'a', 'esplanada', 'dos', 'ministerios', 'em', 'brasilia', 'hoje', ',', 'mostra', 'video']\n",
            "['foto', 'mostra', 'buraco', 'gigante', 'feito', 'por', 'meteorito', 'em', 'pirassununga', ',', 'interior', 'de', 'sao', 'paulo']\n",
            "['mulher', 'pula', 'de', 'bungee', 'jump', 'sem', 'cordas', 'por', 'distracao', 'do', 'instrutor', ',', 'mostra', 'video']\n",
            "['justica', 'ordena', 'bloqueio', 'do', 'whatsapp', 'por', 'dois', 'dias', 'a', 'partir', 'da', 'meia-noite']\n",
            "['ivete', 'sangalo', 'posa', 'com', 'camiseta', 'lula', 'livre', 'em', 'evento']\n",
            "['filho', 'de', 'lula', 'dirige', 'camaro', ',', 'fala', 'do', 'preco', 'da', 'gasolina', 'e', 'zomba', 'dos', 'brasileiros']\n",
            "['procon', 'diz', 'que', 'o', 'preco', 'do', 'gas', 'de', 'cozinha', 'nao', 'pode', 'passar', 'de', 'r', '$', '65']\n",
            "['assassino', 'de', 'marielle', 'foi', 'descoberto', '!', 'e', 'thiago', 'macaco', ',', 'preto', ',', 'pobre', 'e', 'traficante']\n",
            "['leite', 'godam', 'esta', 'envenenado', 'e', 'causa', 'doencas', 'em', 'pessoas', 'e', 'animais']\n",
            "['ondas', 'do', 'mar', 'na', 'siberia', '(', 'russia', ')', 'congelam', 'a', '-62', 'c', ',', 'mostra', 'video']\n",
            "['stj', 'autoriza', 'cancelamento', 'definitivo', 'da', 'cnh', 'em', 'caso', 'de', 'ipva', 'atrasado']\n",
            "['neymar', 'faz', 'gesto', 'obsceno', 'em', 'foto', 'oficial', 'da', 'selecao', 'brasileira', 'para', 'a', 'copa']\n",
            "['selecao', 'brasileira', 'posa', 'com', 'mensagem', 'lula', 'livre', 'em', 'foto', 'oficial', 'da', 'copa']\n",
            "['papa', 'francisco', 'enviou', 'rosario', 'de', 'presente', 'para', 'lula', ',', 'preso', 'em', 'curitiba']\n",
            "['neta', 'de', 'roberto', 'carlos', 'canta', 'a', 'montanha', '(', 'obrigado', ',', 'senhor', ')', ',', 'mostra', 'video']\n",
            "['feministas', 'invadem', 'igreja', 'e', 'defecam', 'durante', 'protesto']\n",
            "['celular', 'pega', 'fogo', 'no', 'bolso', 'de', 'jovem', 'em', 'shopping', ',', 'mostra', 'video']\n",
            "['selecao', 'da', 'alemanha', 'viajou', 'para', 'a', 'russia', 'em', 'voo', 'comercial', 'na', 'classe', 'economica']\n",
            "['motoqueiro', 'sofre', 'acidente', ',', 'faz', 'profecia', 'e', 'morre', 'em', 'valparaiso', '(', 'go', ')']\n",
            "['arrastao', 'na', 'avenida', 'radial', 'leste', ',', 'em', 'sao', 'paulo', ',', 'e', 'flagrado', 'em', 'video']\n",
            "['brasileira', 'marcela', 'pereira', 'ficou', 'em', '2', 'lugar', 'no', 'concurso', 'de', 'fisica', 'nuclear', 'na', 'russia']\n",
            "['ovos', 'de', 'plastico', 'sao', 'fabricados', 'na', 'china', 'e', 'vendidos', 'como', 'reais', ',', 'mostra', 'video']\n",
            "['neymar', 'esta', 'fora', 'da', 'copa', 'do', 'mundo', '2018', ',', 'confirma', 'cbf']\n",
            "['ponte', 'pedra', 'do', 'cavalo', ',', 'na', 'bahia', ',', 'esta', 'prestes', 'a', 'cair', 'por', 'causa', 'de', 'rachaduras']\n",
            "['junior', 'tavares', ',', 'jogador', 'do', 'sao', 'paulo', ',', 'e', 'flagrado', 'beijando', 'david', 'brazil']\n",
            "['marcus', 'vinicius', ',', 'estudante', 'morto', 'na', 'mare', ',', 'aparece', 'em', 'foto', 'com', 'arma', 'na', 'mao']\n",
            "['homem', 'contrata', 'garota', 'e', 'descobre', 'que', 'ela', 'e', 'sua', 'esposa']\n",
            "['novo', 'supermercado', 'drive-thru', 'sera', 'inaugurado', 'em', 'dubai', 'em', '2019', ',', 'mostra', 'video']\n",
            "['cachorro', 'arranca', 'penis', 'de', 'pedofilo', 'durante', 'estupro', 'nos', 'eua']\n",
            "['torcedora', 'argentina', 'sofre', 'com', 'violencia', 'policial', 'em', 'moscou']\n",
            "['pontos', 'pretos', 'nas', 'batatas', 'sao', 'substancias', 'cancerigenas']\n",
            "['bandeira', 'do', 'brasil', 'foi', 'formada', 'pela', 'natureza', 'no', 'rio', 'amazonas', ',', 'mostra', 'foto']\n",
            "['um', 'dos', 'irmaos', 'siameses', 'e', 'gay', 'e', 'outro', 'e', 'hetero', ',', 'mostra', 'foto']\n",
            "['claudia', 'abreu', 'diz', 'que', 'foi', 'torturada', 'na', 'ditadura', 'quando', 'tinha', '5', 'anos']\n",
            "['aecio', 'neves', 'e', 'flagrado', 'beijando', 'sergio', 'moro', ',', 'mostra', 'foto']\n",
            "['avioes', 'pintam', 'o', 'ceu', 'de', 'moscou', 'com', 'as', 'cores', 'do', 'time', 'vencedor', 'apos', 'jogos', 'da', 'copa']\n",
            "['video', 'mostra', 'resgate', 'das', 'criancas', 'em', 'caverna', 'na', 'tailandia']\n",
            "['gol', 'esta', 'dando', 'passagens', 'aereas', 'gratis', 'nesse', 'meio', 'de', 'ano']\n",
            "['fotos', 'mostram', 'kolinda', 'grabar', ',', 'presidente', 'da', 'croacia', ',', 'de', 'biquini', 'na', 'praia']\n",
            "['neymar', 'e', 'thiago', 'silva', 'fazem', 'festa', 'em', 'barco', 'apos', 'derrota', 'do', 'brasil', 'na', 'copa']\n",
            "['carros', 'batem', 'em', 'barreiras', 'invisiveis', 'e', 'sofrem', 'acidentes', 'em', 'toluca', '(', 'mexico', ')']\n",
            "['moro', 'fretou', 'aviao', 'de', 'portugal', 'para', 'o', 'brasil', 'para', 'tratar', 'de', 'prisao', 'de', 'lula']\n",
            "['eduardo', 'bolsonaro', 'diz', 'que', 'curtiu', 'a', 'foto', 'de', 'lula', ',', 'mas', 'foi', 'sem', 'querer']\n",
            "['fotos', 'mostram', 'mansao', 'de', 'edir', 'macedo', 'em', 'campos', 'do', 'jordao', '(', 'sp', ')']\n",
            "['croacia', 'vendeu', 'a', 'copa', 'do', 'mundo', 'para', 'a', 'fifa', 'e', 'entregou', 'jogo', 'contra', 'franca']\n",
            "['croacia', 'vai', 'doar', 'premio', 'ganho', 'na', 'copa', 'para', 'criancas', 'carentes', ',', 'diz', 'tecnico']\n",
            "['lei', 'federal', '274.2022', 'proibe', 'reboque', 'de', 'veiculos', 'apos', 'infracao', 'de', 'transito']\n",
            "['pre-candidato', 'chico', 'leite', 'diz', 'que', 'quer', 'prisao', 'de', 'moro', 'em', '24', 'horas']\n",
            "['catolicos', 'podem', 'ser', 'presos', 'e', 'mortos', 'na', 'nicaragua', 'a', 'partir', 'de', 'agora']\n",
            "['veganos', 'nazistas', 'fundaram', 'movimento', 'veganista', 'emergente', 'do', 'chile']\n",
            "['acai', 'e', 'ensacado', 'sem', 'higiene', 'e', 'moido', 'com', 'os', 'pes', ',', 'mostra', 'video']\n",
            "['mulher', 'atira', 'em', 'outra', 'apos', 'discussao', 'em', 'festa', 'na', 'piscina']\n",
            "['repassem', 'video', 'de', 'mulher', 'espancando', 'bebes', 'para', 'que', 'ela', 'seja', 'presa']\n",
            "['flamengo', 'foi', 'rebaixado', 'no', 'brasileirao', 'de', '1995', ',', 'mas', 'cbf', 'fez', 'manobra']\n",
            "['cantor', 'diz', 'que', 'jesus', 'e', 'gay', 'e', 'travesti', 'durante', 'festival', 'lula', 'livre']\n",
            "['familia', 'sarney', 'fraudou', 'mega-sena', 'para', 'ganhar', 'premio', 'acumulado']\n",
            "['whatsapp', 'sinaliza', 'mensagem', 'como', 'encaminhada', 'para', 'poder', 'punir', 'usuarios']\n",
            "['elefante', 'carrega', 'filhote', 'de', 'leao', 'na', 'tromba', ',', 'mostra', 'foto']\n",
            "['video', 'mostra', 'movimento', 'da', 'crosta', 'terrestre', 'na', 'italia', 'que', 'ocorreu', 'ontem']\n",
            "['pastor', 'faz', 'perna', 'de', 'crianca', 'crescer', 'com', 'oracao', 'e', 'milagre', ',', 'mostra', 'video']\n",
            "['geraldo', 'alckmin', 'vai', 'apoiar', 'candidato', 'nei', 'santos', ',', 'do', 'pcc', ',', 'nestas', 'eleicoes']\n",
            "['video', 'mostra', 'bruce', 'lee', 'jogando', 'tenis', 'de', 'mesa', 'com', 'munchaku', 'em', '1970']\n",
            "['corpo', 'da', 'policial', 'feminina', 'juliane', 'e', 'encontrado', 'carbonizado', ',', 'mostram', 'fotos']\n",
            "['professor', 'expulsa', 'aluno', 'por', 'vestir', 'camiseta', 'de', 'bolsonaro', 'em', 'sala', 'de', 'aula']\n",
            "['gilmar', 'mendes', 'autoriza', 'lula', 'a', 'fazer', 'campanha', 'eleitoral', 'pelo', 'brasil']\n",
            "['bolsonaro', 'foi', 'preso', 'em', '1987', 'e', 'expulso', 'do', 'exercito', 'por', 'planejar', 'atentado']\n",
            "['foto', 'mostra', 'praia', 'lotada', 'na', 'china', 'durante', 'o', 'verao']\n",
            "['mulheres', 'usam', 'oleo', 'do', 'apagao', 'para', 'sequestrar', 'criancas', 'e', 'tirar', 'orgaos']\n",
            "['diretor', 'da', 'globo', 'carlos', 'henrique', 'schroeder', 'se', 'demitiu', 'apos', 'entrevista', 'de', 'bolsonaro']\n",
            "['dossie', 'ursal', 'denuncia', 'uniao', 'das', 'republicas', 'socialistas', 'da', 'america', 'latina']\n",
            "['olavo', 'de', 'carvalho', 'disse', 'subestime', 'a', 'religiao', 'socialista', ',', 'e', 'eles', 'vao', 'arrebentar', 'voce']\n",
            "['roseana', 'sarney', 'cai', 'em', 'rio', 'apos', 'ponte', 'desabar', 'em', 'sao', 'luis', '(', 'ma', ')']\n",
            "['disco', 'voador', 'e', 'flagrado', 'em', 'bocaiuva', 'do', 'sul', '(', 'pr', ')', ',', 'mostra', 'video']\n",
            "['ficha', 'de', 'terrorista', 'de', 'dilma', 'rousseff', '(', 'e', 'de', 'serra', ')', 'vaza', 'na', 'internet']\n",
            "['guilherme', 'boulos', 'era', 'professor', 'e', 'funcionario', 'fantasma', 'da', 'usp']\n",
            "['bandidos', 'fazem', 'assalto', 'em', 'igreja', 'de', 'ipanema', '(', 'rio', ')', 'durante', 'missa']\n",
            "['walmart', 'coloca', 'placa', 'em', 'gondola', 'que', 'diz', 'um', 'bebe', 'nunca', 'e', 'um', 'erro']\n",
            "['jabor', 'diz', 'que', 'amoedo', 'iria', 'para', 'o', '2', 'turno', 'se', 'tivesse', 'votos', 'de', 'quem', 'diz', 'que', 'ele', 'e', 'o', 'melhor']\n",
            "['adidas', 'busca', 'influenciadores', 'para', 'ganhar', 'presentes', 'e', 'postar', 'no', 'instagram']\n",
            "['nicolas', 'maduro', 'acaba', 'de', 'cair', 'e', 'venezuela', 'entra', 'em', 'guerra']\n",
            "['dilma', 'comete', 'gafe', 'com', 'manifestantes', 'que', 'fazem', 'greve', 'de', 'fome']\n",
            "['silvio', 'santos', 'cita', 'bolsonaro', 'como', '``', 'proximo', 'presidente', \"''\", 'e', 'declara', 'apoio']\n",
            "['coca-cola', 'vai', 'deixar', 'o', 'brasil', 'por', 'causa', 'do', 'aumento', 'de', 'imposto']\n",
            "['foto', 'mostra', 'marina', 'silva', 'invadindo', 'fazenda', 'com', 'o', 'mst', 'em', '1986']\n",
            "['junior', 'cigano', 'briga', 'em', 'bar']\n",
            "['exercito', 'matou', 'dezenas', 'de', 'bandidos', 'no', 'rio', 'de', 'janeiro']\n",
            "['capa', 'do', 'kit', 'gay', 'de', 'haddad', 'vaza', 'na', 'internet']\n",
            "['caixa', 'libera', 'o', 'saque', 'do', 'fgts', 'para', 'quem', 'trabalhou', 'entre', '2005', 'e', '2018']\n",
            "['livro', 'aparelho', 'sexual', 'e', 'cia', 'faz', 'parte', 'do', 'kit', 'gay', 'distribuido', 'em', 'escolas']\n",
            "['mcdonalds', 'esta', 'dando', 'cupons', 'gratuitos', 'para', '2', 'combos', 'de', '78', 'aniversario']\n",
            "['bandidos', 'roubam', 'bagageiro', 'de', 'onibus', 'em', 'movimento', 'no', 'rio', 'de', 'janeiro']\n",
            "['aviao', 'russo', 'sukhoi', '30', 'faz', 'acrobacias', 'com', 'danca', 'no', 'solo', 'em', 'video']\n",
            "['arnaldo', 'jabor', 'diz', 'que', 'bolsonaro', 'nao', 'e', 'o', 'candidato', 'ideal', ',', 'mas', 'que', 'vai', 'votar', 'nele']\n",
            "['algoritmo', 'do', 'facebook', 'mudou', 'e', 'so', 'mostra', 'posts', 'de', '25', 'amigos']\n",
            "['presos', 'abrem', 'conta', 'no', 'banco', 'do', 'brasil', 'para', 'ganhar', 'bolsa', 'presidiario', ',', 'mostram', 'fotos']\n",
            "['nova', 'greve', 'dos', 'caminhoneiros', 'vai', 'acontecer', 'apos', 'o', 'feriado']\n",
            "['menina', 'de', '11', 'anos', 'foi', 'sequestrada', 'em', 'marica', 'e', 'esta', 'desaparecida']\n",
            "['bolsonaro', 'diz', 'que', 'nordestino', 'e', 'tao', 'burro', 'que', 'nao', 'consegue', 'falar', 'haddad']\n",
            "['video', 'mostra', 'alisson', ',', 'goleiro', 'da', 'selecao', ',', 'com', 'tres', 'mulheres']\n",
            "['notas', 'de', '100', 'dolares', 'velhas', 'deixarao', 'de', 'circular', 'e', 'nao', 'valerao', 'em', '2019']\n",
            "['manuela', \"d'avila\", ',', 'vice', 'de', 'haddad', ',', 'tem', 'tatuagens', 'de', 'che', 'guevara', 'e', 'lenin']\n",
            "['estatua', 'de', 'baphomet', 'foi', 'inaugurada', 'em', 'oklahoma', ',', 'nos', 'estados', 'unidos']\n",
            "['william', 'bonner', 'fala', 'que', 'homem', 'morreu', 'e', 'ja', 'teve', 'alta', 'durante', 'jornal', 'nacional']\n",
            "['video', 'mostra', 'animal', 'que', 'e', 'cruzamento', 'de', 'porco', 'com', 'ovelha']\n",
            "['estao', 'dando', 'maquina', 'de', 'cafe', 'dolce', 'gusto', 'gratis', 'em', 'site', 'no', 'whatsapp']\n",
            "['fernanda', 'montenegro', 'diz', 'que', 'mulher', 'que', 'se', 'respeita', 'nao', 'vota', 'em', 'bolsonaro']\n",
            "['ivete', 'sangalo', 'apoia', 'bolsonaro', 'e', 'posa', 'com', 'camiseta', '``', 'bolsonaro', 'presidente', \"''\"]\n",
            "['no', 'mineirao', ',', 'torcidas', 'de', 'atletico', 'e', 'cruzeiro', 'fazem', 'homenagem', 'a', 'bolsonaro']\n",
            "['vaza', 'audio', 'de', 'bolsonaro', 'no', 'hospital', 'xingando', 'enfermeira', ',', 'ibope', 'e', '``', 'teatro', \"''\"]\n",
            "['datafolha', 'esta', 'fazendo', 'pesquisa', 'eleitoral', 'em', 'site', 'no', 'whatsapp']\n",
            "['adelio', 'bispo', 'e', 'flagrado', 'com', 'lula', ',', 'dilma', ',', 'gleisi', ',', 'boulos', 'e', 'marinho', 'em', 'foto']\n",
            "['flavio', 'bolsonaro', 'diz', 'que', 'garcom', 'morto', 'pela', 'policia', 'correu', 'risco', 'por', 'usar', 'guarda-chuva']\n",
            "['sandy', 'apoia', 'bolsonaro', 'e', 'diz', 'que', 'concorda', 'com', 'candidato', 'do', 'psl']\n",
            "['aviao', 'rodopia', 'no', 'ar', 'durante', 'o', 'tufao', 'mangkhut', 'nas', 'filipinas']\n",
            "['alto', 'comando', 'do', 'exercito', 'fara', 'pericia', 'nas', 'urnas', 'eletronicas', 'nas', 'eleicoes']\n",
            "['rodrigo', 'santoro', 'apoia', 'bolsonaro', 'e', 'posa', 'com', 'camiseta', 'de', 'apoio', 'a', 'candidato']\n",
            "['ratinho', 'vai', 'entrevistar', 'bolsonaro', 'no', 'dia', 'do', 'debate', 'da', 'globo', ',', '04/10']\n",
            "['bolsonaro', 'votou', 'contra', 'a', 'lei', 'brasileira', 'de', 'inclusao', '(', 'lbi', ')', 'na', 'camara']\n",
            "['jean', 'wyllys', ',', 'maria', 'do', 'rosario', 'e', 'gleisi', 'nao', 'se', 'elegeram', 'e', 'estao', 'fora']\n",
            "['haddad', 'diz', 'que', 'a', 'igreja', 'vai', 'pagar', 'caro', 'por', 'prender', 'lula', ',', 'o', 'filho', 'de', 'deus']\n",
            "['haddad', 'disse', 'hoje', 'que', ',', 'se', 'ele', 'for', 'eleito', ',', 'lula', 'vai', 'subir', 'a', 'rampa', 'e', 'sera', 'presidente']\n",
            "['filho', 'de', 'bolsonaro', 'xinga', 'nordestinos', 'que', 'votaram', 'no', 'pt', 'em', 'video']\n",
            "['fernando', 'gabeira', 'diz', 'que', 'bolsonaro', 'pode', 'ser', 'limitado', ',', 'mas', 'e', 'um', 'patriota']\n",
            "['temer', 'declara', 'apoio', 'a', 'haddad', 'no', '2', 'turno', 'das', 'eleicoes', '2018']\n",
            "['sergio', 'moro', 'diz', 'que', 'aceita', 'ser', 'ministro', 'da', 'justica', 'de', 'bolsonaro']\n",
            "['equipe', 'de', 'haddad', 'tera', 'cortella', ',', 'drauzio', ',', 'ciro', 'e', 'barbosa', 'em', 'ministerios']\n",
            "['bolsonaro', 'votou', 'contra', 'aprovacao', 'da', 'lei', 'maria', 'da', 'penha', 'na', 'camara']\n",
            "['chef', 'faz', 'gesto', 'para', 'bolsonaro', 'e', 'seus', 'eleitores']\n",
            "['fonte', 'no', 'japao', 'em', 'forma', 'de', 'mulher', 'estarrece', 'o', 'mundo', 'com', 'seu', 'design']\n",
            "['havaianas', 'da', 'um', 'par', 'de', 'sandalias', 'gratis', 'em', 'site', 'no', 'whatsapp']\n",
            "['amelia', 'teles', 'matou', 'e', 'esquartejou', 'militares', 'durante', 'a', 'ditadura']\n",
            "['tupperware', 'da', 'garrafinha', 'de', '500ml', 'em', 'site', 'no', 'whatsapp']\n",
            "['bolsonaro', 'pode', 'perder', 'o', 'mandato', 'e', 'ser', 'cassado', 'antes', 'de', 'ser', 'presidente']\n",
            "['lula', 'e', 'dono', 'de', '52', '%', 'das', 'acoes', 'da', 'folha', 'de', 's.paulo']\n",
            "['o', 'boticario', 'esta', 'dando', 'perfume', 'love', 'lily', 'em', 'site', 'no', 'whatsapp']\n",
            "['tempestade', 'causa', 'panico', 'no', 'bar', 'baden', 'baden', 'em', 'campos', 'do', 'jordao', '(', 'sp', ')']\n",
            "['orkut', 'liberou', 'todos', 'os', 'perfis', ',', 'fotos', 'e', 'publicacoes', 'por', 'tempo', 'limitado']\n",
            "['vote', 'nao', 'na', 'peticao', 'para', 'impugnar', 'bolsonaro', 'nopeticaopublica.org']\n",
            "['homem', 'cai', 'do', 'ceu', '(', 'ou', 'de', 'aviao', ')', 'em', 'estacionamento']\n",
            "['paulo', 'guedes', 'diz', 'a', 'jornalista', 'que', 'vai', 'privatizar', 'c', '...', 'de', 'curioso']\n",
            "['unicamp', 'distribui', 'soro', 'para', 'picada', 'de', 'escorpiao', 'no', 'telefone', '(', '19', ')', '99685-8936']\n",
            "['tornado', 'de', 'fogo', 'em', 'toronto', '(', 'canada', ')', 'e', 'retratado', 'em', 'video', 'de', '1', 'milhao']\n",
            "['alexandre', 'frota', 'vai', 'assumir', 'controle', 'da', 'prova', 'do', 'enem']\n",
            "['niobio', 'pode', 'salvar', 'o', 'brasil', 'e', 'nos', 'tornar', 'o', 'pais', 'mais', 'rico', 'do', 'mundo']\n",
            "['daniela', 'mercury', 'ganhou', 'r', '$', '13,9', 'milhoes', 'da', 'lei', 'rouanet', 'para', 'aderir', 'ao', '#', 'elenao']\n",
            "['paul', 'mccartney', 'veste', 'camiseta', 'de', 'apoio', 'a', 'lula', 'e', 'elogia', 'ex-presidente']\n",
            "['medicos', 'cubanos', 'nao', 'tem', 'formacao', 'medica', 'e', 'sao', 'apenas', 'feldsher']\n",
            "['carlos', 'moreno', ',', 'garoto-propaganda', 'da', 'bombril', ',', 'morreu', 'hoje']\n",
            "['chapecoense', 'e', 'america', '(', 'mg', ')', 'escalaram', 'jogadores', 'irregulares', 'no', 'brasileirao']\n",
            "['michelle', 'bolsonaro', 'interpreta', 'pai', 'nosso', 'em', 'libras', ',', 'mostra', 'video']\n",
            "['natal', 'de', 'algodao', 'dara', 'uma', 'peca', 'de', 'roupa', 'por', 'compartilhamento', 'de', 'post', 'no', 'facebook']\n",
            "['crianca', 'encontrada', 'em', 'cascavel', 'procura', 'familia', '.', 'compartilhe', 'a', 'foto', 'dele']\n",
            "['japao', 'testa', 'fogos', 'de', 'artificio', 'com', 'design', ',', 'mostra', 'video']\n",
            "['coca-cola', 'esta', 'dando', 'bolsa', 'termica', 'em', 'site', 'de', 'promocao', 'de', 'natal']\n",
            "['mulher', 'cai', 'durante', 'baile', 'funk']\n",
            "['abrigo', 'vai', 'ganhar', 'uma', 'tonelada', 'de', 'racao', 'se', 'video', 'tiver', '750', 'compartilhamentos']\n",
            "['neto', 'de', 'raul', 'seixas', 'canta', 'musica', 'do', 'avo', ',', 'mostra', 'video']\n",
            "['bolsonaro', 'pede', 'que', 'eleitores', 'ignorem', 'os', 'partidos', 'vermelhos', 'anti-brasileiros']\n",
            "['michelle', 'bolsonaro', 'esta', 'na', 'lista', 'de', 'propina', 'de', 'do', 'governador', 'pezao']\n",
            "['videos', 'mostram', 'momento', 'exato', 'do', 'rompimento', 'da', 'barragem', 'e', 'menina', 'se', 'salvando', 'em', 'brumadinho']\n",
            "['o', 'boticario', 'da', 'locao', 'hidratante', 'deleite', 'para', 'quem', 'compartilhar', 'no', 'whatsapp']\n",
            "['funcionarios', 'da', 'vale', 'cantam', 'no', 'dia', 'do', 'desastre', 'de', 'brumadinho', ',', 'mostra', 'video']\n",
            "['amostra', 'gratis', 'do', 'novo', 'batom', 'da', 'quem', 'disse', ',', 'berenice', 'em', 'site', 'no', 'whatsapp']\n",
            "['ursene', ':', 'estados', 'do', 'nordeste', 'se', 'unem', 'e', 'formam', 'republica', 'socialista']\n",
            "['damares', 'alves', 'diz', 'que', 'crossfit', 'e', 'coisa', 'de', 'lgbt', 'e', 'quer', 'acabar', 'com', 'academias']\n",
            "['samba-enredo', 'da', 'uniao', 'da', 'ilha', 'em', '2019', 'sera', 'uma', 'homenagem', 'aos', 'autistas']\n",
            "['gloria', 'maria', 'ja', 'foi', 'apresentadora', 'do', 'jornal', 'nacional']\n",
            "['pt', 'e', 'a', 'segunda', 'maior', 'organizacao', 'criminosa', 'do', 'mundo', 'em', 'ranking']\n",
            "['papa', 'manda', 'mensagem', 'de', 'apoio', 'a', 'maduro', 'em', '2019']\n",
            "['mst', 'queimou', 'laboratorio', 'que', 'produzia', 'vacina', 'contra', 'a', 'meningite']\n",
            "['xuxa', 'condena', 'postagem', 'de', 'bolsonaro', 'e', 'discute', 'com', 'presidente', 'no', 'twitter']\n",
            "['jeito', 'certo', 'de', 'descascar', 'abacaxi', 'e', 'arrancando', 'os', 'gomos', 'com', 'as', 'maos']\n",
            "['video', 'de', 'anitta', 'com', 'gabriel', 'medina', 'no', 'carnaval', 'circula', 'na', 'internet']\n",
            "['tire', 'sua', 'foto', 'do', 'perfil', 'para', 'nao', 'ser', 'hackeado', 'e', 'bloqueado', 'no', 'whatsapp']\n",
            "['video', 'mostra', 'momento', 'da', 'queda', 'do', 'aviao', 'boeing', '737', 'na', 'etiopia']\n",
            "['regina', 'duarte', 'diz', 'que', 'brasil', 'inventou', 'a', 'prisao', 'esportiva', ':', 'policia', 'prende', 'e', 'stf', 'solta']\n",
            "['reporter', 'joslei', 'cardinot', 'morreu', 'assassinado', ',', 'mostra', 'video']\n",
            "['filho', 'de', 'motorista', 'anderson', 'gomes', 'nao', 'recebe', 'pensao', 'porque', 'marielle', 'nao', 'assinou', 'carteira']\n",
            "['linda', 'tartaruga', 'azul', 'e', 'de', 'vidro', 'vive', 'no', 'oceano', 'atlantico', ',', 'mostra', 'foto']\n",
            "['moro', 'diz', 'que', 'e', 'impossivel', 'acabar', 'com', 'o', 'crime', 'porque', 'a', 'mente', 'humana', 'vem', 'do', 'homem', 'macaco']\n",
            "['inventario', 'de', 'dona', 'marisa', 'mostra', 'que', 'ela', 'ganhou', 'r', '$', '12', 'milhoes', '``', 'do', 'nada', \"''\"]\n",
            "['crivella', 'antecipa', 'feriado', 'de', 'sao', 'jorge', 'para', 'quinta-feira', 'santa', 'no', 'rio', 'de', 'janeiro']\n",
            "['pele', 'morreu', 'hoje', 'pela', 'manha', ',', 'em', '2019', ',', 'aos', '79', 'anos', 'de', 'idade']\n",
            "['dra', '.', 'juliana', 'miguita', 'diz', ',', 'em', 'audio', ',', 'que', 'cebola', 'na', 'geladeira', 'e', 'um', 'veneno']\n",
            "['5', 'teorias', 'da', 'conspiracao', 'que', 'envolvem', 'presidentes', 'americanos']\n",
            "['o', 'boticario', 'esta', 'dando', 'kit', 'lily', 'de', 'dia', 'das', 'maes', 'em', 'site', 'no', 'whatsapp']\n",
            "['fifa', 'multa', 'palmeiras', 'por', 'faixa', 'de', '1', 'campeao', 'mundial', 'de', 'clubes']\n",
            "['manicure', 'de', 'fundo', 'de', 'quintal', 'deixa', 'mulher', 'com', 'infeccao', 'no', 'pe', ',', 'mostra', 'foto']\n",
            "['vacina', 'do', 'h1n1', 'destroi', 'a', 'imunidade', 'das', 'pessoas', 'e', 'da', 'cancer']\n",
            "['padre', 'fabio', 'de', 'melo', 'faz', 'texto', '``', 'aprenda', 'a', 'nunca', 'mais', 'ser', 'idiota', \"''\", 'sobre', 'a', 'vida']\n",
            "['professor', 'da', 'universidade', 'de', 'brasilia', 'usa', 'salto', 'alto', 'em', 'congresso', 'do', 'psol']\n",
            "['laura', 'muller', ',', 'especialista', 'do', 'altas', 'horas', ',', 'recomenda', 'produto', 'milagroso']\n",
            "['william', 'bonner', 'foi', 'autuado', 'pela', 'prf', 'por', 'dirigir', 'com', 'a', 'cnh', 'vencida']\n",
            "['tubarao', 'e', 'atacado', 'por', 'cachorros', 'na', 'praia', 'de', 'boa', 'viagem', ',', 'recife', '(', 'pe', ')']\n",
            "['fotos', 'mostram', 'pessoas', 'infectadas', 'pelo', 'virus', 'mayaro']\n",
            "['foto', 'mostra', 'manifestacoes', 'pro-bolsonaro', 'de', '26/05', 'em', 'vitoria', '(', 'espirito', 'santo', ')']\n",
            "['canario', 'da', 'mata', 'eleva', 'asas', 'e', 'faz', 'oracao', 'ao', 'nascer', 'do', 'sol', ',', 'mostra', 'video']\n",
            "['paulinho', 'da', 'forca', 'diz', '``', 'se', 'nao', 'empurrarmos', 'brasil', 'para', 'o', 'abismo', ',', 'bolsonaro', 'sera', 'reeleito', \"''\"]\n",
            "['bolsonaro', 'oferece', 'capim', 'para', 'nordestinos', 'comerem', ',', 'mostra', 'video']\n",
            "['o', 'boticario', 'esta', 'dando', 'kits', 'de', 'perfumes', 'para', 'o', 'dia', 'dos', 'namorados', 'em', 'site']\n",
            "['sargento', 'manoel', 'silva', 'rodrigues', ',', 'preso', 'com', 'cocaina', 'na', 'espanha', ',', 'e', 'filiado', 'ao', 'pt']\n",
            "['deltan', 'dallagnol', 'diz', 'a', 'moro', 'nao', 'temos', 'provas', 'quanto', 'ao', 'triplex', 'em', 'conversa', 'vazada']\n",
            "['foto', 'mostra', 'sargento', 'preso', 'com', '39kg', 'de', 'cocaina', 'junto', 'a', 'lula', 'e', 'dona', 'marisa']\n",
            "['video', 'da', 'atriz', 'paolla', 'oliveira', 'vazou', 'na', 'internet']\n",
            "['governo', 'aprova', 'reducao', 'do', 'pis', 'de', 'r', '$', '998', 'para', 'r', '$', '468']\n",
            "['o', 'boticario', 'da', 'pincel', 'chanfrado', 'e', 'esponja', 'em', 'site', 'no', 'whatsapp']\n",
            "['usar', 'perfume', 'enquanto', 'o', 'ar-condicionado', 'do', 'carro', 'esta', 'ligado', 'causa', 'incendio']\n",
            "['piloto', 'de', 'aviao', 'desvia', 'de', 'caminhao-tanque', 'em', 'atentado', 'terrorista', 'na', 'argelia']\n",
            "['foto', 'mostra', 'militares', 'botando', 'lula', 'para', 'correr', 'durante', 'ditadura']\n",
            "['se', 'eu', 'fosse', 'voce', ',', 'nao', 'acreditaria', 'tao', 'facilmente', 'nas', 'denuncias', 'do', 'pavao', 'misterioso']\n",
            "['calor', 'de', '63c', 'no', 'kuwait', 'fez', 'semaforo', ',', 'carro', 'e', 'chinelo', 'derreterem']\n",
            "['miriam', 'leitao', 'foi', 'demitida', 'da', 'tv', 'globo', 'e', '``', 'chegou', 'o', 'fim', \"''\"]\n",
            "['miriam', 'leitao', 'segura', 'um', 'fuzil', '(', 'furadeira', ')', 'em', 'foto', 'tirada', 'pela', 'guerrilha']\n",
            "['glenn', 'greenwald', 'tem', 'infarto', 'e', 'e', 'internado', 'as', 'pressas', 'apos', 'overdose', 'de', 'cocaina']\n",
            "['advogados', 'de', 'lula', 'sao', 'os', 'mesmos', 'de', 'adelio', ',', 'hackers', 'e', 'glenn', 'grenwald']\n",
            "['bolsonaro', 'e', 'homenageado', 'por', 'marca', 'de', 'papel', 'higienico', 'na', 'alemanha']\n",
            "['xvideos', 'anuncia', 'no', 'twitter', '@', 'xv', 'que', 'saira', 'do', 'ar', 'no', 'brasil', 'em', 'setembro']\n",
            "['missao', 'indiana', 'chandrayaan', 'divulga', 'fotos', 'incriveis', 'da', 'terra', 'tiradas', 'do', 'espaco']\n",
            "['bolsonaro', 'e', 'michelle', 'gastaram', 'r', '$', '6,1', 'milhoes', 'de', 'cartao', 'corporativo', 'em', 'sete', 'meses']\n",
            "['bolsonaro', 'so', 'anda', 'com', 'a', 'faixa', 'presidencial', 'depois', 'que', 'virou', 'presidente']\n",
            "['ford', 'esta', 'distribuindo', '500', 'carros', 'ecosport', 'gratuitos', 'em', 'site', 'no', 'whatsapp']\n",
            "['clint', 'eastwood', 'morreu', 'aos', '89', 'anos', 'apos', 'ataque', 'cardiaco']\n",
            "['bolsonaro', 'traz', 'ultrassom', 'que', 'destroi', 'celulas', 'cancerigenas', 'de', 'israel']\n",
            "['video', 'de', 'homem', 'batendo', 'em', 'crianca', 'deve', 'ser', 'compartilhado', 'para', 'que', 'ele', 'seja', 'preso']\n",
            "['foto', 'mostra', 'lixo', 'em', 'parque', 'no', 'dia', 'seguinte', 'a', 'passeata', 'do', 'meio', 'ambiente']\n",
            "['sesi', 'abriu', 'centenas', 'de', 'vagas', 'de', 'emprego', 'em', 'site', 'no', 'whatsapp']\n",
            "['nasir', 'ul', 'hiraqi', ',', 'homem', 'mais', 'rico', 'do', 'kuwait', ',', 'morreu', 'e', 'deixou', 'tesouro', 'para', 'tras']\n",
            "['globo', 'demitiu', 'luciano', 'huck', 'apos', 'ele', 'ter', 'manifestado', 'interesse', 'em', 'ser', 'presidente']\n",
            "['dilma', 'diz', 'que', ',', 'assim', 'como', 'janot', ',', 'ia', 'matar', 'moro', 'e', 'se', 'suicidar']\n",
            "['seis', 'pessoas', 'morreram', 'de', 'overdose', 'de', 'cocaina', 'no', 'rock', 'in', 'rio', '2019']\n",
            "['empresario', 'ganhou', 'milhoes', 'no', 'cassino']\n",
            "['ganhe', 'milhoes', 'de', 'bonus', 'em', 'apostas', 'esportivas']\n",
            "['magazine', 'luiza', 'sorteia', 'tv', 'de', '50', 'polegadas', 'em', 'site', 'no', 'whatsapp', 'e', 'no', 'facebook']\n",
            "['lindbergh', 'farias', 'e', 'flagrado', 'comprando', 'drogas', 'no', 'morro', 'do', 'vidigal', '(', 'rio', 'de', 'janeiro', ')']\n",
            "['aviao', 'da', 'eurowings', 'quica', 'em', 'aeroporto', 'durante', 'pouso', 'arriscado', ',', 'mostra', 'video']\n",
            "['facas', 'escondidas', 'dentro', 'de', 'pentes', 'sao', 'as', 'novas', 'ameacas', 'em', 'escolas', 'do', 'brasil']\n",
            "['beber', 'cerveja', 'sem', 'lavar', 'a', 'lata', 'causa', 'larva', 'na', 'boca', 'de', 'homem']\n",
            "['carro', 'explode', 'em', 'posto', 'de', 'gasolina', 'porque', 'criancas', 'estavam', 'brincando', 'com', 'o', 'celular']\n",
            "['robo', 'israelense', 'de', 'guerra', 'atira', 'com', 'precisao', 'em', 'treinamento', ',', 'mostra', 'video']\n",
            "['tramontina', 'esta', 'dando', '3', 'mil', 'jogos', 'de', 'panelas', 'gratis', 'em', 'site', 'no', 'whatsapp']\n",
            "['botijao', 'de', 'gas', 'vazio', 'explode', 'porque', 'dois', 'homens', 'bateram', 'e', 'balancaram', 'o', 'objeto']\n",
            "['maisa', 'diz', 'que', 'nao', 'fez', 'prova', 'do', 'enem', 'porque', '``', 'e', 'coisa', 'de', 'pobre', \"''\"]\n",
            "['video', 'mostra', 'chegada', 'de', 'lula', 'em', 'sao', 'bernardo', 'do', 'campo', 'com', 'fogos', 'de', 'artificio']\n",
            "['galvao', 'bueno', 'pediu', 'demissao', 'da', 'globo', 'e', 'vai', 'para', 'emissora', 'concorrente']\n",
            "['dilma', 'disse', 'que', 'torce', 'para', 'que', 'mancha', 'de', 'oleo', 'nao', 'chegue', 'a', 'minas', 'gerais']\n",
            "['video', 'de', 'jogadores', 'da', 'selecao', 'brasileira', 'sub-17', 'em', 'festa', 'na', 'piscina', 'vaza', 'na', 'internet']\n",
            "['video', 'de', 'jeanine', 'anez', ',', 'presidente', 'interina', 'da', 'bolivia', ',', 'com', 'um', 'homem', 'vaza', 'na', 'internet']\n",
            "['onibus', 'em', 'londres', 'mostra', 'os', 'dizeres', 'lula', 'ladrao', ',', 'mostra', 'video']\n",
            "['jose', 'dirceu', 'diz', 'que', 'maconaria', 'sabotou', 'a', 'esquerda', 'na', 'america', 'latina']\n",
            "['suspeita', 'de', 'doping', 'pode', 'tirar', 'o', 'titulo', 'do', 'flamengo', 'na', 'libertadores']\n",
            "['mac', 'cosmetics', 'distribui', 'gloss', 'e', 'batom', 'gratis', 'em', 'site', 'no', 'whatsapp']\n",
            "['eca', 'de', 'queiroz', 'disse', 'que', 'politicos', 'sao', 'como', 'fraldas', 'e', 'devem', 'ser', 'trocados']\n",
            "['bolsonaro', 'acusa', 'leonardo', 'dicaprio', 'de', 'ter', 'afundado', 'um', 'navio', 'em', 'resposta', 'a', 'criticas', 'no', 'twitter']\n",
            "['lula', 'foi', 'expulso', 'de', 'paraty', '(', 'rj', ')', 'por', 'manifestantes', 'que', 'moram', 'na', 'cidade']\n",
            "['ceara', 'escalou', 'jogador', 'irregular', 'e', 'cruzeiro', 'pode', 'ser', 'beneficiado', 'no', 'brasileirao']\n",
            "['lewandowski', 'e', 'flagrado', 'algemado', 'em', 'foto', 'de', 'grupo', 'terrorista', 'tirada', 'em', '1965']\n",
            "['santander', 'libera', 'cadastros', 'aprovados', 'com', 'r', '$', '700', 'de', 'credito', 'em', 'site', 'no', 'whatsapp']\n",
            "['bolsonaro', 'trouxe', 'scanner', 'de', 'israel', 'para', 'prf', 'monitorar', 'carros', 'em', 'movimento']\n",
            "['milena', 'bemfica', ',', 'esposa', 'do', 'goleiro', 'jean', ',', 'postou', 'video', 'dando', 'bom', 'dia', 'sem', 'roupa']\n",
            "['presidente', 'da', 'seguradora', 'lider', 'dpvat', 'e', 'luciano', 'bivar', 'e', 'a', 'vice', 'e', 'a', 'esposa', 'de', 'gilmar', 'mendes']\n",
            "['firmino', 'e', 'pego', 'no', 'doping', ',', 'liverpool', 'e', 'eliminado', 'e', 'flamengo', 'pode', 'ganhar', 'mundial']\n",
            "['bolsonaro', 'constroi', 'a', 'maior', 'ponte', 'ferroviaria', 'da', 'america', 'latina', 'em', '11', 'meses']\n",
            "['maradona', 'e', 'nomeado', 'secretario', 'de', 'politica', 'antidrogas', 'da', 'argentina']\n",
            "['video', 'mostra', 'tentativa', 'de', 'assassinato', 'do', 'principe', 'herdeiro', 'da', 'arabia', 'saudita', 'hoje']\n",
            "['hospital', 'do', 'cancer', 'pede', 'para', 'voce', 'repassar', 'foto', 'de', 'flores', 'porque', 'a', 'cada', '10', 'pessoas', 'eles', 'ganham', '1', 'euro']\n",
            "['energeticos', 'derretem', 'alimentos', 'e', 'fazem', 'mal', 'a', 'saude', ',', 'mostra', 'video']\n",
            "['dra', '.', 'paula', 'mello', 'campos', 'anuncia', 'produto', 'que', 'garante', 'sorriso', 'perfeito']\n",
            "['morcegos', ',', 'serpentes', 'e', 'cachorros', 'sao', 'comercializados', 'em', 'feira', 'na', 'china', ',', 'mostram', 'fotos']\n",
            "['bloqueio', 'do', 'whatsapp', 'por', 'dois', 'dias', '(', '48', 'horas', ')', 'e', 'determinado', 'pela', 'justica', 'em', '2020']\n",
            "['teste', 'com', 'copo', 'de', 'agua', 'define', 'se', 'cafe', 'e', 'puro', 'ou', 'impuro', ',', 'mostra', 'video']\n",
            "['vitamina', 'c', 'com', 'zinco', 'e', 'vitamina', 'd', 'curam', 'e', 'previnem', 'o', 'coronavirus']\n",
            "['reflexo', 'de', 'crianca', 'fica', 'parado', 'no', 'espelho', 'e', 'lanca', 'olhar', 'maligno', ',', 'mostra', 'video']\n",
            "['autor', 'de', 'caneta', 'azul', ',', 'manoel', 'gomes', ',', 'e', 'flagrado', 'com', 'outro', 'homem', 'em', 'video']\n",
            "['roupa', 'nova', 'faz', 'homenagem', 'a', 'regina', 'duarte', 'com', 'nova', 'versao', 'de', 'musica', 'dona']\n",
            "['arnold', 'schwarzenegger', 'posa', 'com', 'camiseta', 'bolsonaro', 'presidente']\n",
            "['os', 'simpsons', 'previu', 'a', 'chegada', 'do', 'coronavirus', 'em', 'episodio', 'de', '1993']\n",
            "['lula', 'acaba', 'de', 'ser', 'expulso', 'da', 'franca', 'e', 'o', 'pior', 'acontece', ',', 'mostra', 'video']\n",
            "['video', 'mostra', 'ronaldinho', 'gaucho', 'jogando', 'futsal', 'no', 'torneio', 'do', 'presidio']\n",
            "['vacina', 'do', 'novo', 'coronavirus', '(', 'covid-19', ')', 'vira', 'com', 'microchip', 'para', 'colher', 'identidade', 'da', 'populacao']\n",
            "['1', 'de', 'abril', ',', '5.000', 'textos', ',', '13', 'milhoes', 'de', 'acessos', 'e', '3', 'topicos']\n",
            "['salim', 'mattar', ',', 'dono', 'da', 'localiza', ',', 'grava', 'video', 'criticando', 'quarentena', ',', 'congresso', 'e', 'apoiando', 'bolsonaro']\n",
            "['morre', 'carbonizado', 'homem', 'que', 'representou', 'demonio', 'arrastando', 'jesus', 'no', 'carnaval']\n",
            "['bolsonaro', 'assina', 'decreto', 'presidencial', 'determinando', 'o', 'fim', 'do', 'isolamento']\n",
            "['peticao', 'publica', 'que', 'pede', 'compartilhamento', 'vai', 'ajudar', 'no', 'fechamento', 'do', 'congresso']\n",
            "['mistura', 'de', 'alho', ',', 'acafrao', 'e', 'limao', 'previne', 'e', 'cura', 'o', 'coronavirus']\n",
            "['dono', 'de', 'lava', 'jato', 'de', 'maringa', 'e', 'morto', 'pela', 'guarda', 'civil', 'por', 'nao', 'respeitar', 'quarentena', 'do', 'coronavirus']\n",
            "['facebook', 'bloqueou', 'video', 'do', 'mandetta', 'e', 'apagou', 'perfil', 'oficial', 'de', 'sergio', 'moro']\n",
            "['rodrigo', 'maia', 'posa', 'para', 'foto', 'abracado', 'com', 'presidente', 'da', 'china', 'xi', 'jinping']\n",
            "['joao', 'doria', 'pinta', 'bandeira', 'do', 'partido', 'comunista', 'chines', '(', 'pcc', ')', 'em', 'muro', 'durante', 'quarentena', 'em', 'sp']\n",
            "['criatura', 'estranha', 'nunca', 'vista', 'antes', 'e', 'encontrada', 'no', 'sul', 'da', 'colombia']\n",
            "['sergio', 'moro', 'toma', 'cafe', 'da', 'manha', 'com', 'rodrigo', 'maia', 'e', 'joice', 'hasselmann', 'apos', 'pedir', 'demissao', 'no', 'ministerio', 'da', 'justica']\n",
            "['aecio', 'neves', 'e', 'cunhado', 'de', 'moro', ',', 'que', 'e', 'filho', 'de', 'fundador', 'do', 'psdb']\n",
            "['governador', 'do', 'para', ',', 'helder', 'barbalho', ',', 'escondeu', 'toneladas', 'de', 'cloroquina', 'em', 'um', 'galpao']\n",
            "['doria', 'admitiu', 'que', 'ha', 'duplicidade', 'no', 'numero', 'de', 'mortos', 'por', 'coronavirus']\n",
            "['consulte', 'se', 'voce', 'tem', 'direito', 'ao', 'saque', 'do', 'fgts', 'de', 'r', '$', '1045', 'em', 'site', 'no', 'whatsapp']\n",
            "['praia', 'de', 'botafogo', ',', 'no', 'rio', 'de', 'janeiro', ',', 'ficou', 'limpa', 'por', 'causa', 'da', 'quarentena']\n",
            "['filho', 'de', 'william', 'bonner', 'e', 'fatima', 'bernardes', 'solicitou', 'e', 'recebeu', 'auxilio', 'emergencial']\n",
            "['fotos', 'mostram', 'mofo', 'em', 'shopping', 'no', 'brasil', 'que', 'ficou', 'com', 'o', 'ar-condicionado', 'desligado']\n",
            "['bolsonaro', 'confirma', 'possivel', 'cancer', 'de', 'pele', 'e', 'faz', 'desabafo']\n",
            "['china', 'vai', 'distribuir', 'vacina', 'contra', 'o', 'coronavirus', 'de', 'graca', 'para', 'o', 'mundo']\n",
            "['william', 'bonner', 'e', 'internado', 'em', 'estado', 'grave', 'com', 'suspeita', 'de', 'covid-19']\n",
            "['luciano', 'hang', ',', 'dono', 'da', 'havan', ',', 'se', 'cadastrou', 'e', 'recebeu', 'auxilio', 'emergencial']\n",
            "['skol', 'esta', 'dando', 'geladeirinha', 'gratis', 'em', 'site', 'no', 'whatsapp']\n",
            "['isolamento', 'social', 'foi', 'inutil', 'porque', 'oms', 'disse', 'que', 'assintomaticos', 'nao', 'transmitem', 'covid-19']\n",
            "['enxofre', 'do', 'alho', 'previne', 'e', 'cura', 'o', 'coronavirus', '(', 'covid-19', ')']\n",
            "['dexametasona', 'e', 'a', 'cura', 'para', 'a', 'covid-19', 'e', 'previne', 'o', 'coronavirus']\n",
            "['governo', 'federal', 'decreta', 'proibicao', 'de', 'aulas', 'presenciais', 'em', 'todo', 'brasil']\n",
            "['jonathan', 'galindo', ',', 'o', 'homem', 'pateta', ',', 'e', 'um', 'ser', 'sobrenatural', 'que', 'vai', 'atacar', 'voce', 'e', 'a', 'sua', 'familia']\n",
            "['caseiro', 'do', 'sitio', 'onde', 'estava', 'queiroz', 'e', 'encontrado', 'morto', ',', 'mostra', 'foto']\n",
            "['dubai', 'comemora', 'fim', 'do', 'lockdown', 'e', 'pandemia', 'da', 'covid-19', 'com', 'video', 'incrivel']\n",
            "['ivo', 'gomes', ',', 'irmao', 'de', 'ciro', 'gomes', ',', 'grava', 'audio', 'sobre', 'transposicao', 'do', 'rio', 'sao', 'francisco', ',', 'pt', 'e', 'bolsonaro']\n",
            "['acabou', 'a', 'gasolina', 'em', 'caracas', '(', 'venezuela', ')', 'e', 'carros', 'fizeram', 'fila', 'gigante', 'hoje', ',', 'mostra', 'video']\n",
            "['bolsonaro', 'criou', ',', 'em', 'decreto', ',', 'o', 'stm', 'para', 'detonar', 'ministros', 'corruptos', 'do', 'stf']\n",
            "['bolsonaro', 'vai', 'unir', 'bolsa', 'familia', 'e', 'criar', 'beneficio', 'permanente', 'de', 'r', '$', '1200']\n",
            "['foto', 'mostra', 'rosto', 'de', 'jesus', 'em', 'nuvem', 'e', 'deve', 'ser', 'compartilhada', 'em', 'todos', 'os', 'grupos']\n",
            "['carlos', 'henrique', 'schroeder', 'grava', 'video', 'detonando', 'a', 'globo', 'antes', 'de', 'pedir', 'demissao']\n",
            "['lojas', 'americanas', 'joga', 'todos', 'os', 'livros', 'de', 'felipe', 'neto', 'no', 'lixo']\n",
            "['natura', 'contratou', 'thammy', 'miranda', 'para', 'estrelar', 'propaganda', 'na', 'tv', 'de', 'dia', 'dos', 'pais']\n",
            "['demissao', 'de', 'faustao', 'e', 'saida', 'de', 'william', 'bonner', 'do', 'jn', 'fazem', 'a', 'globo', 'sofrer']\n",
            "['governo', 'vai', 'demitir', '396', 'mil', 'funcionarios', 'publicos', 'que', 'receberam', 'auxilio', 'emergencial']\n",
            "['aplicacao', 'de', 'ozonio', '(', 'ozonioterapia', ')', 'e', 'a', 'cura', 'para', 'a', 'covid-19', '(', 'coronavirus', ')']\n",
            "['lula', 'esta', 'com', 'coronavirus', '(', 'covid-19', ')', 'e', 'exame', 'foi', 'confirmado', 'pelo', 'hospital', 'adventista']\n",
            "['alho', 'e', 'vinagre', 'de', 'maca', 'sao', 'a', 'cura', 'para', 'a', 'covid-19', 'e', 'matam', 'o', 'coronavirus']\n",
            "['usp', 'comprovou', 'que', 'pessoas', 'em', 'isolamento', '(', 'confinamento', ')', 'sao', 'mais', 'vulneraveis', 'a', 'covid-19']\n",
            "['filha', 'de', 'jose', 'de', 'abreu', 'e', 'flagrada', 'bebada', 'e', 'humilhando', 'funcionario', 'de', 'quiosque']\n",
            "['menina', 'perdida', 'que', 'so', 'fala', 'que', 'e', 'de', 'campo', 'grande', 'e', 'chora', 'procura', 'os', 'pais']\n",
            "['george', 'soros', 'promete', 'ajudar', 'argentina', 'se', 'pais', 'prolongar', 'quarentena', 'e', 'prejudicar', 'bolsonaro']\n",
            "['policia', 'federal', 'encontra', 'r', '$', '200', 'milhoes', 'no', 'endereco', 'de', 'bolsonaro']\n",
            "['videos', 'verdade', 'fora', 'da', 'midia', 'falam', 'a', 'verdade', 'ao', 'alertar', 'contra', 'vacina', 'e', 'termometro']\n",
            "['secretaria', 'de', 'educacao', 'de', 'sao', 'paulo', 'retira', 'termo', '``', 'antes', 'de', 'cristo', \"''\", 'de', 'livros', 'escolares']\n",
            "['robert', 'f.', 'kennedy', 'jr.', 'escreve', 'texto', 'que', 'aponta', 'que', 'vacina', 'mrna', 'contra', 'covid-19', 'altera', 'o', 'dna']\n",
            "['lei', '13.246', 'foi', 'criada', 'por', 'bolsonaro', 'e', 'acaba', 'com', 'o', 'dia', 'das', 'bruxas', 'no', 'brasil']\n",
            "['anvisa', 'liberou', 'ivermectina', 'e', 'mudou', 'dose', 'para', 'tratar', 'a', 'covid-19']\n",
            "['foto', 'mostra', 'manuela', 'davila', 'com', 'olheiras', 'e', 'tatuagens', 'de', 'che', 'guevera', 'e', 'lenin']\n",
            "['dilma', 'aparece', 'em', 'foto', 'com', 'pablo', 'escobar', ',', 'cristina', 'kirchner', 'e', 'nestor', 'kirchner']\n",
            "['chineses', 'estao', 'comprando', 'os', 'hoteis', 'de', 'porto', 'seguro', 'que', 'faliram', 'na', 'quarentena']\n",
            "['trump', 'disse', 'que', 'imprensa', 'do', 'brasil', 'teme', 'mais', 'um', 'militar', 'do', 'que', 'um', 'corrupto']\n",
            "['bolsonaro', 'foi', 'aplaudido', 'de', 'pe', 'na', 'onu', 'e', 'teve', '90', '%', 'de', 'aprovacao']\n",
            "['vacina', 'chinesa', 'causa', 'dano', 'genetico', 'e', 'causou', 'reacoes', 'adversas', 'graves', 'em', 'grande', 'numero', 'de', 'voluntarios']\n",
            "['papa', 'francisco', 'e', 'flagrado', 'fumando', 'em', 'foto', 'com', 'evo', 'morales']\n",
            "['escorpioes', 'cresceram', 'e', 'se', 'reproduziram', 'dentro', 'de', 'berinjela', ',', 'mostra', 'video']\n",
            "['pepita', 'de', 'ouro', 'de', '4121', 'gramas', 'e', 'girafa', 'encontrada', 'por', 'ong', 'na', 'amazonia']\n",
            "['laboratorios', 'que', 'produzem', 'a', 'vacina', 'contra', 'covid-19', 'terao', 'isencao', 'de', 'responsabilidade', 'civil']\n",
            "['trump', 'segura', 'a', 'bandeira', 'do', 'brasil', 'durante', 'evento', ',', 'mostra', 'foto']\n",
            "['compartilhe', 'a', 'mensagem', 'de', 'teresa', 'fidalgo', 'com', '20', 'amigos', 'senao', 'voce', 'vai', 'morrer', 'em', '20', 'dias']\n",
            "['naja', 'gigante', 'e', 'vista', 'na', 'china', 'e', 'pode', 'chegar', 'ao', 'brasil']\n",
            "['eduardo', 'leite', ',', 'governador', 'do', 'rio', 'grande', 'do', 'sul', ',', 'usa', 'terno', 'rosa']\n",
            "['milhares', 'de', 'cientistas', 'e', 'medicos', 'provam', 'fraude', 'da', 'covid-19']\n",
            "['pix', 'e', 'a', 'marca', 'da', 'besta', 'e', 'veio', 'para', 'impor', 'nova', 'ordem', 'mundial']\n",
            "['foto', 'mostra', 'ministros', 'do', 'stf', 'de', 'toga', 'vermelha', 'durante', 'sessao']\n",
            "['maguila', 'esta', 'ha', 'dois', 'anos', 'em', 'um', 'asilo', 'e', 'sem', 'receber', 'visitas']\n",
            "['bruno', 'covas', 'comprou', '38', 'mil', 'caixoes', 'por', 'causa', 'da', 'covid-19', 'e', 'tem', '20', 'mil', 'empilhados']\n",
            "['goleiro', 'bruno', 'foi', 'envenenado', 'e', 'esta', 'em', 'estado', 'grave']\n",
            "['oms', 'condena', 'lockdown', 'e', 'e', 'contra', 'o', 'isolamento', 'para', 'combater', 'a', 'covid-19']\n",
            "['abono', 'emergencial', 'de', 'natal', 'esta', 'disponivel', 'em', 'site', 'no', 'whatsapp']\n",
            "['el', 'mercurio', 'fez', 'charge', 'de', 'bolsonaro', ',', 'chile', 'e', '``', 'morte', 'comunista', \"''\"]\n",
            "['estudo', 'aponta', 'que', 'a', 'nitazoxanida', 'e', 'a', 'cura', 'para', 'a', 'covid-19']\n",
            "['mcdonalds', 'vai', 'dar', 'dois', 'big', 'macs', 'em', 'promocao', '``', 'vassoura', 'thru', \"''\"]\n",
            "['lentilha', 'cura', 'depressao', 'e', 'ansiedade', 'porque', 'tem', 'muito', 'litio']\n",
            "['compartilhe', 'texto', 'para', 'fazer', 'bypass', 'de', 'algoritmo', 'do', 'facebook']\n",
            "['wisconsin', 'teve', 'mais', 'votos', 'do', 'que', 'populacao', 'apta', 'para', 'votar']\n",
            "['biden', 'ameacou', 'bombardear', 'brasil', 'se', 'pais', 'nao', 'cuidar', 'da', 'amazonia']\n",
            "['simpsons', 'previu', 'fraude', 'nas', 'eleicoes', 'dos', 'eua', ',', 'vitoria', 'de', 'biden', 'e', 'futuro', 'sombrio']\n",
            "['boulos', 'criou', 'laranjal', 'com', 'empresas', 'fantasmas', 'nas', 'eleicoes', '2020']\n",
            "['enxaguantes', 'bucais', 'sao', 'descobertos', 'como', 'a', 'cura', 'da', 'covid-19']\n",
            "['mascara', 'baixa', 'imunidade', ',', 'causa', 'agua', 'no', 'pulmao', 'e', 'pneumonia']\n",
            "['doria', 'anuncia', 'lockdown', 'em', 'sao', 'paulo', 'em', 'novembro', 'de', '2020']\n",
            "['cacau', 'show', 'da', '5000', 'panetones', 'de', 'black', 'friday', 'no', 'whatsapp']\n",
            "['mst', 'derrubou', 'torres', 'em', 'estacao', 'de', 'transmissao', 'no', 'amapa']\n",
            "['virus', 'chines', 'e', 'a', 'maior', 'farsa', 'e', 'nao', 'existe', 'nada', 'que', 'possa', 'ser', 'feito']\n",
            "['pensilvania', 'confirma', 'fraude', ',', 'cancela', 'votos', 'e', 'trump', 'vence', 'eleicoes']\n",
            "['pandemia', 'ja', 'acabou', 'e', 'nao', 'ha', 'necessidade', 'de', 'vacinas']\n",
            "['neta', 'de', 'agnaldo', 'rayol', 'canta', 'ave', 'maria', 'junto', 'com', 'avo']\n",
            "['joe', 'biden', 'diz', '``', 'enquanto', 'nao', 'elegerem', 'outro', 'presidente', 'do', 'brasil', 'nao', 'me', 'apresentem', 'a', 'bolsonaro', \"''\"]\n",
            "['dra', '.', 'christiane', 'northrup', 'esta', 'certa', 'ao', 'alertar', 'que', 'vacina', 'muda', 'dna', 'e', 'nos', 'transforma', 'em', 'antenas', '5g']\n",
            "['boulos', 'esta', 'internado', 'no', 'albert', 'einstein', 'para', 'se', 'tratar', 'da', 'covid-19']\n",
            "['general', 'lessa', \"''\", 'denuncia', 'bolsa', 'ditadura', 'e', 'pede', 'estado', 'de', 'guerra']\n",
            "['cariocas', 'fazem', 'romaria', 'na', 'presidente', 'dutra', 'para', 'se', 'vacinar', 'em', 'sao', 'paulo']\n",
            "['sadia', 'da', 'ceia', 'de', 'natal', 'completa', 'e', 'peru', 'em', 'site', 'no', 'whatsapp']\n",
            "['apostas', 'esportivas', 'sao', 'ilegais', 'no', 'brasil']\n",
            "['ivermectina', 'aniquila', 'covid-19', 'se', 'usada', 'preventivamente']\n",
            "['covid-19', 'e', 'h1n1', '+', 'hiv', 'e', 'vacina', 'tera', 'chip', 'de', 'plasma', 'para', 'reduzir', 'populacao']\n",
            "['china', 'comprou', '1', 'bilhao', 'de', 'vacinas', 'da', 'australia', 'e', 'nao', 'usara', 'coronavac']\n",
            "['seringa', 'falsa', 'com', 'agulha', 'retratil', 'esta', 'sendo', 'usada', 'em', 'vacinas', 'contra', 'covid-19']\n",
            "['walter', 'moinho', 'fez', 'pintura', '``', 'a', 'vida', 'em', '2022', \"''\", 'que', 'previa', 'pessoas', 'isoladas']\n",
            "['ex-mulher', 'de', 'bolsonaro', ',', 'valeria', 'bolsonaro', ',', 'e', 'expulsa', 'do', 'psl']\n",
            "['homem', 'morreu', 'de', 'ataque', 'cardiaco', 'em', 'israel', 'por', 'causa', 'da', 'vacina', 'contra', 'covid-19']\n",
            "['china', 'descarta', 'coronavac', 'e', 'anvisa', 'diz', 'que', 'fabrica', 'da', 'sinovac', 'esta', 'fora', 'do', 'padrao']\n",
            "['video', 'mostra', 'que', '``', 'nao', 'devemos', 'ter', 'esperanca', 'nas', 'vacinas', \"''\"]\n",
            "['nao', 'havera', 'mais', 'regresso', 'a', 'normalidade', 'que', 'viviamos']\n",
            "['video', '``', 'vacuna', 'show', \"''\", '(', '``', 'vacina', 'show', \"''\", ')', 'mostra', 'farsa', 'da', 'vacinacao', 'contra', 'covid-19']\n",
            "['antivacinas', 'e', 'bolsonaristas', 'inundam', 'redes', 'sociais', 'com', 'denuncias', 'falsas', 'de', 'vacinacao', 'encenada']\n",
            "['tabela', 'mostra', 'cronograma', 'de', 'vacinacao', 'no', 'brasil', 'e', 'em', 'estados', '(', 'rj', ',', 'am', 'etc', '.', ')']\n",
            "['ministerio', 'da', 'saude', 'faz', 'pesquisa', 'e', 'pede', 'codigo', 'sms', 'do', 'whatsapp']\n",
            "['vacina', 'natural', 'do', 'organismo', 'imuniza', 'contra', 'covid-19']\n",
            "['ford', ',', 'manaus', 'e', 'congresso', ':', 'videos', 'antigos', 'sao', 'retirados', 'de', 'contexto', 'para', 'fortalecer', 'teses', 'falsas', ',', 'antidemocraticas', 'e', 'negacionistas']\n",
            "['enfermeira', 'que', 'tomou', 'vacina', 'mostrou', 'bilhete', 'chora', 'bolsonaro']\n",
            "['reporter', 'pergunta', 'a', 'medico', 'negro', 'que', 'tomou', 'vacina', 'na', 'bahia', 'se', 'ele', 'e', 'motorista', 'ou', 'socorrista']\n",
            "['fazer', 'um', 'ensaio', 'academico', '(', 'estrutura', ',', 'tema', 'etc', '.', ')', 'e', 'impossivel']\n",
            "['whatsapp', 'vai', 'deixar', 'de', 'funcionar', 'em', 'todos', 'celulares', 'samsung', ',', 'motorola', ',', 'sony', 'e', 'iphone']\n",
            "['influenciadores', 'digitais', 'do', 'brasil', 'nao', 'tem', 'relevancia', 'no', 'exterior']\n",
            "['pai', 'de', 'menino', 'acorrentado', 'no', 'barril', 'foi', 'espancado', 'na', 'cadeia']\n",
            "['video', 'mostra', 'primeiro', 'trem', 'saindo', 'de', 'mato', 'grosso', 'para', 'porto', 'de', 'santos']\n",
            "['ministra', 'damares', 'alves', 'diz', 'que', 'transar', 'e', 'de', 'esquerda']\n",
            "['arthur', 'lira', 'resolveu', 'retirar', 'quatro', 'carros', 'e', 'segurancas', 'de', 'rodrigo', 'maia']\n",
            "['bolsonaro', 'demitiu', 'mourao', 'do', 'cargo', 'de', 'vice-presidente']\n",
            "['voce', 'recebeu', 'dinheiro', 'via', 'pix', 'e', 'precisa', 'desbloquear', 'credito', 'no', 'email']\n",
            "['daniel', 'silveira', 'aparece', 'com', 'luciano', 'huck', 'e', 'cristiano', 'ronaldo', 'em', 'fotos']\n",
            "['ronaldo', 'caiado', 'zerou', 'icms', 'de', 'combustiveis', 'em', 'goias']\n",
            "['alexandre', 'de', 'moraes', 'e', 'flordelis', 'aparecem', 'abracados', 'em', 'foto']\n",
            "['hospital', 'do', 'cancer', 'vai', 'ganhar', '1', 'euro', 'por', '10', 'pessoas', 'que', 'repassarem', 'foto', 'de', 'flores']\n",
            "['paulo', 'guedes', 'gastou', 'r', '$', '120', 'mil', 'com', 'viagra', ',', 'mostra', 'cnn']\n",
            "['26', 'pessoas', 'morreram', 'por', 'causa', 'de', 'vacinas', 'contra', 'covid-19', 'nas', 'ultimas', '24', 'horas', ',', 'diz', 'anvisa']\n",
            "['coronavac', 'nao', 'funciona', 'contra', 'nova', 'cepa', 'da', 'covid-19', ',', 'diz', 'estudo']\n",
            "['carta', 'enviada', 'a', 'bolsonaro', 'esta', 'certa', 'ao', 'afirmar', 'que', 'covid-19', 'e', 'uma', 'farsa']\n",
            "['zema', 'anuncia', ',', 'em', '2021', ',', 'lockdown', 'em', 'todas', 'cidades', 'de', 'minas', 'gerais', 'e', 'fechamento', 'de', 'fronteiras']\n",
            "['adidas', 'esta', 'dando', 'sapatos', 'de', 'presente', 'de', 'dia', 'da', 'mulher']\n",
            "['atencao', ':', 'ate', 'o', 'momento', ',', 'ninguem', 'morreu', 'por', 'causa', 'da', 'vacina', 'contra', 'covid-19', 'no', 'brasil']\n",
            "['felipe', 'andreoli', 'e', 'carol', 'barcellos', 'sao', 'flagrados', 'em', 'video', 'nos', 'bastidores', 'da', 'globo']\n",
            "['apple', 'lancou', 'seu', 'primeiro', 'carro', 'eletrico', ',', 'mostra', 'video']\n",
            "['previsao', 'de', 'vacinacao', 'para', 'covid-19', 'do', 'ministerio', 'da', 'saude', 'ate', '08/08', 'e', 'divulgada']\n",
            "['whatsapp', 'gold', 'e', 'video', 'gambarelli', 'sao', 'virus', 'que', 'bloqueiam', 'o', 'telefone', 'e', 'sua', 'conta', 'bancaria']\n",
            "['ofertas', 'falsas', 'de', 'brindes', 'gratis', 'enganam', '(', 'de', 'novo', ')', 'milhoes', 'de', 'inocentes', 'no', 'whatsapp']\n",
            "['video', 'mostra', 'policiais', 'apedrejados', 'em', 'protesto', 'contra', 'lockdown', 'na', 'argentina']\n",
            "['filho', 'de', 'marcelo', 'freixo', 'saiu', 'com', 'travestis', ',', 'nao', 'pagou', 'e', 'foi', 'exposto', 'em', 'video']\n",
            "['geraldo', 'luis', ',', 'apresentador', ',', 'morreu', 'hoje', 'e', 'brasil', 'ficou', 'em', 'luto']\n",
            "['87', 'mil', 'reprovaram', 'na', 'redacao', 'do', 'enem', 'por', 'usar', 'pronome', 'neutro']\n",
            "['santinho', 'de', 'dr.', 'jairinho', 'tinha', 'dizeres', 'fechado', 'com', 'bolsonaro', 'e', 'defensor', 'da', 'familia']\n",
            "['inss', 'liga', 'para', 'aposentado', 'fazer', 'prova', 'de', 'vida', 'online']\n",
            "['ivete', 'sangalo', 'e', 'flagrada', 'em', 'festa', 'de', 'familia', 'durante', 'a', 'pandemia']\n",
            "['inquerito', '6897', 'de', 'alexandre', 'de', 'moraes', 'proibe', 'bolsonaro', 'de', 'decretar', 'intervencao', 'militar']\n",
            "['estudo', 'em', 'sorocaba', 'prova', 'que', 'tratamento', 'precoce', 'com', 'kit', 'covid', 'tem', '99', '%', 'de', 'eficacia']\n",
            "['milagres', 'do', 'tratamento', 'precoce', 'em', 'cidades', 'brasileiras', 'reforcam', 'teses', 'falsas', 'sobre', 'remedios', 'sem', 'eficacia', 'contra', 'a', 'covid-19']\n",
            "['jogos', 'online', 'nao', 'substituemcassinosfisicos']\n",
            "['desenvolvedor', 'prova', 'que', 'ha', 'fraude', 'no', 'codigo-fonte', 'das', 'urnas', 'eletronicas', 'no', 'brasil']\n",
            "['app', 'modificado', 'da', 'netflix', 'desbloqueia', 'filmes', 'e', 'assinaturas', 'gratis']\n",
            "['erro', 'no', 'julgamento', 'do', 'stf', 'deixa', 'lula', 'inelegivel']\n",
            "['eduardo', 'costa', 'recebeu', 'ameacas', 'de', 'morte', 'de', 'politicos', 'apos', 'lancar', 'musica', 'cuidado']\n",
            "['nao', 'e', 'possivel', 'pesquisar', 'preco', 'do', 'gas', 'no', 'brasil']\n",
            "['new', 'york', 'times', 'destaca', 'dizeres', 'bolsonaro', 'foverer', 'e', 'brazil', 'whants', 'to', 'be', 'free', 'na', 'capa']\n",
            "['bolsonaro', 'comemorou', 'morte', 'de', 'paulo', 'gustavo', 'por', 'covid-19']\n",
            "['video', 'mostra', 'bonde', 'do', 'jacare', 'apos', 'acao', 'policial', 'no', 'jacarezinho']\n",
            "['capsulas', 'de', 'remedios', '(', 'esoral', 'e', 'enterofuryl', ')', 'vem', 'com', 'pregos', 'dentro']\n",
            "['coronavirus', 'foi', 'testado', 'como', 'arma', 'biologica', 'pela', 'china', ',', 'mostram', 'documentos']\n",
            "['relatorio', 'de', 'alerta', 'ao', 'brasil', 'aponta', 'que', 'coronavac', 'nao', 'ira', 'controlar', 'pandemia']\n",
            "['nova', 'regra', 'do', 'whatsapp', 'gera', 'onda', 'de', 'desinformacao', 'sobre', 'o', 'aplicativo']\n",
            "['lula', 'doou', 'r', '$', '25', 'milhoes', 'para', 'o', 'hamas', 'quando', 'era', 'presidente']\n",
            "['lula', 'diz', 'que', 'negros', 'sao', 'vagabundos', 'e', 'escravos']\n",
            "['pazuello', 'diz', 'nao', 'estou', 'aqui', 'para', 'satisfazer', 'fetiche', 'de', 'gazela', 'para', 'randolfe']\n",
            "['irmao', 'antonio', 'pede', 'compartilhamento', 'para', 'crianca', 'ganhar', 'r', '$', '1', 'por', 'foto', 'no', 'whatsapp']\n",
            "['a', 'jogatina', 'e', 'banida', 'no', 'brasil', 'e', 'e', 'possivel', 'ganhar', 'a', 'vida', 'como', 'apostador']\n",
            "['anthony', 'fauci', 'diz', ',', 'em', 'e-mail', 'vazado', ',', 'que', 'coronavirus', 'e', 'fruto', 'de', 'engenharia', 'genetica']\n",
            "['foto', 'mostra', 'tese', 'de', 'doutorado', 'da', 'dra', '.', 'nise', 'yamaguchi', 'escrita', 'a', 'mao']\n",
            "['luana', 'araujo', 'e', 'uma', 'medica', 'falsa', 'e', 'a', 'mascara', 'dela', 'caiu']\n",
            "['video', 'mostra', '40', 'mil', 'bois', 'em', 'carretas', 'na', 'fazenda', 'de', 'lula', 'em', 'sao', 'felix', 'do', 'xingu', '(', 'pa', ')']\n",
            "['washington', 'post', 'destaca', 'motociata', 'de', 'bolsonaro', 'como', 'maior', 'tour', 'de', 'moto', 'em', 'capa']\n",
            "['globo', 'admite', 'que', 'motociata', 'de', 'bolsonaro', 'e', 'a', 'maior', 'do', 'mundo']\n",
            "['astrazeneca', 'anuncia', 'que', 'vacina', 'contra', 'covid-19', 'falhou']\n",
            "['anthony', 'fauci', 'vai', 'ser', 'preso', 'pelo', 'senado', 'dos', 'eua']\n",
            "['lula', 'e', 'haddad', 'posam', 'com', 'faixa', 'lazaro', 'e', 'inocente']\n",
            "['facebook', 'esta', 'bloqueando', 'posts', 'com', 'bolsonaro', 'reeleito', 'na', 'rede', 'social']\n",
            "['mulher', 'passou', 'fome', 'e', 'comeu', 'um', 'gato', 'por', 'causa', 'de', 'lockdown', 'em', 'araraquara']\n",
            "['cristiano', 'ronaldofoiculpadopelaquedanas', 'acoes', 'da', 'coca', 'cola']\n",
            "['reporter', 'laurene', 'santos', ',', 'atacada', 'por', 'bolsonaro', ',', 'foi', 'flagrada', 'sem', 'mascara', 'durante', 'a', 'pandemia']\n",
            "['fatima', 'bernardes', 'questiona', 'se', 'houve', 'excesso', 'da', 'policia', 'na', 'morte', 'de', 'lazaro', 'barbosa']\n",
            "['dilma', 'rousseff', 'faz', 'comentario', 'sobre', 'segunda', 'dose', 'da', 'vacina', 'e', 'comete', 'gafe']\n",
            "['forcas', 'armadas', 'do', 'chile', 'entram', 'em', 'acao', ',', 'ficam', 'ao', 'lado', 'do', 'presidente', 'sebastian', 'pinera', 'e', 'declaram', 'guerra', 'aos', 'socialistas', 'comunistas']\n",
            "['joao', 'doria', 'e', 'internado', 'com', 'endocardite', 'bacteriana']\n",
            "['policia', 'federal', 'desenterra', '26', 'caixoes', 'com', 'pedras', 'de', 'vitimas', 'da', 'covid-19']\n",
            "['14', 'salario', 'para', 'aposentados', 'e', 'pensionistas', 'do', 'inss', 'e', 'aprovado']\n",
            "['onda', 'de', 'frio', 'mais', 'forte', 'desde', '1955', 'esta', 'prevista', 'nos', 'proximos', 'dias']\n",
            "['foto', 'mostra', 'rodovia', 'asfaltada', '(', 'sp-321', ')', 'pelo', 'governo', 'bolsonaro']\n",
            "['apagao', 'no', 'cnpq', 'resultou', 'em', 'perda', 'de', 'dados', 'por', 'falta', 'de', 'backup', 'na', 'placa', 'do', 'servidor']\n",
            "['neve', 'em', 'curitiba', 'faz', 'gelo', 'cobrir', 'pista', 'da', 'linha', 'verde', 'hoje', ',', 'mostra', 'video']\n",
            "['italo', 'ferreira', 'disse', 'meu', 'maior', 'premio', 'seria', 'ver', 'o', 'lula', 'na', 'cadeia', 'ao', 'new', 'york', 'times']\n",
            "['vaza', 'video', 'de', 'joice', 'hasselmann', 'se', 'machucando', 'sozinha', 'enquanto', 'dorme']\n",
            "['brasil', 'pode', 'ser', 'expulso', 'das', 'olimpiadas', 'por', 'causa', 'de', 'sopa', 'de', 'sushi']\n",
            "['diluvio', 'na', 'china', 'ja', 'matou', 'mais', 'de', '6', 'mil', 'pessoas', 'no', 'tunel', 'jingguang']\n",
            "['contato']\n",
            "['autores']\n",
            "['pesquise']\n",
            "['politica', 'de', 'privacidade']\n",
            "['assai', 'atacadista', 'esta', 'doando', 'alcool', 'gel', 'e', 'cestas', 'basicas', 'contra', 'o', 'novo', 'coronavirus']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Removendo as stopwords na coluna que tem a notícia tokenizada\n",
        "contador4 = 0\n",
        "for row in df['message_norm_treatment_ssw2']:\n",
        "  if(contador4 in indices):\n",
        "    df['message_norm_treatment_ssw2'][contador4] = [word for word in row if word not in stops]\n",
        "  contador4+=1"
      ],
      "metadata": {
        "id": "z88PPr9vZUGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Removendo as stopwords avançadas na coluna que tem a notícia tokenizada\n",
        "contador = 0\n",
        "for row in df['message_norm_treatment_ssw2']:\n",
        "  if(contador in indices):\n",
        "    df['message_norm_treatment_ssw2'][contador] = [word for word in row if ((word not in lista_final_stopwords) and (len(word)>3))]\n",
        "  contador+=1"
      ],
      "metadata": {
        "id": "e3iUq-IEXh-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['message_norm_treatment_ssw2']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkAlTePccQHX",
        "outputId": "62f037f0-8aa3-4b0b-c6e6-ed81ad9b7490"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0             [simpsons, episodio, manifestacoes, brasil]\n",
              "1       ['alerta', 'geral', 'atencao', 'fanta', 'propa...\n",
              "2       ['novo', 'mega', 'campeao', 'brasil', 'enrique...\n",
              "3       [lulinha, filho, lula, comprou, aviao, gulfstr...\n",
              "4       [pastor, sergio, helder, chutou, santa, virou,...\n",
              "                              ...                        \n",
              "5196                                            [contato]\n",
              "5197                                            [autores]\n",
              "5198                                           [pesquise]\n",
              "5199                              [politica, privacidade]\n",
              "5200    [assai, atacadista, doando, alcool, cestas, ba...\n",
              "Name: message_norm_treatment_ssw2, Length: 5201, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transforma_maiusculas_para_minusculas = lambda x: str(x).lower()\n",
        "# Aplica a função lambda em todos os elementos do dataframe\n",
        "df['message_norm_treatment_ssw2'] = df['message_norm_treatment_ssw2'].apply(transforma_maiusculas_para_minusculas)"
      ],
      "metadata": {
        "id": "oDOlla0urtlz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['datetime'] = pd.to_datetime(df['datetime'], errors='coerce')\n",
        "df_antes_2016 = df[df[\"datetime\"].dt.year < 2016]"
      ],
      "metadata": {
        "id": "UZuL_1pnmaAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['datetime'] = pd.to_datetime(df['datetime'], errors='coerce')\n",
        "df_2016 = df[df[\"datetime\"].dt.year == 2016]"
      ],
      "metadata": {
        "id": "kqynJnq5maAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['datetime'] = pd.to_datetime(df['datetime'], errors='coerce')\n",
        "df_2017 = df[df[\"datetime\"].dt.year == 2017]"
      ],
      "metadata": {
        "id": "3vNjTuVwmaAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['datetime'] = pd.to_datetime(df['datetime'], errors='coerce')\n",
        "df_2018 = df[df[\"datetime\"].dt.year == 2018]"
      ],
      "metadata": {
        "id": "VwWsgAcRmaAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['datetime'] = pd.to_datetime(df['datetime'], errors='coerce')\n",
        "df_2019 = df[df[\"datetime\"].dt.year == 2019]"
      ],
      "metadata": {
        "id": "HVPU-mlfmaAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['datetime'] = pd.to_datetime(df['datetime'], errors='coerce')\n",
        "df_2020 = df[df[\"datetime\"].dt.year == 2020]"
      ],
      "metadata": {
        "id": "BnD1xj_7maAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['datetime'] = pd.to_datetime(df['datetime'], errors='coerce')\n",
        "df_2021 = df[df[\"datetime\"].dt.year == 2021]"
      ],
      "metadata": {
        "id": "t_eKDsoWmaAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SEPARAÇÃO TEMPORAL"
      ],
      "metadata": {
        "id": "IiRwBgsRIwQ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2013-2015"
      ],
      "metadata": {
        "id": "1A86EaUHI0GQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trienio = df_antes_2016['message_norm_treatment_ssw2']"
      ],
      "metadata": {
        "id": "oY2RqEruIFe6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trienio = trienio.apply(lambda x: ' '.join(x))"
      ],
      "metadata": {
        "id": "DXpEsxCPIFil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trienio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55E3Jto7IFpG",
        "outputId": "3438c30f-d994-4c36-b23a-b95c64618e59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      [ ' s i m p s o n s ' ,   ' e p i s o d i o ' ...\n",
              "1      [ ' a l e r t a ' ,   ' g e r a l ' ,   ' a t ...\n",
              "2      [ ' n o v o ' ,   ' m e g a ' ,   ' c a m p e ...\n",
              "3      [ ' l u l i n h a ' ,   ' f i l h o ' ,   ' l ...\n",
              "4      [ ' p a s t o r ' ,   ' s e r g i o ' ,   ' h ...\n",
              "                             ...                        \n",
              "215    [ ' v e r d a d e ' ,   ' p o b r e ' ,   ' e ...\n",
              "216    [ ' n o i t e ' ,   ' p e s s o a l ' ,   ' f ...\n",
              "217    [ ' c r i s t i a n i s m o ' ,   ' b a n i d ...\n",
              "218    [ ' p a p a i ' ,   ' n o e l ' ,   ' i n v e ...\n",
              "219    [ ' s a i b a ' ,   ' d i a s ' ,   ' 2 0 1 5 ...\n",
              "Name: message_norm_treatment_ssw2, Length: 220, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2016"
      ],
      "metadata": {
        "id": "FUxBWDukJBMx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tm_2016 = df_2016['message_norm_treatment_ssw2']"
      ],
      "metadata": {
        "id": "Nz3sauKKIFsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tm_2016 = tm_2016.apply(lambda x: ' '.join(x))"
      ],
      "metadata": {
        "id": "SP-jXI0qIFvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tm_2016"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DgwBF4wYJQOo",
        "outputId": "decb78e7-94c3-435e-fd18-9df391c8d812"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "220    [ ' b a t e u ' ,   ' f i l h a ' ,   ' a n o ...\n",
              "221    [ ' f i l h a ' ,   ' r e n a n ' ,   ' c a l ...\n",
              "222    [ ' m a m a e ' ,   ' a t e n c a o ' ,   ' s ...\n",
              "223    [ ' u r g e n t e ' ,   ' d i g a ' ,   ' c o ...\n",
              "224    [ ' v e r d a d e ' ,   ' p r e - s a l ' ,   ...\n",
              "                             ...                        \n",
              "787    [ ' v i d e o ' ,   ' f l a g r a ' ,   ' s u ...\n",
              "788    [ ' a d o r e i ' ,   ' n o v a ' ,   ' f u n ...\n",
              "789    [ ' l u t e i ' ,   ' d i t a d u r a ' ,   ' ...\n",
              "790    [ ' m a r c o s ' ,   ' t r i g u e i r o ' , ...\n",
              "791    [ ' d o a c a o ' ,   ' f i l h o t e s ' ,   ...\n",
              "Name: message_norm_treatment_ssw2, Length: 572, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2017"
      ],
      "metadata": {
        "id": "vasNvDQgJVPa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tm_2017 = df_2017['message_norm_treatment_ssw2']"
      ],
      "metadata": {
        "id": "8X1M7JIEJQRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tm_2017 = tm_2017.apply(lambda x: ' '.join(x))"
      ],
      "metadata": {
        "id": "ZWrP38fxJQT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tm_2017"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilzigLm0JQWw",
        "outputId": "f77ce088-91df-4c82-d3dd-4f474614830f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "792     [ ' m a d r u g a d a ' ,   ' f a l l e c i o ...\n",
              "793     [ ' p i t b u l l ' ,   ' a t a c a ' ,   ' l ...\n",
              "794     [ ' m o t o r i s t a ' ,   ' u b e r ' ,   ' ...\n",
              "795     [ ' 2 0 1 7 ' ,   ' m a g i c o . 0 1 - 0 1 - ...\n",
              "796     [ ' f a m i l i a ' ,   ' o b a m a ' ,   ' d ...\n",
              "                              ...                        \n",
              "1526    [ ' s h a k e s p e a r e ' ,   ' s i n t o ' ...\n",
              "1527    [ ' c a r t a o ' ,   ' c r e d i t o ' ,   ' ...\n",
              "1528    [ ' c a r l o s ' ,   ' d i r e t o r ' ,   ' ...\n",
              "1529    [ ' p o l i c i a ' ,   ' f e d e r a l ' ,   ...\n",
              "1530    [ ' t h o m a s ' ,   ' b e a t i e ' ,   ' r ...\n",
              "Name: message_norm_treatment_ssw2, Length: 739, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2018"
      ],
      "metadata": {
        "id": "czKIEsWwJeIS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tm_2018 = df_2018['message_norm_treatment_ssw2']"
      ],
      "metadata": {
        "id": "2LOdJndmJQYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tm_2018 = tm_2018.apply(lambda x: ' '.join(x))"
      ],
      "metadata": {
        "id": "ACPqvA7IJQbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tm_2018"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-HUKPKIJQdl",
        "outputId": "65b9478c-2752-428d-b09e-6b9be34dc70b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1531    [ ' d o r i a ' ,   ' c a n c e l a ' ,   ' h ...\n",
              "1532    [ ' i r m a o ' ,   ' l u l a ' ,   ' r e v e ...\n",
              "1533    [ ' b u r g e r ' ,   ' k i n g ' ,   ' d a n ...\n",
              "1534    [ ' v i r a d a ' ,   ' t i j u c a ' ,   ' q ...\n",
              "1535    [ ' a s s a i ' ,   ' a t a c a d i s t a ' , ...\n",
              "                              ...                        \n",
              "2560    [ ' i n s c r i c o e s ' ,   ' p r o j e t o ...\n",
              "2561    [ ' c o m p r e i ' ,   ' m e l a n c i a ' , ...\n",
              "2562    [ ' n e t o ' ,   ' r a u l ' ,   ' s e i x a ...\n",
              "2563    [ ' a n o s ' ,   ' f a b i o ' ,   ' j u n i ...\n",
              "2564    [ ' g o v e r n o ' ,   ' g r o e l a n d i a ...\n",
              "Name: message_norm_treatment_ssw2, Length: 1034, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2019"
      ],
      "metadata": {
        "id": "xyTUdzpOJpOy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tm_2019 = df_2019['message_norm_treatment_ssw2']"
      ],
      "metadata": {
        "id": "pMJb15JcJQgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tm_2019 = tm_2019.apply(lambda x: ' '.join(x))"
      ],
      "metadata": {
        "id": "BvsQTu7fJQiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tm_2019"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "brp0NJzxJQk4",
        "outputId": "7d7a087b-8a1d-461b-fade-fed2dc3e5348"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2565    [ ' b o l s o n a r o ' ,   ' p e d e ' ,   ' ...\n",
              "2566    [ ' m i c h e l l e ' ,   ' b o l s o n a r o ...\n",
              "2567    [ ' t a r d e ' ,   ' p e s s o a l ' ,   ' i ...\n",
              "2568    [ ' a c o r d a r ' ,   ' a m e r i c a ' ,   ...\n",
              "2569    [ ' s i t e ' ,   ' s e g u r a d o r a ' ,   ...\n",
              "                              ...                        \n",
              "3438    [ ' r e d e ' ,   ' g l o b o ' ,   ' c o n t ...\n",
              "3439    [ ' j o s e ' ,   ' a b r e u ' ,   ' p e g o ...\n",
              "3440    [ ' v e r d a d e i r o ' ,   ' m a n e * f l ...\n",
              "3441    [ ' v e j a ' ,   ' r e a l ' ,   ' m o t i v ...\n",
              "3442    [ ' c i d a d a o ' ,   ' f a b i o ' ,   ' p ...\n",
              "Name: message_norm_treatment_ssw2, Length: 878, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2020"
      ],
      "metadata": {
        "id": "-B31z2cpJw76"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tm_2020 = df_2020['message_norm_treatment_ssw2']"
      ],
      "metadata": {
        "id": "96exfc9HJQm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tm_2020 = tm_2020.apply(lambda x: ' '.join(x))"
      ],
      "metadata": {
        "id": "9kR38HD0JQoz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tm_2020"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDmmMgDIJQrP",
        "outputId": "e9326313-ed3c-4fd5-e97c-dd995ca4728a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3443    [ ' l u l i n h a ' ,   ' c p f s ' ,   ' c a ...\n",
              "3444    [ ' g o v e r n o ' ,   ' f e d e r a l ' ,   ...\n",
              "3445    [ ' a p o i a r ' ,   ' b o l s o n a r o ' , ...\n",
              "3446    [ ' c a r r o ' ,   ' m o v i d o ' ,   ' a g ...\n",
              "3447    [ ' r e s p o n d a ' ,   ' p e r g u n t a s ...\n",
              "                              ...                        \n",
              "4600    [ ' h o j e ' ,   ' j a p a o ' ,   ' f o r m ...\n",
              "4601    [ ' m a i o r ' ,   ' f r a u d e ' ,   ' h u ...\n",
              "4602    [ ' h o m e m ' ,   ' m o r r e u ' ,   ' a t ...\n",
              "4603    [ ' j o r n a l i s t a ' ,   ' f i c a d o ' ...\n",
              "4604    [ ' a l e r t a r ' ,   ' r e s e t ' ,   ' m ...\n",
              "Name: message_norm_treatment_ssw2, Length: 1162, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2021"
      ],
      "metadata": {
        "id": "PbWJPQ6uJ41L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tm_2021 = df_2021['message_norm_treatment_ssw2']"
      ],
      "metadata": {
        "id": "PSFyDylkJQti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tm_2021 = tm_2021.apply(lambda x: ' '.join(x))"
      ],
      "metadata": {
        "id": "dCQO19FTJQv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tm_2021"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKqNMjj2JQyH",
        "outputId": "66b0f17d-ceda-47d5-bf24-83b53d9db2eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4605    [ ' m a r i o ' ,   ' s e r g i o ' ,   ' c o ...\n",
              "4606    [ ' v a c i n a ' ,   ' p f i z e r ' ,   ' a ...\n",
              "4607    [ ' d e n u n c i a ' ,   ' g r a v i s s i m ...\n",
              "4608    [ ' c a r a ' ,   ' p a r i s ' ,   ' e n c h ...\n",
              "4609    [ ' a t e n c a o ' ,   ' b r a s i l ' ,   ' ...\n",
              "                              ...                        \n",
              "5191    [ ' b r a s i l ' ,   ' e x p u l s o ' ,   ' ...\n",
              "5192    [ ' d i l u v i o ' ,   ' c h i n a ' ,   ' m ...\n",
              "5193    [ ' m a r i a n a ' ,   ' g o d o y ' ,   ' g ...\n",
              "5194    [ ' m i l i t a r ' ,   ' p r e n d e ' ,   ' ...\n",
              "5195    [ ' g e n t e ' ,   ' v e l h o ' ,   ' l u l ...\n",
              "Name: message_norm_treatment_ssw2, Length: 591, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SEPARAÇÃO CATEGÓRICA"
      ],
      "metadata": {
        "id": "aW_yufZ2KA-j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Política"
      ],
      "metadata": {
        "id": "jcElGdbBKF7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "aux = df.loc[df.category == 'Política']\n",
        "tm_politica = aux['message_norm_treatment_ssw2']"
      ],
      "metadata": {
        "id": "7kR5QW0cJQ0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tm_politica = tm_politica.apply(lambda x: ''.join(x))"
      ],
      "metadata": {
        "id": "7WIS_vXlJQ2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tm_politica"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOj3lsXxJQ4p",
        "outputId": "89974487-cd74-40ed-b465-31eff7bea2cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2       ['novo', 'mega', 'campeao', 'brasil', 'enrique...\n",
              "3       ['lulinha', 'filho', 'lula', 'comprou', 'aviao...\n",
              "5       ['manda', 'nesse', 'pais', 'sento', 'quiser', ...\n",
              "6       ['mensagem', 'falsa', 'conta', 'briga', 'luiz'...\n",
              "20      ['foto', 'dilma', 'rousseff', 'fuzil', 'montag...\n",
              "                              ...                        \n",
              "5176    ['estadao', 'exclusivo', 'farsa', 'revelada', ...\n",
              "5179    ['urgente', 'exercito', 'determina', 'voto', '...\n",
              "5189    ['vaza', 'video', 'joice', 'hasselmann', 'mach...\n",
              "5190    ['bolsonaro', 'casamento', 'isolada', 'ex-mulh...\n",
              "5195    ['gente', 'velho', 'luladrao', 'endoideceu', '...\n",
              "Name: message_norm_treatment_ssw2, Length: 1508, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Brasil"
      ],
      "metadata": {
        "id": "M_uX6ufUKO1s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "aux = df.loc[df.category == 'Brasil']\n",
        "tm_brasil = aux['message_norm_treatment_ssw2']"
      ],
      "metadata": {
        "id": "CpolEGHpJQ7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tm_brasil = tm_brasil.apply(lambda x: ''.join(x))"
      ],
      "metadata": {
        "id": "dAr_W7NcJQ9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tm_brasil"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shF-Lp8HJQ_x",
        "outputId": "0f242c6b-7a1d-453a-fcec-1d60bcb9f29c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8       ['natanael', 'bufalo', 'matou', 'menina', 'ano...\n",
              "13      ['senha', 'invertida', 'avisa', 'policia', 'ca...\n",
              "19      ['explica', 'video', 'ovni', 'agudos-sp', 'par...\n",
              "21      ['royal', 'canin', 'dona', 'instituto', 'royal...\n",
              "27      ['ashley', 'flores', 'desaparecida', 'duas', '...\n",
              "                              ...                        \n",
              "5178    ['foto', 'mostra', 'rodovia', 'asfaltada', 'sp...\n",
              "5183    ['olha', 'frio', 'chegando', 'parana', 'chuva'...\n",
              "5185    ['produtores', 'tomam', 'medidas', 'evitar', '...\n",
              "5186    ['neve', 'curitiba', 'gelo', 'cobrir', 'pista'...\n",
              "5193    ['mariana', 'godoy', 'globo', 'redetv', 'suica...\n",
              "Name: message_norm_treatment_ssw2, Length: 1013, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saúde"
      ],
      "metadata": {
        "id": "d3LyPuLxKYus"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "aux = df.loc[df.category == 'Saúde']\n",
        "tm_saude = aux['message_norm_treatment_ssw2']"
      ],
      "metadata": {
        "id": "kStc3Wy4JRBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tm_saude = tm_saude.apply(lambda x: ''.join(x))"
      ],
      "metadata": {
        "id": "Gl7_mPp1JRD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tm_saude"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_-hkLf2JRGe",
        "outputId": "b4761099-8f15-4ca2-e978-19548b03ba28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1       ['alerta', 'geral', 'atencao', 'fanta', 'propa...\n",
              "10      ['bomba', 'saiba', 'querer', 'comer', 'gosta',...\n",
              "12      ['mensagem', 'falsa', 'rato', 'encontrado', 'a...\n",
              "14                            ['*agua', 'dormir**.cerca']\n",
              "16      ['inca', 'precisa', 'doacoes', 'toucas', 'cria...\n",
              "                              ...                        \n",
              "5151    ['gente', 'reclassificados', 'olhem', 'legal',...\n",
              "5163    ['policia', 'federal', 'desenterra', 'caixoes'...\n",
              "5166    ['linda', 'homenagem', 'hospital', 'lucas', 'c...\n",
              "5171    ['informacao', 'infectado', 'covid', 'vezes', ...\n",
              "5182    ['hospital', 'lucas-puc', 'amanhaceu', 'pacien...\n",
              "Name: message_norm_treatment_ssw2, Length: 673, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mundo"
      ],
      "metadata": {
        "id": "1-IQildsKfkU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "aux = df.loc[df.category == 'Mundo']\n",
        "tm_mundo = aux['message_norm_treatment_ssw2']"
      ],
      "metadata": {
        "id": "WuOeeFUjJRIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tm_mundo = tm_mundo.apply(lambda x: ''.join(x))"
      ],
      "metadata": {
        "id": "EEdcdQmrJRK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tm_mundo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLwrbayFJRNS",
        "outputId": "cb7a7d27-1d8d-45a0-8906-57298e4b04b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9       ['mentira', 'chineses', 'comem', 'bebes', 'aum...\n",
              "15      ['atencao', 'garotas', 'filipinas', 'pisoteara...\n",
              "18      ['homem', 'filmado', 'agredindo', 'menino', 'a...\n",
              "22      ['video', 'carro', 'salva', 'aviao', 'tragedia...\n",
              "28      ['criatura', 'estranha', 'escala', 'predio', '...\n",
              "                              ...                        \n",
              "5170    ['cubanos', 'jogam', 'artistas', 'jornalistas'...\n",
              "5174    ['argentina', 'normal', 'argentina', 'fechado'...\n",
              "5181    ['fazendeiros', 'franceses', 'capricharam', 'p...\n",
              "5192    ['diluvio', 'china', 'matou', 'pessoas', 'tune...\n",
              "5194    ['militar', 'prende', 'bill', 'gates', 'terca-...\n",
              "Name: message_norm_treatment_ssw2, Length: 530, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ciência"
      ],
      "metadata": {
        "id": "0VwhKrCaKofE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "aux = df.loc[df.category == 'Ciência']\n",
        "tm_ciencia = aux['message_norm_treatment_ssw2']"
      ],
      "metadata": {
        "id": "rV_mNWU3JRP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tm_ciencia = tm_ciencia.apply(lambda x: ''.join(x))"
      ],
      "metadata": {
        "id": "vdmrLRlkJRSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tm_ciencia"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4VtaWEeJRU9",
        "outputId": "82daee2b-a1d7-4ba7-96bf-d2b20536edd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "51      ['teoria', 'falsa', 'masaru', 'emoto', 'palavr...\n",
              "63      ['ilusao', 'otica', 'carro', 'fantasma', 'apar...\n",
              "98      ['mito', 'pessoas', 'letra', 'feia', 'intelige...\n",
              "114     ['tempo', 'ouvido', 'falar', 'famoso', 'bio-ch...\n",
              "122     ['ilusao', 'otica', 'foto', 'vestido', 'branco...\n",
              "                              ...                        \n",
              "4699    ['*atencao*', 'fenomeno', 'equinocio', 'afetar...\n",
              "4770    ['imagens', 'marte', 'durante', 'noite', 'envi...\n",
              "4945                         ['ovnis', 'jaragua', 'sul/']\n",
              "5021    ['imagine', 'sentar-se', 'neste', 'lugar', 'du...\n",
              "5175    ['sabe', 'ensinar', 'precisa', 'saber', 'situa...\n",
              "Name: message_norm_treatment_ssw2, Length: 73, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Esporte"
      ],
      "metadata": {
        "id": "nHBCeg6jKxyd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "aux = df.loc[df.category == 'Esporte']\n",
        "tm_esporte = aux['message_norm_treatment_ssw2']"
      ],
      "metadata": {
        "id": "itBhcLUFJRWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tm_esporte = tm_esporte.apply(lambda x: ''.join(x))"
      ],
      "metadata": {
        "id": "YX0Qgh6EJRZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tm_esporte"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wz5XRXqIJRbn",
        "outputId": "3f4ce9c5-83b5-4671-ac06-059c6a0b657f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "38      ['virus', 'logotipo', 'copa', 'mundo', 'mensag...\n",
              "40      ['copa', 'possibilidade', 'copa', 'brasil', 'c...\n",
              "42      ['materia', 'falsa', 'france', 'football', 'co...\n",
              "60      ['lista', 'falsa', 'convocados', 'brasil', 'co...\n",
              "72      ['selecao', 'japonesa', 'viajou', 'copa', 'mun...\n",
              "                              ...                        \n",
              "5153    ['declaracao', 'tanta', 'perna', 'gente', 'rui...\n",
              "5154    ['futebol', 'atacante', 'bahia', 'assume', 'ho...\n",
              "5187    ['italo', 'ferreira', 'maior', 'premio', 'lula...\n",
              "5188               ['custo', 'vila', 'olimpica', 'japao']\n",
              "5191    ['brasil', 'expulso', 'olimpiadas', 'causa', '...\n",
              "Name: message_norm_treatment_ssw2, Length: 217, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Religião"
      ],
      "metadata": {
        "id": "X1RORXpHK9N9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "aux = df.loc[df.category == 'Religião']\n",
        "tm_religiao = aux['message_norm_treatment_ssw2']"
      ],
      "metadata": {
        "id": "n0a0x4CjJRd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tm_religiao = tm_religiao.apply(lambda x: ''.join(x))"
      ],
      "metadata": {
        "id": "WkVgJ-QAJRgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tm_religiao"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjWcFD2iJRlY",
        "outputId": "72b2b2a8-661a-48df-ce73-8fe9aa27ec99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4       ['pastor', 'sergio', 'helder', 'chutou', 'sant...\n",
              "7       ['vestidinho', 'branco', 'historia', 'menina',...\n",
              "24      ['ficcao', 'jovem', 'humilhado', 'cristao', 'j...\n",
              "25      ['conforme', 'publicacao', 'jornal', 'portugue...\n",
              "29      ['montagem', 'serpente', 'sete', 'cabecas', 'e...\n",
              "                              ...                        \n",
              "4887    ['irmaos', 'passando', 'grupos', 'pedido', 'ur...\n",
              "4979    ['letra', 'musica', 'psicografia', 'chico', 'x...\n",
              "5041    ['brasil', 'inteiro', 'corpus', 'christi', 'po...\n",
              "5057    ['noticia_urgente', 'pastor', 'soares', 'funda...\n",
              "5161    ['janeiro', 'estado', 'critico', 'olha', 'most...\n",
              "Name: message_norm_treatment_ssw2, Length: 245, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Entretenimento"
      ],
      "metadata": {
        "id": "BULYAHEALOdW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "aux = df.loc[df.category == 'Entretenimento']\n",
        "tm_entretenimento = aux['message_norm_treatment_ssw2']"
      ],
      "metadata": {
        "id": "3mh5V60tLIhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tm_entretenimento = tm_entretenimento.apply(lambda x: ''.join(x))"
      ],
      "metadata": {
        "id": "gNZC74ZoLIj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tm_entretenimento"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-LheWZNLIml",
        "outputId": "a0406fb4-4bca-45a2-9a36-04c4c6da4871"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       ['simpsons', 'episodio', 'manifestacoes', 'bra...\n",
              "35      ['avril', 'lavigne', 'morreu', 'substituida', ...\n",
              "47      ['foto', 'justin', 'bieber', 'beijando', 'home...\n",
              "55      ['michael', 'jackson', 'vivo', 'teoria', 'cant...\n",
              "78      ['ator', 'sacha', 'baron', 'derruba', 'atriz',...\n",
              "                              ...                        \n",
              "5039    ['*filme', 'intitulado*', '*sorry*', 'dura', '...\n",
              "5108    ['fatima', 'bernardes', 'questiona', 'excesso'...\n",
              "5140    ['gabriel', 'garcia', 'marquez', 'retirou', 'v...\n",
              "5145    ['inacreditavel', 'coral', 'francisco', 'conve...\n",
              "5180    ['parte', 'quadro', 'funcionarios', 'globo', '...\n",
              "Name: message_norm_treatment_ssw2, Length: 431, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tecnologia"
      ],
      "metadata": {
        "id": "gBqVwNYLLXO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "aux = df.loc[df.category == 'Tecnologia']\n",
        "tm_tecnologia = aux['message_norm_treatment_ssw2']"
      ],
      "metadata": {
        "id": "8fFt9wGXLIp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tm_tecnologia = tm_tecnologia.apply(lambda x: ''.join(x))"
      ],
      "metadata": {
        "id": "8F2QciYFLIr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tm_tecnologia"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ty9WN8ddLIuh",
        "outputId": "7649a44a-6be3-4eba-f1a2-f4a82b460a85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11      ['virus', 'homem', 'teste', 'fidelidade', 'des...\n",
              "17      ['facebook', 'doara', 'compartilhamento', 'tra...\n",
              "31      ['ultra', 'slim', 'virus', 'facebook', 'produt...\n",
              "32      ['virus', 'mulher', 'descobre', 'traicao', 'ma...\n",
              "34      ['numero', 'celular', 'clona', 'numero', 'repa...\n",
              "                              ...                        \n",
              "5136    ['auxilio', 'cesta', 'basica', 'beneficio', 'a...\n",
              "5137    ['mercado', 'livre', 'parabens', 'promocao', '...\n",
              "5160    ['vivo', 'informa', 'parabens', 'ganhar', 'sup...\n",
              "5172    ['americanas', 'comemoracao', 'aniversario', '...\n",
              "5184    ['apagao', 'cnpq', 'resultou', 'perda', 'dados...\n",
              "Name: message_norm_treatment_ssw2, Length: 480, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SEPARAÇÃO GERAL"
      ],
      "metadata": {
        "id": "WXmsz_sjLeVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tm_geral = df['message_norm_treatment_ssw2']"
      ],
      "metadata": {
        "id": "HG6uKfDwLIxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tm_geral = tm_geral.apply(lambda x: ''.join(x))"
      ],
      "metadata": {
        "id": "FPWpI48mLIz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tm_geral"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2ERUG2TLI2a",
        "outputId": "a98cca69-0cc2-46ef-f90a-72b2c6312cc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       ['simpsons', 'episodio', 'manifestacoes', 'bra...\n",
              "1       ['alerta', 'geral', 'atencao', 'fanta', 'propa...\n",
              "2       ['novo', 'mega', 'campeao', 'brasil', 'enrique...\n",
              "3       ['lulinha', 'filho', 'lula', 'comprou', 'aviao...\n",
              "4       ['pastor', 'sergio', 'helder', 'chutou', 'sant...\n",
              "                              ...                        \n",
              "5196                                          ['contato']\n",
              "5197                                          ['autores']\n",
              "5198                                         ['pesquise']\n",
              "5199                          ['politica', 'privacidade']\n",
              "5200    ['assai', 'atacadista', 'doando', 'alcool', 'c...\n",
              "Name: message_norm_treatment_ssw2, Length: 5201, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords(texts):\n",
        "    return [[word for word in simple_preprocess(str(doc)) \n",
        "             if word not in stop_words] for doc in texts]\n",
        "def make_bigrams(texts):\n",
        "    return [bigram_mod[doc] for doc in texts]\n",
        "\n",
        "def make_trigrams(texts):\n",
        "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
        "\n",
        "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
        "    texts_out = []\n",
        "    for sent in texts:\n",
        "        doc = nlp(\" \".join(sent)) \n",
        "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
        "    return texts_out"
      ],
      "metadata": {
        "id": "VJPNhDVyoNOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Método LSA - APLICAÇÃO NO DATASET DO TRIÊNIO"
      ],
      "metadata": {
        "id": "Qnn3sG_Mo_Zu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = stopwords.words('portuguese')\n",
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        # deacc=True Remove pontuações\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
        "data = df_antes_2016.message_norm_treatment_ssw2.values.tolist()\n",
        "data_words = list(sent_to_words(data))\n",
        "print(data_words[:1][0][:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9wC6PylNckk",
        "outputId": "6bcc9d26-d03b-4e3c-d1d1-97bf48612a55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['simpsons', 'episodio', 'manifestacoes', 'brasil']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cria os modelos de bigrama e trigrama\n",
        "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # Quanto maior o limite, menos frases.\n",
        "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
        "\n",
        "# Maneira mais rápida de obter uma frase batida como um trigrama/bigrama\n",
        "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "trigram_mod = gensim.models.phrases.Phraser(trigram)"
      ],
      "metadata": {
        "id": "tzWpTdPkoFOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removendo as stopwords\n",
        "data_words_nostops = remove_stopwords(data_words)\n",
        "\n",
        "# Formando os bigramas\n",
        "data_words_bigrams = make_bigrams(data_words_nostops)\n",
        "\n",
        "# Inicializando o modelo spacy 'pt', mantendo apenas o componente tagger (para eficiência)\n",
        "nlp = spacy.load(\"pt_core_news_lg\", disable=['parser', 'ner'])\n",
        "\n",
        "# Fazendo lematização mantendo apenas substantivo, adjetivo, verbo e advérbio\n",
        "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
        "\n",
        "print(data_lemmatized[:1][0][:30])"
      ],
      "metadata": {
        "id": "VKKfNQWSoRkv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "550a8c8c-1642-47bd-841e-50a6070822c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['simpsons', 'episodio', 'Manifestacoes']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Criando dicionário\n",
        "id2word = corpora.Dictionary(data_lemmatized)\n",
        "\n",
        "# Criando Corpus\n",
        "texts = data_lemmatized\n",
        "\n",
        "# Term Document Frequency\n",
        "corpus = [id2word.doc2bow(text) for text in texts]\n",
        "bow = corpus"
      ],
      "metadata": {
        "id": "jZ3QqWDjLJF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Achando o valor de coerência com diferentes números de tópicos\n",
        "for i in range(4,21):\n",
        "    lsi = LsiModel(bow, num_topics=i, id2word=id2word)\n",
        "    coherence_model = CoherenceModel(model=lsi, texts=texts, dictionary=id2word, coherence='c_v')\n",
        "    coherence_score = coherence_model.get_coherence()\n",
        "    print('Coherence score with {} clusters: {}'.format(i, coherence_score))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7uGv0BTzwJ1x",
        "outputId": "2ed081bd-000b-47a7-9f87-38aba54dc82e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coherence score with 4 clusters: 0.5675587871756065\n",
            "Coherence score with 5 clusters: 0.4687616570007854\n",
            "Coherence score with 6 clusters: 0.6105897752849471\n",
            "Coherence score with 7 clusters: 0.6384412013605857\n",
            "Coherence score with 8 clusters: 0.6304464129527062\n",
            "Coherence score with 9 clusters: 0.6329607003224733\n",
            "Coherence score with 10 clusters: 0.6807247404380237\n",
            "Coherence score with 11 clusters: 0.5906255630994702\n",
            "Coherence score with 12 clusters: 0.5662916701540214\n",
            "Coherence score with 13 clusters: 0.5910384701656697\n",
            "Coherence score with 14 clusters: 0.627687239495169\n",
            "Coherence score with 15 clusters: 0.6260988241708346\n",
            "Coherence score with 16 clusters: 0.5873204935594551\n",
            "Coherence score with 17 clusters: 0.6791579282176348\n",
            "Coherence score with 18 clusters: 0.5634694067011867\n",
            "Coherence score with 19 clusters: 0.5312251289433598\n",
            "Coherence score with 20 clusters: 0.5989581084103374\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perfomando SVD na bag of words com o LsiModel para extrair o número ótimo de tópicos\n",
        "lsi = LsiModel(bow, num_topics=10, id2word=id2word)"
      ],
      "metadata": {
        "id": "Uxg2gn7Cx0wY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Achando as 10 palavras com maior associação com os tópicos derivados\n",
        "for topic_num, words in lsi.print_topics(num_words=10):\n",
        "    print('Words in {}: {}.'.format(topic_num, words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pn5Drh5xyFl3",
        "outputId": "ced1b243-a2e1-49d5-cde5-f3a44cfc1f46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words in 0: 0.309*\"fazenda\" + 0.258*\"querer\" + 0.199*\"falar\" + 0.161*\"animal\" + 0.153*\"pessoa\" + 0.146*\"tomar\" + 0.132*\"vaca\" + 0.132*\"saber\" + 0.131*\"presidente\" + 0.119*\"lugar\".\n",
            "Words in 1: -0.408*\"fazenda\" + 0.278*\"querer\" + -0.212*\"animal\" + 0.197*\"falar\" + -0.181*\"vaca\" + 0.155*\"lugar\" + 0.129*\"mandar\" + -0.122*\"possivel\" + 0.098*\"Filho\" + 0.094*\"dizer\".\n",
            "Words in 2: 0.398*\"cama\" + 0.309*\"gazin\" + 0.221*\"saquinho\" + 0.214*\"terra\" + 0.205*\"comprar\" + 0.177*\"cemiterio\" + 0.137*\"video\" + 0.137*\"produto\" + 0.135*\"dormir\" + 0.130*\"tirar\".\n",
            "Words in 3: -0.330*\"atleta\" + -0.283*\"gesto\" + -0.211*\"presidente\" + -0.181*\"brasileiro\" + -0.141*\"proibir\" + -0.140*\"imprensar\" + -0.138*\"respeito\" + -0.123*\"berro\" + 0.108*\"lugar\" + -0.098*\"manifestar\".\n",
            "Words in 4: -0.358*\"pessoa\" + -0.128*\"passar\" + 0.120*\"cama\" + -0.118*\"tomar\" + -0.112*\"enviar\" + -0.110*\"problema\" + -0.107*\"pesquisador\" + 0.107*\"fazenda\" + -0.106*\"doenca\" + 0.103*\"comprar\".\n",
            "Words in 5: -0.341*\"enviar\" + -0.209*\"crime\" + -0.161*\"referir\" + -0.161*\"direito\" + -0.160*\"jornalista\" + -0.157*\"governador\" + -0.157*\"cooperativismo\" + -0.157*\"carta\" + -0.156*\"artigo\" + -0.112*\"apenas\".\n",
            "Words in 6: 0.291*\"filme\" + 0.212*\"malevola\" + -0.143*\"pessoa\" + -0.136*\"doenca\" + -0.135*\"pesquisador\" + -0.123*\"problema\" + 0.112*\"tomar\" + 0.111*\"filho\" + -0.108*\"tratamento\" + -0.106*\"deixar\".\n",
            "Words in 7: 0.322*\"evitar\" + 0.269*\"mude\" + 0.269*\"rumo\" + 0.215*\"colisao\" + 0.169*\"favor\" + -0.163*\"filme\" + 0.163*\"norte\" + 0.162*\"curso\" + 0.161*\"navio\" + 0.142*\"falar\".\n",
            "Words in 8: -0.150*\"malevola\" + -0.135*\"filme\" + -0.130*\"evitar\" + -0.121*\"pesquisador\" + -0.120*\"doenca\" + 0.120*\"realizar\" + -0.116*\"mude\" + -0.116*\"rumo\" + 0.115*\"acelerar\" + 0.114*\"candido\".\n",
            "Words in 9: 0.465*\"fracasso\" + 0.216*\"falar\" + 0.179*\"trabalho\" + 0.174*\"humanamente\" + 0.174*\"fruto\" + 0.157*\"carne\" + 0.153*\"humano\" + 0.137*\"morrer\" + 0.131*\"acabar\" + 0.121*\"sucesso\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Método LSA - APLICAÇÃO NO DATASET DE 2016"
      ],
      "metadata": {
        "id": "wFqCpF8FYKBg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = stopwords.words('portuguese')\n",
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        # deacc=True removes punctuations\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
        "data = df_2016.message_norm_treatment_ssw2.values.tolist()\n",
        "data_words = list(sent_to_words(data))\n",
        "print(data_words[:1][0][:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2992733d-b64f-4fca-a6a3-f611779c7d26",
        "id": "Y168velcYKBh"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['bateu', 'filha', 'anos', 'morto', 'cadeia']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cria os modelos de bigrama e trigrama\n",
        "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # Quanto maior o limite, menos frases.\n",
        "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
        "\n",
        "# Maneira mais rápida de obter uma frase batida como um trigrama/bigrama\n",
        "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "trigram_mod = gensim.models.phrases.Phraser(trigram)"
      ],
      "metadata": {
        "id": "-wDIxQ72YKBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removendo as stopwords\n",
        "data_words_nostops = remove_stopwords(data_words)\n",
        "\n",
        "# Formando os bigramas\n",
        "data_words_bigrams = make_bigrams(data_words_nostops)\n",
        "\n",
        "# Inicializando o modelo spacy 'pt', mantendo apenas o componente tagger (para eficiência)\n",
        "nlp = spacy.load(\"pt_core_news_lg\", disable=['parser', 'ner'])\n",
        "\n",
        "# Fazendo lematização mantendo apenas substantivo, adjetivo, verbo e advérbio\n",
        "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
        "\n",
        "print(data_lemmatized[:1][0][:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93aaa877-3353-46fa-d498-2ec3bed41f5c",
        "id": "-coPeMzZYKBj"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['bater', 'filha', 'ano', 'matar', 'cadeia']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Criando dicionário\n",
        "id2word = corpora.Dictionary(data_lemmatized)\n",
        "\n",
        "# Criando Corpus\n",
        "texts = data_lemmatized\n",
        "\n",
        "# Term Document Frequency\n",
        "corpus = [id2word.doc2bow(text) for text in texts]\n",
        "bow = corpus"
      ],
      "metadata": {
        "id": "cEX3nMJsYKBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Achando o valor de coerência com diferentes números de tópicos\n",
        "for i in range(4,21):\n",
        "    lsi = LsiModel(bow, num_topics=i, id2word=id2word)\n",
        "    coherence_model = CoherenceModel(model=lsi, texts=texts, dictionary=id2word, coherence='c_v')\n",
        "    coherence_score = coherence_model.get_coherence()\n",
        "    print('Coherence score with {} clusters: {}'.format(i, coherence_score))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc89e590-bd97-4048-8250-70664389419f",
        "id": "cdULuchJYKBk"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coherence score with 4 clusters: 0.3601223585716831\n",
            "Coherence score with 5 clusters: 0.4200098413182922\n",
            "Coherence score with 6 clusters: 0.3783588215575953\n",
            "Coherence score with 7 clusters: 0.48148869251048076\n",
            "Coherence score with 8 clusters: 0.3393936359755879\n",
            "Coherence score with 9 clusters: 0.36311351220787974\n",
            "Coherence score with 10 clusters: 0.4367344018542312\n",
            "Coherence score with 11 clusters: 0.439042625106601\n",
            "Coherence score with 12 clusters: 0.40893412039043375\n",
            "Coherence score with 13 clusters: 0.38847866593168623\n",
            "Coherence score with 14 clusters: 0.37593353162808535\n",
            "Coherence score with 15 clusters: 0.3805879368523829\n",
            "Coherence score with 16 clusters: 0.3929771625413423\n",
            "Coherence score with 17 clusters: 0.3788098071174185\n",
            "Coherence score with 18 clusters: 0.35790696529039373\n",
            "Coherence score with 19 clusters: 0.39770318965462736\n",
            "Coherence score with 20 clusters: 0.3149658127579696\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perfomando SVD na bag of words com o LsiModel para extrair o número ótimo de tópicos\n",
        "lsi = LsiModel(bow, num_topics=7, id2word=id2word)"
      ],
      "metadata": {
        "id": "Jq0dKXTzYKBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Achando as 10 palavras com maior associação com os tópicos derivados\n",
        "for topic_num, words in lsi.print_topics(num_words=10):\n",
        "    print('Words in {}: {}.'.format(topic_num, words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44ad658f-cd14-4d82-9cfc-ca9579528427",
        "id": "3NQIP1xkYKBl"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words in 0: -0.227*\"pessoa\" + -0.183*\"falar\" + -0.166*\"ficar\" + -0.147*\"gente\" + -0.138*\"ano\" + -0.132*\"passar\" + -0.129*\"saber\" + -0.120*\"caso\" + -0.119*\"governo\" + -0.115*\"brasileiro\".\n",
            "Words in 1: 0.266*\"falar\" + -0.223*\"causar\" + 0.180*\"filha\" + -0.176*\"quimico\" + 0.175*\"senhor\" + -0.163*\"microcefalia\" + -0.161*\"produto\" + -0.145*\"ministerio_saude\" + -0.143*\"vacina\" + 0.142*\"gente\".\n",
            "Words in 2: -0.315*\"falar\" + -0.253*\"filha\" + -0.238*\"senhor\" + -0.179*\"caso\" + -0.142*\"shopping\" + 0.133*\"governo\" + -0.125*\"causar\" + -0.118*\"forte\" + -0.114*\"estrutura\" + -0.113*\"juntar\".\n",
            "Words in 3: -0.385*\"mulher\" + -0.171*\"caso\" + -0.156*\"direito\" + -0.150*\"denunciar\" + -0.148*\"pico_maximo\" + 0.147*\"quimico\" + -0.146*\"vacino\" + 0.138*\"produto\" + -0.129*\"explicar\" + -0.128*\"provocar\".\n",
            "Words in 4: -0.227*\"governo\" + -0.210*\"brasileiro\" + 0.181*\"mulher\" + -0.154*\"congresso\" + -0.144*\"boliviano\" + 0.140*\"ficar\" + 0.140*\"crianca\" + 0.135*\"gente\" + -0.132*\"empresa\" + -0.129*\"presidente\".\n",
            "Words in 5: -0.397*\"mulher\" + -0.229*\"direito\" + -0.145*\"considerar\" + -0.144*\"ano\" + -0.138*\"filha\" + -0.125*\"senhor\" + 0.124*\"ficar\" + -0.119*\"saudita\" + 0.116*\"denunciar\" + 0.111*\"vacino\".\n",
            "Words in 6: 0.651*\"frango\" + 0.203*\"carne\" + 0.192*\"arsenico\" + 0.169*\"comer\" + 0.153*\"nitro\" + 0.128*\"saude\" + 0.118*\"nivel\" + 0.118*\"novo\" + 0.116*\"risco\" + -0.101*\"grupo\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Método LSA - APLICAÇÃO NO DATASET DE 2017"
      ],
      "metadata": {
        "id": "eA8kfcvoYLRY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = stopwords.words('portuguese')\n",
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        # deacc=True removes punctuations\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
        "data = df_2017.message_norm_treatment_ssw2.values.tolist()\n",
        "data_words = list(sent_to_words(data))\n",
        "print(data_words[:1][0][:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8213cec2-27bc-4039-9533-85995cdf42b0",
        "id": "mNQD1r1gYLRY"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['madrugada', 'fallecio', 'cesar', 'millan', 'encantador', 'encuentra', 'mundo', 'tras', 'noticia', 'dejado', 'paralizados', 'manana', 'quedo', 'oficialmente', 'confirmada', 'diversos', 'medios', 'comunicacion', 'muerte', 'famoso', 'encantador', 'perros', 'cesar', 'millan', 'quien', 'fallecio', 'madrugada', 'ayer', 'tras', 'sufrir']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cria os modelos de bigrama e trigrama\n",
        "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # Quanto maior o limite, menos frases.\n",
        "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
        "\n",
        "# Maneira mais rápida de obter uma frase batida como um trigrama/bigrama\n",
        "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "trigram_mod = gensim.models.phrases.Phraser(trigram)"
      ],
      "metadata": {
        "id": "O-RiHcKBYLRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removendo as stopwords\n",
        "data_words_nostops = remove_stopwords(data_words)\n",
        "\n",
        "# Formando os bigramas\n",
        "data_words_bigrams = make_bigrams(data_words_nostops)\n",
        "\n",
        "# Inicializando o modelo spacy 'pt', mantendo apenas o componente tagger (para eficiência)\n",
        "nlp = spacy.load(\"pt_core_news_lg\", disable=['parser', 'ner'])\n",
        "\n",
        "# Fazendo lematização mantendo apenas substantivo, adjetivo, verbo e advérbio\n",
        "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
        "\n",
        "print(data_lemmatized[:1][0][:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b59261e-a77f-4e55-cd37-2dadd1ded4f7",
        "id": "JTZLeQqrYLRa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['madrugada', 'encantador', 'encuentrar', 'Mundo', 'tras', 'noticiar', 'dejar', 'paralizar', 'Manana', 'quedo', 'oficialmente', 'confirmar', 'medio', 'muerte', 'famoso', 'encantador', 'perro', 'madrugar', 'ayer', 'tras', 'sufrir', 'fulminate', 'paro', 'cardiaco', 'interior', 'domicilio', 'ano', 'quien', 'hiciera', 'famoso']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Criando dicionário\n",
        "id2word = corpora.Dictionary(data_lemmatized)\n",
        "\n",
        "# Criando Corpus\n",
        "texts = data_lemmatized\n",
        "\n",
        "# Term Document Frequency\n",
        "corpus = [id2word.doc2bow(text) for text in texts]\n",
        "bow = corpus"
      ],
      "metadata": {
        "id": "QcWQHSn5YLRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Achando o valor de coerência com diferentes números de tópicos\n",
        "for i in range(4,21):\n",
        "    lsi = LsiModel(bow, num_topics=i, id2word=id2word)\n",
        "    coherence_model = CoherenceModel(model=lsi, texts=texts, dictionary=id2word, coherence='c_v')\n",
        "    coherence_score = coherence_model.get_coherence()\n",
        "    print('Coherence score with {} clusters: {}'.format(i, coherence_score))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e96f94a-4e47-4a3d-84a7-77257193cf36",
        "id": "h-UCJF72YLRb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coherence score with 4 clusters: 0.5397637210449002\n",
            "Coherence score with 5 clusters: 0.4012392169427711\n",
            "Coherence score with 6 clusters: 0.5252097662844052\n",
            "Coherence score with 7 clusters: 0.4332926207849132\n",
            "Coherence score with 8 clusters: 0.41760680438983144\n",
            "Coherence score with 9 clusters: 0.5237880127100976\n",
            "Coherence score with 10 clusters: 0.4563213029734256\n",
            "Coherence score with 11 clusters: 0.4288841113444784\n",
            "Coherence score with 12 clusters: 0.40869573426364925\n",
            "Coherence score with 13 clusters: 0.4869226221989567\n",
            "Coherence score with 14 clusters: 0.4646666380898606\n",
            "Coherence score with 15 clusters: 0.3662567444314577\n",
            "Coherence score with 16 clusters: 0.43895856360998364\n",
            "Coherence score with 17 clusters: 0.39305509068286704\n",
            "Coherence score with 18 clusters: 0.35753972153295804\n",
            "Coherence score with 19 clusters: 0.453041078132057\n",
            "Coherence score with 20 clusters: 0.4271790843661892\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perfomando SVD na bag of words com o LsiModel para extrair o número ótimo de tópicos\n",
        "lsi = LsiModel(bow, num_topics=4, id2word=id2word)"
      ],
      "metadata": {
        "id": "3RWO3CBtYLRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Achando as 10 palavras com maior associação com os tópicos derivados\n",
        "for topic_num, words in lsi.print_topics(num_words=10):\n",
        "    print('Words in {}: {}.'.format(topic_num, words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc48ba5b-15d9-4af3-e61f-97ccc3ec125c",
        "id": "EVG-2hiqYLRc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words in 0: -0.281*\"falar\" + -0.243*\"pessoa\" + -0.180*\"saber\" + -0.158*\"mensagem\" + -0.129*\"acontecer\" + -0.123*\"gente\" + -0.122*\"passar\" + -0.121*\"ano\" + -0.119*\"querer\" + -0.113*\"entrar\".\n",
            "Words in 1: -0.317*\"globo\" + -0.267*\"temer\" + -0.250*\"jornalismo\" + 0.222*\"mensagem\" + -0.211*\"rede_globo\" + -0.209*\"alinhar\" + -0.206*\"noticia\" + 0.140*\"contato\" + -0.126*\"jornal\" + -0.125*\"vincular\".\n",
            "Words in 2: -0.371*\"desconto\" + -0.343*\"aereo\" + -0.341*\"passagem\" + -0.284*\"idoso\" + -0.278*\"mensagem\" + -0.233*\"contato\" + -0.189*\"empresa\" + 0.118*\"falar\" + 0.103*\"gente\" + -0.099*\"grande\".\n",
            "Words in 3: 0.616*\"fruta\" + 0.349*\"comer\" + 0.218*\"estomago\" + 0.200*\"mensagem\" + 0.150*\"comida\" + 0.148*\"paciente\" + 0.142*\"vazio\" + 0.141*\"cancro\" + 0.127*\"forma\" + 0.105*\"fatia\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Método LSA - APLICAÇÃO NO DATASET DE 2018"
      ],
      "metadata": {
        "id": "rUzJh5BiYL1Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = stopwords.words('portuguese')\n",
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        # deacc=True removes punctuations\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
        "data = df_2018.message_norm_treatment_ssw2.values.tolist()\n",
        "data_words = list(sent_to_words(data))\n",
        "print(data_words[:1][0][:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd31fb6e-a403-4b54-a38c-48363310f43a",
        "id": "s5k8zr1hYL1Y"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['doria', 'cancela', 'homenagem', 'dona', 'marisa', 'viaduto', 'nome', 'professora', 'heley']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cria os modelos de bigrama e trigrama\n",
        "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # Quanto maior o limite, menos frases.\n",
        "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
        "\n",
        "# Maneira mais rápida de obter uma frase batida como um trigrama/bigrama\n",
        "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "trigram_mod = gensim.models.phrases.Phraser(trigram)"
      ],
      "metadata": {
        "id": "q_u5sTfIYL1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removendo as stopwords\n",
        "data_words_nostops = remove_stopwords(data_words)\n",
        "\n",
        "# Formando os bigramas\n",
        "data_words_bigrams = make_bigrams(data_words_nostops)\n",
        "\n",
        "# Inicializando o modelo spacy 'pt', mantendo apenas o componente tagger (para eficiência)\n",
        "nlp = spacy.load(\"pt_core_news_lg\", disable=['parser', 'ner'])\n",
        "\n",
        "# Fazendo lematização mantendo apenas substantivo, adjetivo, verbo e advérbio\n",
        "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
        "\n",
        "print(data_lemmatized[:1][0][:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df5bafe0-a635-481c-9565-7e330367754d",
        "id": "KesgU5S0YL1Z"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['dorio', 'cancelar', 'homenagem', 'viaduto', 'nome', 'professora']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Criando dicionário\n",
        "id2word = corpora.Dictionary(data_lemmatized)\n",
        "\n",
        "# Criando Corpus\n",
        "texts = data_lemmatized\n",
        "\n",
        "# Term Document Frequency\n",
        "corpus = [id2word.doc2bow(text) for text in texts]\n",
        "bow = corpus"
      ],
      "metadata": {
        "id": "uuulRBgpYL1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Achando o valor de coerência com diferentes números de tópicos\n",
        "for i in range(4,21):\n",
        "    lsi = LsiModel(bow, num_topics=i, id2word=id2word)\n",
        "    coherence_model = CoherenceModel(model=lsi, texts=texts, dictionary=id2word, coherence='c_v')\n",
        "    coherence_score = coherence_model.get_coherence()\n",
        "    print('Coherence score with {} clusters: {}'.format(i, coherence_score))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d137c10-4812-4a2f-9e55-4329d9c9105e",
        "id": "g8GAyacwYL1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coherence score with 4 clusters: 0.6170136460648865\n",
            "Coherence score with 5 clusters: 0.5306272292926292\n",
            "Coherence score with 6 clusters: 0.55104340074215\n",
            "Coherence score with 7 clusters: 0.5793893917360678\n",
            "Coherence score with 8 clusters: 0.575312105362538\n",
            "Coherence score with 9 clusters: 0.5403506208723301\n",
            "Coherence score with 10 clusters: 0.5167271927556939\n",
            "Coherence score with 11 clusters: 0.4654535988429147\n",
            "Coherence score with 12 clusters: 0.5218433268937495\n",
            "Coherence score with 13 clusters: 0.5596056810505786\n",
            "Coherence score with 14 clusters: 0.4982495892087561\n",
            "Coherence score with 15 clusters: 0.4747003951664345\n",
            "Coherence score with 16 clusters: 0.47545036075581304\n",
            "Coherence score with 17 clusters: 0.46875204311967483\n",
            "Coherence score with 18 clusters: 0.4436449975136594\n",
            "Coherence score with 19 clusters: 0.4431174842508744\n",
            "Coherence score with 20 clusters: 0.46166792991221184\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perfomando SVD na bag of words com o LsiModel para extrair o número ótimo de tópicos\n",
        "lsi = LsiModel(bow, num_topics=4, id2word=id2word)"
      ],
      "metadata": {
        "id": "7L9DnOEkYL1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Achando as 10 palavras com maior associação com os tópicos derivados\n",
        "for topic_num, words in lsi.print_topics(num_words=10):\n",
        "    print('Words in {}: {}.'.format(topic_num, words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36beef49-0c11-4417-be4e-280e7fb992bd",
        "id": "-DVizaYjYL1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words in 0: 0.234*\"bolsonaro\" + 0.209*\"pessoa\" + 0.179*\"ficar\" + 0.158*\"querer\" + 0.155*\"passar\" + 0.146*\"saber\" + 0.144*\"ano\" + 0.133*\"falar\" + 0.127*\"gente\" + 0.122*\"presidente\".\n",
            "Words in 1: 0.329*\"forca\" + 0.251*\"area\" + 0.193*\"auxiliar\" + 0.176*\"delegar\" + 0.153*\"militar\" + 0.151*\"intervencao_militar\" + 0.150*\"policia\" + 0.148*\"quartel\" + 0.144*\"estadual\" + 0.144*\"especial\".\n",
            "Words in 2: -0.660*\"bolsonaro\" + -0.195*\"candidato\" + -0.160*\"voto\" + 0.152*\"passar\" + 0.137*\"ajudar\" + -0.120*\"votar\" + -0.106*\"presidente\" + -0.105*\"eleicao\" + -0.104*\"urna\" + 0.100*\"mensagem\".\n",
            "Words in 3: -0.298*\"olhar\" + 0.270*\"gripe\" + -0.211*\"comer\" + -0.183*\"passar\" + 0.181*\"vacino\" + 0.178*\"paciente\" + 0.172*\"chegar\" + 0.171*\"ano\" + 0.163*\"tomar\" + 0.146*\"falar\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Método LSA - APLICAÇÃO NO DATASET DE 2019"
      ],
      "metadata": {
        "id": "wnuhnKkXYMVA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = stopwords.words('portuguese')\n",
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        # deacc=True removes punctuations\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
        "data = df_2019.message_norm_treatment_ssw2.values.tolist()\n",
        "data_words = list(sent_to_words(data))\n",
        "print(data_words[:1][0][:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dafae019-b31f-4d99-8c00-006afe98e309",
        "id": "0oo9Il8YYMVB"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['bolsonaro', 'pede', 'eleitores', 'ignorem', 'partidos', 'vermelhos', 'anti', 'brasileiros']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cria os modelos de bigrama e trigrama\n",
        "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # Quanto maior o limite, menos frases.\n",
        "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
        "\n",
        "# Maneira mais rápida de obter uma frase batida como um trigrama/bigrama\n",
        "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "trigram_mod = gensim.models.phrases.Phraser(trigram)"
      ],
      "metadata": {
        "id": "TfBHKeX7YMVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removendo as stopwords\n",
        "data_words_nostops = remove_stopwords(data_words)\n",
        "\n",
        "# Formando os bigramas\n",
        "data_words_bigrams = make_bigrams(data_words_nostops)\n",
        "\n",
        "# Inicializando o modelo spacy 'pt', mantendo apenas o componente tagger (para eficiência)\n",
        "nlp = spacy.load(\"pt_core_news_lg\", disable=['parser', 'ner'])\n",
        "\n",
        "# Fazendo lematização mantendo apenas substantivo, adjetivo, verbo e advérbio\n",
        "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
        "\n",
        "print(data_lemmatized[:1][0][:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "766691bd-d87e-4a0a-9ba9-3b3edce466bd",
        "id": "yAQYbZkZYMVC"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['bolsonaro', 'pedir', 'eleitor', 'ignorar', 'partido', 'vermelho', 'anti', 'brasileiro']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Criando dicionário\n",
        "id2word = corpora.Dictionary(data_lemmatized)\n",
        "\n",
        "# Criando Corpus\n",
        "texts = data_lemmatized\n",
        "\n",
        "# Term Document Frequency\n",
        "corpus = [id2word.doc2bow(text) for text in texts]\n",
        "bow = corpus"
      ],
      "metadata": {
        "id": "6Ii1hEEQYMVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Achando o valor de coerência com diferentes números de tópicos\n",
        "for i in range(4,21):\n",
        "    lsi = LsiModel(bow, num_topics=i, id2word=id2word)\n",
        "    coherence_model = CoherenceModel(model=lsi, texts=texts, dictionary=id2word, coherence='c_v')\n",
        "    coherence_score = coherence_model.get_coherence()\n",
        "    print('Coherence score with {} clusters: {}'.format(i, coherence_score))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "937ad17f-be4d-428b-cf97-e7b034201b5f",
        "id": "lZsb4pUzYMVD"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coherence score with 4 clusters: 0.4893616792842391\n",
            "Coherence score with 5 clusters: 0.4306672809253927\n",
            "Coherence score with 6 clusters: 0.4506440865643499\n",
            "Coherence score with 7 clusters: 0.4025194802910517\n",
            "Coherence score with 8 clusters: 0.42321495100192497\n",
            "Coherence score with 9 clusters: 0.45382509691904194\n",
            "Coherence score with 10 clusters: 0.37477498194245673\n",
            "Coherence score with 11 clusters: 0.4393273895794817\n",
            "Coherence score with 12 clusters: 0.3731483057181304\n",
            "Coherence score with 13 clusters: 0.3823829570799054\n",
            "Coherence score with 14 clusters: 0.4163257680291113\n",
            "Coherence score with 15 clusters: 0.4190800365043979\n",
            "Coherence score with 16 clusters: 0.3879640771296204\n",
            "Coherence score with 17 clusters: 0.4038570532141187\n",
            "Coherence score with 18 clusters: 0.3951694144545029\n",
            "Coherence score with 19 clusters: 0.3689787193107883\n",
            "Coherence score with 20 clusters: 0.378096246309253\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perfomando SVD na bag of words com o LsiModel para extrair o número ótimo de tópicos\n",
        "lsi = LsiModel(bow, num_topics=4, id2word=id2word)"
      ],
      "metadata": {
        "id": "5ZC-0qoQYMVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Achando as 10 palavras com maior associação com os tópicos derivados\n",
        "for topic_num, words in lsi.print_topics(num_words=10):\n",
        "    print('Words in {}: {}.'.format(topic_num, words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fb9059a-ed11-4d24-e385-e486de634425",
        "id": "bsU2Eq3_YMVE"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words in 0: -0.346*\"acabar\" + -0.263*\"pessoa\" + -0.180*\"ano\" + -0.179*\"falar\" + -0.169*\"presidente\" + -0.155*\"bolsonaro\" + -0.144*\"aumento\" + -0.144*\"salario\" + -0.144*\"partido\" + -0.141*\"ficar\".\n",
            "Words in 1: 0.475*\"acabar\" + 0.247*\"aumento\" + -0.214*\"presidente\" + 0.211*\"salario\" + 0.208*\"partido\" + -0.195*\"falar\" + 0.189*\"aprovar\" + 0.138*\"redutor\" + 0.138*\"aposentado\" + 0.135*\"reduzir\".\n",
            "Words in 2: -0.519*\"presidente\" + -0.356*\"bolsonaro\" + 0.168*\"abaixo\" + 0.162*\"amigo\" + 0.159*\"site\" + 0.153*\"pessoa\" + -0.149*\"brasileiro\" + 0.126*\"compartilhar\" + 0.121*\"receber\" + 0.120*\"casa\".\n",
            "Words in 3: -0.326*\"abaixo\" + -0.323*\"site\" + 0.311*\"falar\" + 0.237*\"pessoa\" + -0.208*\"receber\" + -0.197*\"compartilhar\" + -0.150*\"presidente\" + 0.128*\"matar\" + -0.105*\"ganhar\" + -0.099*\"mensagem\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Método LSA - APLICAÇÃO NO DATASET DE 2020"
      ],
      "metadata": {
        "id": "WLOgrkEAYMsY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = stopwords.words('portuguese')\n",
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        # deacc=True removes punctuations\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
        "data = df_2020.message_norm_treatment_ssw2.values.tolist()\n",
        "data_words = list(sent_to_words(data))\n",
        "print(data_words[:1][0][:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ff9921d-3eab-4823-d2f5-ff79ad63c58d",
        "id": "EeguVVMXYMsZ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['lulinha', 'cpfs', 'cadeia', 'nesse', 'filho', 'puta', 'puta', 'parte', 'receita', 'federal', 'descobre', 'lulinha', 'alterava', 'data', 'nascimento', 'filiacao', 'receita', 'federal', 'descobre', 'lulinha', 'dono', 'alterava', 'filiacao', 'data', 'nascimento']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cria os modelos de bigrama e trigrama\n",
        "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # Quanto maior o limite, menos frases.\n",
        "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
        "\n",
        "# Maneira mais rápida de obter uma frase batida como um trigrama/bigrama\n",
        "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "trigram_mod = gensim.models.phrases.Phraser(trigram)"
      ],
      "metadata": {
        "id": "57jkvrk0YMsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removendo as stopwords\n",
        "data_words_nostops = remove_stopwords(data_words)\n",
        "\n",
        "# Formando os bigramas\n",
        "data_words_bigrams = make_bigrams(data_words_nostops)\n",
        "\n",
        "# Inicializando o modelo spacy 'pt', mantendo apenas o componente tagger (para eficiência)\n",
        "nlp = spacy.load(\"pt_core_news_lg\", disable=['parser', 'ner'])\n",
        "\n",
        "# Fazendo lematização mantendo apenas substantivo, adjetivo, verbo e advérbio\n",
        "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
        "\n",
        "print(data_lemmatized[:1][0][:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80e4fe23-616e-42ce-f1d3-9e3c1b5fc34a",
        "id": "RqkLkUtvYMsa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['lulinha', 'cpfs', 'cadeia', 'filho', 'puto', 'puto', 'parte', 'receita', 'federal', 'descobrir', 'lulinha', 'alterar', 'data', 'nascimento', 'filiacao', 'receita', 'federal', 'descobrir', 'lulinha', 'dono', 'alterar', 'filiacao', 'data', 'nascimento']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Criando dicionário\n",
        "id2word = corpora.Dictionary(data_lemmatized)\n",
        "\n",
        "# Criando Corpus\n",
        "texts = data_lemmatized\n",
        "\n",
        "# Term Document Frequency\n",
        "corpus = [id2word.doc2bow(text) for text in texts]\n",
        "bow = corpus"
      ],
      "metadata": {
        "id": "eSGKZCq2YMsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Achando o valor de coerência com diferentes números de tópicos\n",
        "for i in range(4,21):\n",
        "    lsi = LsiModel(bow, num_topics=i, id2word=id2word)\n",
        "    coherence_model = CoherenceModel(model=lsi, texts=texts, dictionary=id2word, coherence='c_v')\n",
        "    coherence_score = coherence_model.get_coherence()\n",
        "    print('Coherence score with {} clusters: {}'.format(i, coherence_score))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6715fbba-d47e-4ff3-da63-fbace65869b3",
        "id": "Q4n02zdHYMsb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coherence score with 4 clusters: 0.3932720862521225\n",
            "Coherence score with 5 clusters: 0.4278598939116004\n",
            "Coherence score with 6 clusters: 0.49901715888414727\n",
            "Coherence score with 7 clusters: 0.33859924761402904\n",
            "Coherence score with 8 clusters: 0.3700880854861172\n",
            "Coherence score with 9 clusters: 0.33072074144711594\n",
            "Coherence score with 10 clusters: 0.4554863767483379\n",
            "Coherence score with 11 clusters: 0.391828953323175\n",
            "Coherence score with 12 clusters: 0.4098248346207552\n",
            "Coherence score with 13 clusters: 0.40824833050236037\n",
            "Coherence score with 14 clusters: 0.3787646649261011\n",
            "Coherence score with 15 clusters: 0.3960207923587834\n",
            "Coherence score with 16 clusters: 0.36823008276700264\n",
            "Coherence score with 17 clusters: 0.365346421609319\n",
            "Coherence score with 18 clusters: 0.3518221635663537\n",
            "Coherence score with 19 clusters: 0.34411307510487993\n",
            "Coherence score with 20 clusters: 0.37443521155656795\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perfomando SVD na bag of words com o LsiModel para extrair o número ótimo de tópicos\n",
        "lsi = LsiModel(bow, num_topics=6, id2word=id2word)"
      ],
      "metadata": {
        "id": "H_AoFVt-YMsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Achando as 10 palavras com maior associação com os tópicos derivados\n",
        "for topic_num, words in lsi.print_topics(num_words=10):\n",
        "    print('Words in {}: {}.'.format(topic_num, words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6922daf7-3800-422b-af10-e5a3a6c2238a",
        "id": "mW-Pr1KAYMsc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words in 0: 0.341*\"pessoa\" + 0.260*\"falar\" + 0.179*\"virus\" + 0.175*\"bolsonaro\" + 0.152*\"gente\" + 0.151*\"querer\" + 0.144*\"saber\" + 0.133*\"agua\" + 0.131*\"tomar\" + 0.120*\"ficar\".\n",
            "Words in 1: 0.437*\"bolsonaro\" + -0.286*\"agua\" + -0.247*\"falar\" + -0.175*\"cafe\" + -0.164*\"tomar\" + 0.159*\"moro\" + -0.155*\"virus\" + -0.151*\"pessoa\" + 0.143*\"brasileiro\" + -0.141*\"sangue\".\n",
            "Words in 2: -0.434*\"falar\" + -0.348*\"cafe\" + -0.343*\"bolsonaro\" + 0.215*\"pessoa\" + -0.180*\"sangue\" + -0.167*\"tomar\" + 0.161*\"virus\" + -0.102*\"moro\" + -0.095*\"descarregar\" + -0.095*\"saber\".\n",
            "Words in 3: 0.473*\"agua\" + -0.270*\"pegar\" + -0.206*\"entender\" + -0.164*\"mascara\" + -0.160*\"morrer\" + -0.157*\"querer\" + 0.145*\"corpo\" + -0.123*\"cara\" + -0.117*\"pega_mascara\" + 0.109*\"banho\".\n",
            "Words in 4: -0.280*\"agua\" + 0.245*\"cafe\" + -0.215*\"bolsonaro\" + -0.214*\"pegar\" + -0.170*\"colocar\" + 0.153*\"falar\" + 0.151*\"cadastro\" + 0.150*\"site\" + 0.144*\"novo\" + 0.135*\"tomar\".\n",
            "Words in 5: -0.366*\"pessoa\" + 0.246*\"virus\" + 0.191*\"mundial\" + 0.175*\"chinês\" + 0.130*\"empresa\" + -0.130*\"receber\" + -0.125*\"cadastro\" + 0.122*\"ano\" + 0.121*\"mundo\" + 0.119*\"guerra\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Método LSA - APLICAÇÃO NO DATASET DE 2021"
      ],
      "metadata": {
        "id": "DCsQJ4JgYNDw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = stopwords.words('portuguese')\n",
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        # deacc=True removes punctuations\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
        "data = df_2021.message_norm_treatment_ssw2.values.tolist()\n",
        "data_words = list(sent_to_words(data))\n",
        "print(data_words[:1][0][:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "679c2d2a-9603-4bf6-8408-d4e3779c4736",
        "id": "2ZueTNE4YNDx"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['mario', 'sergio', 'cortella', 'vais', 'andando', 'xicara', 'cafe', 'repente', 'empurra', 'derrames', 'cafe', 'lado', 'derramaste', 'cafe', 'empurrou', 'resposta', 'errada', 'derramaste', 'cafe', 'tinhas', 'cafe', 'caneca', 'terias', 'derramado', 'tiveres', 'xicara', 'derramar', 'portanto', 'vida', 'sacode']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cria os modelos de bigrama e trigrama\n",
        "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # Quanto maior o limite, menos frases.\n",
        "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
        "\n",
        "# Maneira mais rápida de obter uma frase batida como um trigrama/bigrama\n",
        "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "trigram_mod = gensim.models.phrases.Phraser(trigram)"
      ],
      "metadata": {
        "id": "AVVKQZoIYNDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removendo as stopwords\n",
        "data_words_nostops = remove_stopwords(data_words)\n",
        "\n",
        "# Formando os bigramas\n",
        "data_words_bigrams = make_bigrams(data_words_nostops)\n",
        "\n",
        "# Inicializando o modelo spacy 'pt', mantendo apenas o componente tagger (para eficiência)\n",
        "nlp = spacy.load(\"pt_core_news_lg\", disable=['parser', 'ner'])\n",
        "\n",
        "# Fazendo lematização mantendo apenas substantivo, adjetivo, verbo e advérbio\n",
        "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
        "\n",
        "print(data_lemmatized[:1][0][:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f85a9f8f-9665-49ae-eb00-ecd73804472e",
        "id": "f9_MjDubYNDy"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['andar', 'xicara', 'cafe', 'repente', 'empurrar', 'derrame', 'cafe', 'lado', 'derramaste', 'cafe', 'empurrar', 'resposta', 'errar', 'cafe', 'tinhas', 'cafe', 'caneca', 'terias', 'derramar', 'tiver', 'xicara', 'derramar', 'vida', 'sacoder', 'tiver', 'dentro', 'derramar', 'podes', 'vida', 'fingir']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Criando dicionário\n",
        "id2word = corpora.Dictionary(data_lemmatized)\n",
        "\n",
        "# Criando Corpus\n",
        "texts = data_lemmatized\n",
        "\n",
        "# Term Document Frequency\n",
        "corpus = [id2word.doc2bow(text) for text in texts]\n",
        "bow = corpus"
      ],
      "metadata": {
        "id": "H0BLd3z_YNDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Achando o valor de coerência com diferentes números de tópicos\n",
        "for i in range(4,21):\n",
        "    lsi = LsiModel(bow, num_topics=i, id2word=id2word)\n",
        "    coherence_model = CoherenceModel(model=lsi, texts=texts, dictionary=id2word, coherence='c_v')\n",
        "    coherence_score = coherence_model.get_coherence()\n",
        "    print('Coherence score with {} clusters: {}'.format(i, coherence_score))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e143de2a-0405-44ac-94fb-af3ab9486ebf",
        "id": "wnPxGW6pYNDz"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coherence score with 4 clusters: 0.37957942760243646\n",
            "Coherence score with 5 clusters: 0.4146550953346024\n",
            "Coherence score with 6 clusters: 0.47597472723500217\n",
            "Coherence score with 7 clusters: 0.4394051498631172\n",
            "Coherence score with 8 clusters: 0.36140864173757714\n",
            "Coherence score with 9 clusters: 0.43220260754085704\n",
            "Coherence score with 10 clusters: 0.38667968038013345\n",
            "Coherence score with 11 clusters: 0.36827747131551103\n",
            "Coherence score with 12 clusters: 0.3753148811869203\n",
            "Coherence score with 13 clusters: 0.3622325687810856\n",
            "Coherence score with 14 clusters: 0.3914975936486547\n",
            "Coherence score with 15 clusters: 0.35450709548671744\n",
            "Coherence score with 16 clusters: 0.40052355950368923\n",
            "Coherence score with 17 clusters: 0.36149399037868934\n",
            "Coherence score with 18 clusters: 0.3847828392371386\n",
            "Coherence score with 19 clusters: 0.4007233465742729\n",
            "Coherence score with 20 clusters: 0.41245460553770397\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perfomando SVD na bag of words com o LsiModel para extrair o número ótimo de tópicos\n",
        "lsi = LsiModel(bow, num_topics=6, id2word=id2word)"
      ],
      "metadata": {
        "id": "0GR-ZDJsYNDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Achando as 10 palavras com maior associação com os tópicos derivados\n",
        "for topic_num, words in lsi.print_topics(num_words=10):\n",
        "    print('Words in {}: {}.'.format(topic_num, words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26e0ca3c-8738-4e4e-afef-86bbfd2c8e49",
        "id": "mj0jvN3yYND0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words in 0: -0.235*\"pessoa\" + -0.191*\"querer\" + -0.183*\"gente\" + -0.177*\"presidente\" + -0.163*\"bolsonaro\" + -0.156*\"tomar\" + -0.148*\"vacino\" + -0.141*\"ficar\" + -0.141*\"governo\" + -0.130*\"covid\".\n",
            "Words in 1: 0.351*\"presidente\" + 0.304*\"bolsonaro\" + 0.203*\"governo\" + -0.182*\"gente\" + -0.179*\"covid\" + -0.171*\"agua\" + 0.164*\"brasileiro\" + -0.155*\"pessoa\" + 0.142*\"governador\" + 0.131*\"vacino\".\n",
            "Words in 2: 0.399*\"vacino\" + 0.258*\"tomar\" + 0.224*\"dose\" + -0.190*\"presidente\" + 0.182*\"vacina\" + 0.169*\"pessoa\" + 0.156*\"producao\" + -0.142*\"bolsonaro\" + -0.130*\"covid\" + -0.126*\"querer\".\n",
            "Words in 3: 0.267*\"governo\" + -0.238*\"presidente\" + 0.208*\"oxigenio\" + 0.149*\"governador\" + 0.147*\"producao\" + -0.137*\"tomar\" + -0.136*\"politico\" + -0.126*\"acabar\" + 0.123*\"dose\" + 0.121*\"hospital\".\n",
            "Words in 4: 0.343*\"bolsonaro\" + -0.285*\"oxigenio\" + -0.210*\"governador\" + -0.187*\"pessoa\" + 0.166*\"covid\" + 0.142*\"medico\" + 0.140*\"receber\" + 0.139*\"entregar\" + 0.120*\"governo_federal\" + -0.120*\"agua\".\n",
            "Words in 5: -0.262*\"cadastro\" + -0.250*\"beneficio\" + -0.243*\"informar\" + 0.211*\"bolsonaro\" + -0.201*\"familiar\" + 0.198*\"agua\" + 0.197*\"gente\" + -0.172*\"site\" + -0.168*\"receber\" + 0.157*\"presidente\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Método LSA - APLICAÇÃO NO DATASET DE POLÍTICA"
      ],
      "metadata": {
        "id": "fel6wYAJYNag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tm_politica"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XznlkWfYv9hr",
        "outputId": "f57c70a0-1070-45a8-f8b6-b947a6a96eff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2       ['novo', 'mega', 'campeao', 'brasil', 'enrique...\n",
              "3       ['lulinha', 'filho', 'lula', 'comprou', 'aviao...\n",
              "5       ['manda', 'nesse', 'pais', 'sento', 'quiser', ...\n",
              "6       ['mensagem', 'falsa', 'conta', 'briga', 'luiz'...\n",
              "20      ['foto', 'dilma', 'rousseff', 'fuzil', 'montag...\n",
              "                              ...                        \n",
              "5176    ['estadao', 'exclusivo', 'farsa', 'revelada', ...\n",
              "5179    ['urgente', 'exercito', 'determina', 'voto', '...\n",
              "5189    ['vaza', 'video', 'joice', 'hasselmann', 'mach...\n",
              "5190    ['bolsonaro', 'casamento', 'isolada', 'ex-mulh...\n",
              "5195    ['gente', 'velho', 'luladrao', 'endoideceu', '...\n",
              "Name: message_norm_treatment_ssw2, Length: 1508, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = stopwords.words('portuguese')\n",
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        # deacc=True removes punctuations\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
        "data = tm_politica.values.tolist()\n",
        "data_words = list(sent_to_words(data))\n",
        "print(data_words[:1][0][:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d71d741-e14d-4fc6-9bc6-67400e9a0c8b",
        "id": "wOSdHBNMYNah"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['novo', 'mega', 'campeao', 'brasil', 'enriquecimento', 'subito', 'proprietario', 'fazenda', 'fazenda', 'fortaleza', 'comprada', 'certificada', 'cartorio', 'registro', 'imoveis', 'proprietario', 'fabio', 'luis', 'lula', 'silva', 'propriedade', 'fazenda', 'regiao', 'valparaiso', 'sppreco', 'milhoes', 'reaispois', 'lulinha', 'filho', 'presidente']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cria os modelos de bigrama e trigrama\n",
        "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # Quanto maior o limite, menos frases.\n",
        "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
        "\n",
        "# Maneira mais rápida de obter uma frase batida como um trigrama/bigrama\n",
        "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "trigram_mod = gensim.models.phrases.Phraser(trigram)"
      ],
      "metadata": {
        "id": "9ux0JEgUYNah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removendo as stopwords\n",
        "data_words_nostops = remove_stopwords(data_words)\n",
        "\n",
        "# Formando os bigramas\n",
        "data_words_bigrams = make_bigrams(data_words_nostops)\n",
        "\n",
        "# Inicializando o modelo spacy 'pt', mantendo apenas o componente tagger (para eficiência)\n",
        "nlp = spacy.load(\"pt_core_news_lg\", disable=['parser', 'ner'])\n",
        "\n",
        "# Fazendo lematização mantendo apenas substantivo, adjetivo, verbo e advérbio\n",
        "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
        "\n",
        "print(data_lemmatized[:1][0][:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e46cc3a-bee9-4398-a4f4-d01e4bb09186",
        "id": "A1-EJHo4YNai"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['novo', 'mega', 'subito', 'proprietario', 'fazenda', 'fazenda', 'fortaleza', 'comprar', 'certificar', 'cartorio', 'registro', 'imovel', 'proprietario', 'propriedade', 'fazenr', 'valparaiso', 'reaispois', 'Filho', 'presidente', 'ano', 'subempregar', 'zoologico', 'acabar', 'comprar', 'fazenda', 'fortalezar', 'porteira', 'fechar', 'localizar', 'margem']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Criando dicionário\n",
        "id2word = corpora.Dictionary(data_lemmatized)\n",
        "\n",
        "# Criando Corpus\n",
        "texts = data_lemmatized\n",
        "\n",
        "# Term Document Frequency\n",
        "corpus = [id2word.doc2bow(text) for text in texts]\n",
        "bow = corpus"
      ],
      "metadata": {
        "id": "eFS4DAThYNaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Achando o valor de coerência com diferentes números de tópicos\n",
        "for i in range(4,21):\n",
        "    lsi = LsiModel(bow, num_topics=i, id2word=id2word)\n",
        "    coherence_model = CoherenceModel(model=lsi, texts=texts, dictionary=id2word, coherence='c_v')\n",
        "    coherence_score = coherence_model.get_coherence()\n",
        "    print('Coherence score with {} clusters: {}'.format(i, coherence_score))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "510919e8-7dff-441e-97ca-fe173beead4a",
        "id": "uQ6-jC7IYNaj"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coherence score with 4 clusters: 0.42332385382597004\n",
            "Coherence score with 5 clusters: 0.36535562328649485\n",
            "Coherence score with 6 clusters: 0.379086074229179\n",
            "Coherence score with 7 clusters: 0.38505650019069043\n",
            "Coherence score with 8 clusters: 0.43354323899422315\n",
            "Coherence score with 9 clusters: 0.37628461714772515\n",
            "Coherence score with 10 clusters: 0.41297152288151856\n",
            "Coherence score with 11 clusters: 0.36331394871685146\n",
            "Coherence score with 12 clusters: 0.38358330929300827\n",
            "Coherence score with 13 clusters: 0.42139258505608895\n",
            "Coherence score with 14 clusters: 0.40377741253127863\n",
            "Coherence score with 15 clusters: 0.36104569173578477\n",
            "Coherence score with 16 clusters: 0.37800386974505584\n",
            "Coherence score with 17 clusters: 0.366903503801818\n",
            "Coherence score with 18 clusters: 0.36818699789399045\n",
            "Coherence score with 19 clusters: 0.37710462767028824\n",
            "Coherence score with 20 clusters: 0.35615770026570864\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perfomando SVD na bag of words com o LsiModel para extrair o número ótimo de tópicos\n",
        "lsi = LsiModel(bow, num_topics=8, id2word=id2word)"
      ],
      "metadata": {
        "id": "64SYp8hFYNak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Achando as 10 palavras com maior associação com os tópicos derivados\n",
        "for topic_num, words in lsi.print_topics(num_words=10):\n",
        "    print('Words in {}: {}.'.format(topic_num, words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63bc930b-5e86-4ae7-9c55-67c302553f67",
        "id": "BkeQBokhYNak"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words in 0: 0.408*\"bolsonaro\" + 0.310*\"presidente\" + 0.175*\"governo\" + 0.158*\"acabar\" + 0.147*\"querer\" + 0.143*\"pessoa\" + 0.140*\"brasileiro\" + 0.129*\"saber\" + 0.124*\"falar\" + 0.120*\"pai\".\n",
            "Words in 1: 0.726*\"bolsonaro\" + -0.248*\"acabar\" + -0.139*\"ano\" + -0.133*\"pessoa\" + -0.108*\"pai\" + -0.101*\"dinheiro\" + -0.098*\"aumento\" + -0.097*\"governo\" + -0.095*\"salario\" + -0.095*\"falar\".\n",
            "Words in 2: -0.562*\"presidente\" + 0.434*\"acabar\" + 0.268*\"bolsonaro\" + 0.171*\"aumento\" + 0.153*\"partido\" + -0.151*\"governo\" + 0.130*\"aprovar\" + 0.119*\"pessoa\" + -0.113*\"brasileiro\" + -0.102*\"temer\".\n",
            "Words in 3: -0.345*\"globo\" + 0.294*\"presidente\" + -0.246*\"temer\" + -0.239*\"falar\" + -0.238*\"jornalismo\" + -0.203*\"noticia\" + -0.201*\"alinhar\" + -0.200*\"rede_globo\" + -0.133*\"jornal\" + 0.126*\"governo\".\n",
            "Words in 4: 0.517*\"presidente\" + -0.450*\"governo\" + 0.268*\"acabar\" + -0.185*\"federal\" + 0.124*\"partido\" + -0.121*\"mandar\" + 0.118*\"aumento\" + -0.111*\"brasileiro\" + -0.110*\"governador\" + -0.102*\"medico\".\n",
            "Words in 5: -0.426*\"querer\" + -0.275*\"falar\" + 0.201*\"governo\" + 0.183*\"ano\" + -0.147*\"mandar\" + -0.141*\"lugar\" + -0.124*\"povo\" + 0.120*\"acabar\" + -0.116*\"tomar\" + -0.109*\"filho\".\n",
            "Words in 6: 0.307*\"governo\" + -0.191*\"politico\" + -0.187*\"pessoa\" + -0.177*\"congresso\" + 0.157*\"acabar\" + 0.144*\"presidente\" + -0.141*\"moro\" + 0.138*\"dinheiro\" + -0.137*\"mensagem\" + 0.137*\"governador\".\n",
            "Words in 7: 0.254*\"moro\" + 0.178*\"morar\" + 0.171*\"morer\" + 0.162*\"dizer\" + -0.156*\"deputado\" + -0.146*\"voto\" + 0.145*\"acabar\" + -0.145*\"congresso\" + 0.138*\"querer\" + 0.136*\"brasileiro\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Método LSA - APLICAÇÃO NO DATASET DE BRASIL"
      ],
      "metadata": {
        "id": "uVI2TjOGYNxI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = stopwords.words('portuguese')\n",
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        # deacc=True removes punctuations\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
        "data = tm_brasil.values.tolist()\n",
        "data_words = list(sent_to_words(data))\n",
        "print(data_words[:1][0][:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5a8f7d6-a8eb-4d98-a5a5-69711e0e8c15",
        "id": "FZvebaZMYNxI"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['natanael', 'bufalo', 'matou', 'menina', 'anos', 'foragido']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cria os modelos de bigrama e trigrama\n",
        "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # Quanto maior o limite, menos frases.\n",
        "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
        "\n",
        "# Maneira mais rápida de obter uma frase batida como um trigrama/bigrama\n",
        "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "trigram_mod = gensim.models.phrases.Phraser(trigram)"
      ],
      "metadata": {
        "id": "Q7lsA55OYNxJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removendo as stopwords\n",
        "data_words_nostops = remove_stopwords(data_words)\n",
        "\n",
        "# Formando os bigramas\n",
        "data_words_bigrams = make_bigrams(data_words_nostops)\n",
        "\n",
        "# Inicializando o modelo spacy 'pt', mantendo apenas o componente tagger (para eficiência)\n",
        "nlp = spacy.load(\"pt_core_news_lg\", disable=['parser', 'ner'])\n",
        "\n",
        "# Fazendo lematização mantendo apenas substantivo, adjetivo, verbo e advérbio\n",
        "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
        "\n",
        "print(data_lemmatized[:1][0][:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "323e4c6f-7b35-4f1f-9a00-90641cc48c2e",
        "id": "htVVYW4wYNxJ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['natanael', 'bufalo', 'matar', 'menina', 'ano', 'foragido']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Criando dicionário\n",
        "id2word = corpora.Dictionary(data_lemmatized)\n",
        "\n",
        "# Criando Corpus\n",
        "texts = data_lemmatized\n",
        "\n",
        "# Term Document Frequency\n",
        "corpus = [id2word.doc2bow(text) for text in texts]\n",
        "bow = corpus"
      ],
      "metadata": {
        "id": "h9sKZ6ycYNxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Achando o valor de coerência com diferentes números de tópicos\n",
        "for i in range(4,21):\n",
        "    lsi = LsiModel(bow, num_topics=i, id2word=id2word)\n",
        "    coherence_model = CoherenceModel(model=lsi, texts=texts, dictionary=id2word, coherence='c_v')\n",
        "    coherence_score = coherence_model.get_coherence()\n",
        "    print('Coherence score with {} clusters: {}'.format(i, coherence_score))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "873fc63b-c06d-4e17-b96f-1aa36c828a26",
        "id": "W0ecGKcOYNxL"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coherence score with 4 clusters: 0.48646096263285066\n",
            "Coherence score with 5 clusters: 0.49797109354731584\n",
            "Coherence score with 6 clusters: 0.43564905195078946\n",
            "Coherence score with 7 clusters: 0.48339193246033674\n",
            "Coherence score with 8 clusters: 0.45578814257481587\n",
            "Coherence score with 9 clusters: 0.4372824166144031\n",
            "Coherence score with 10 clusters: 0.4463888842855395\n",
            "Coherence score with 11 clusters: 0.48274914620831294\n",
            "Coherence score with 12 clusters: 0.4317094517291014\n",
            "Coherence score with 13 clusters: 0.40159607544650994\n",
            "Coherence score with 14 clusters: 0.39184292025032647\n",
            "Coherence score with 15 clusters: 0.41099393982388543\n",
            "Coherence score with 16 clusters: 0.3955484093099752\n",
            "Coherence score with 17 clusters: 0.38906705705905875\n",
            "Coherence score with 18 clusters: 0.3918446811557395\n",
            "Coherence score with 19 clusters: 0.3749767019883104\n",
            "Coherence score with 20 clusters: 0.3807422776205391\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perfomando SVD na bag of words com o LsiModel para extrair o número ótimo de tópicos\n",
        "lsi = LsiModel(bow, num_topics=5, id2word=id2word)"
      ],
      "metadata": {
        "id": "v6VXFz9OYNxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Achando as 10 palavras com maior associação com os tópicos derivados\n",
        "for topic_num, words in lsi.print_topics(num_words=10):\n",
        "    print('Words in {}: {}.'.format(topic_num, words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc65b3bf-54c3-4257-bb08-3efb53623f3d",
        "id": "v0mdibG0YNxL"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words in 0: -0.235*\"falar\" + -0.217*\"pessoa\" + -0.186*\"passar\" + -0.184*\"ficar\" + -0.159*\"saber\" + -0.140*\"ano\" + -0.137*\"gente\" + -0.132*\"casa\" + -0.127*\"acontecer\" + -0.126*\"crianca\".\n",
            "Words in 1: -0.411*\"falar\" + -0.353*\"filha\" + -0.277*\"senhor\" + -0.193*\"shopping\" + -0.155*\"juntar\" + -0.155*\"estrutura\" + -0.152*\"forte\" + 0.151*\"passar\" + -0.145*\"mensagem\" + 0.126*\"crianca\".\n",
            "Words in 2: -0.292*\"forca\" + -0.240*\"area\" + -0.171*\"auxiliar\" + -0.154*\"delegar\" + -0.148*\"aereo\" + -0.148*\"partir\" + 0.137*\"crianca\" + -0.135*\"estadual\" + -0.131*\"intervencao_militar\" + -0.128*\"secretario\".\n",
            "Words in 3: -0.409*\"desconto\" + -0.384*\"passagem\" + -0.373*\"aereo\" + -0.317*\"idoso\" + -0.276*\"empresa\" + 0.127*\"forca\" + -0.106*\"ano\" + -0.101*\"grande\" + -0.099*\"adquirir\" + 0.094*\"area\".\n",
            "Words in 4: 0.344*\"crianca\" + -0.227*\"governo\" + -0.219*\"receber\" + 0.218*\"matar\" + -0.162*\"saber\" + -0.153*\"brasileiro\" + -0.149*\"trabalhar\" + 0.147*\"ficar\" + 0.138*\"aereo\" + 0.136*\"desconto\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Método LSA - APLICAÇÃO NO DATASET DE SAÚDE"
      ],
      "metadata": {
        "id": "NLTHrZy5ZvYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = stopwords.words('portuguese')\n",
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        # deacc=True removes punctuations\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
        "data = tm_saude.values.tolist()\n",
        "data_words = list(sent_to_words(data))\n",
        "print(data_words[:1][0][:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6072ba91-f8ca-4d91-d82d-5d7ce982eec4",
        "id": "p0oirfY8ZvYz"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['alerta', 'geral', 'atencao', 'fanta', 'propaganda', 'parouporque', 'reparem', 'propaganda', 'quase', 'midia', 'repassando', 'mail', 'abaixo', 'conhecimento', 'prevencao', 'principalmente', 'bebem', 'refrigerante', 'fanta', 'uva', 'este', 'mail', 'repassado', 'dentro', 'hospital', 'trabalha', 'pessoa', 'amiga', 'fato', 'confirmado']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cria os modelos de bigrama e trigrama\n",
        "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # Quanto maior o limite, menos frases.\n",
        "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
        "\n",
        "# Maneira mais rápida de obter uma frase batida como um trigrama/bigrama\n",
        "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "trigram_mod = gensim.models.phrases.Phraser(trigram)"
      ],
      "metadata": {
        "id": "JTUvoKBUZvY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removendo as stopwords\n",
        "data_words_nostops = remove_stopwords(data_words)\n",
        "\n",
        "# Formando os bigramas\n",
        "data_words_bigrams = make_bigrams(data_words_nostops)\n",
        "\n",
        "# Inicializando o modelo spacy 'pt', mantendo apenas o componente tagger (para eficiência)\n",
        "nlp = spacy.load(\"pt_core_news_lg\", disable=['parser', 'ner'])\n",
        "\n",
        "# Fazendo lematização mantendo apenas substantivo, adjetivo, verbo e advérbio\n",
        "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
        "\n",
        "print(data_lemmatized[:1][0][:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83764906-e019-4c82-e938-833842f719c2",
        "id": "FRNVovfTZvY0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['alertar', 'geral', 'atencao', 'fanto', 'propaganda', 'parouporque', 'repar', 'propaganda', 'quase', 'midia', 'repassar', 'mail', 'abaixo', 'conhecimento', 'prevencao', 'principalmente', 'beber', 'refrigerante', 'fanta', 'uva', 'mail', 'repassar', 'dentro', 'hospital', 'trabalhar', 'pessoa', 'amigo', 'fato', 'confirmar', 'tres']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Criando dicionário\n",
        "id2word = corpora.Dictionary(data_lemmatized)\n",
        "\n",
        "# Criando Corpus\n",
        "texts = data_lemmatized\n",
        "\n",
        "# Term Document Frequency\n",
        "corpus = [id2word.doc2bow(text) for text in texts]\n",
        "bow = corpus"
      ],
      "metadata": {
        "id": "Nd_zpCn8ZvY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Achando o valor de coerência com diferentes números de tópicos\n",
        "for i in range(4,21):\n",
        "    lsi = LsiModel(bow, num_topics=i, id2word=id2word)\n",
        "    coherence_model = CoherenceModel(model=lsi, texts=texts, dictionary=id2word, coherence='c_v')\n",
        "    coherence_score = coherence_model.get_coherence()\n",
        "    print('Coherence score with {} clusters: {}'.format(i, coherence_score))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66b2baa9-3844-4897-8cdf-869f1d8133a4",
        "id": "AeCaqZfbZvY1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coherence score with 4 clusters: 0.4192804304649701\n",
            "Coherence score with 5 clusters: 0.4476327092881651\n",
            "Coherence score with 6 clusters: 0.4131639973716286\n",
            "Coherence score with 7 clusters: 0.42545876871358207\n",
            "Coherence score with 8 clusters: 0.4066632280760556\n",
            "Coherence score with 9 clusters: 0.36246819484884524\n",
            "Coherence score with 10 clusters: 0.3834141873959457\n",
            "Coherence score with 11 clusters: 0.37996834197987\n",
            "Coherence score with 12 clusters: 0.3959865199863026\n",
            "Coherence score with 13 clusters: 0.3646531229139436\n",
            "Coherence score with 14 clusters: 0.38714073532665144\n",
            "Coherence score with 15 clusters: 0.4000353232697763\n",
            "Coherence score with 16 clusters: 0.3807732143295258\n",
            "Coherence score with 17 clusters: 0.3843909549916853\n",
            "Coherence score with 18 clusters: 0.379675227364975\n",
            "Coherence score with 19 clusters: 0.3853108977117394\n",
            "Coherence score with 20 clusters: 0.3649963279499212\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perfomando SVD na bag of words com o LsiModel para extrair o número ótimo de tópicos\n",
        "lsi = LsiModel(bow, num_topics=5, id2word=id2word)"
      ],
      "metadata": {
        "id": "ut8ejZSsZvY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Achando as 10 palavras com maior associação com os tópicos derivados\n",
        "for topic_num, words in lsi.print_topics(num_words=10):\n",
        "    print('Words in {}: {}.'.format(topic_num, words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0c89fec-7ca6-4d08-f3b4-c759ad2d4c31",
        "id": "7cZBNCeFZvY2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words in 0: 0.347*\"pessoa\" + 0.254*\"falar\" + 0.241*\"virus\" + 0.238*\"tomar\" + 0.210*\"gente\" + 0.195*\"agua\" + 0.130*\"pegar\" + 0.129*\"vacino\" + 0.125*\"causar\" + 0.117*\"passar\".\n",
            "Words in 1: 0.260*\"causar\" + -0.250*\"falar\" + 0.241*\"zika\" + -0.221*\"pegar\" + 0.185*\"virus\" + -0.181*\"gente\" + 0.175*\"vacina\" + -0.174*\"tomar\" + 0.164*\"quimico\" + -0.162*\"mascara\".\n",
            "Words in 2: 0.550*\"agua\" + -0.278*\"pegar\" + -0.247*\"mascara\" + 0.161*\"falar\" + 0.160*\"tomar\" + -0.119*\"entender\" + -0.116*\"governo\" + 0.107*\"sangue\" + 0.099*\"banho\" + 0.099*\"corpo\".\n",
            "Words in 3: 0.365*\"agua\" + -0.361*\"tomar\" + 0.323*\"pegar\" + 0.267*\"mascara\" + -0.266*\"vacino\" + 0.192*\"virus\" + 0.144*\"entender\" + -0.129*\"cafe\" + -0.117*\"caso\" + -0.114*\"ficar\".\n",
            "Words in 4: -0.291*\"falar\" + -0.264*\"cafe\" + -0.226*\"tomar\" + 0.222*\"comer\" + 0.178*\"fruta\" + -0.173*\"virus\" + 0.151*\"pessoa\" + -0.145*\"vacino\" + 0.135*\"hospital\" + -0.134*\"sangue\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Método LSA - APLICAÇÃO NO DATASET DE ENTRETENIMENTO"
      ],
      "metadata": {
        "id": "ktELAypJYOIg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = stopwords.words('portuguese')\n",
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        # deacc=True removes punctuations\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
        "data = tm_entretenimento.values.tolist()\n",
        "data_words = list(sent_to_words(data))\n",
        "print(data_words[:1][0][:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c332a06-0b1c-4d45-a465-e49bba59473f",
        "id": "Qd2Zf8mXYOIg"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['simpsons', 'episodio', 'manifestacoes', 'brasil']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cria os modelos de bigrama e trigrama\n",
        "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # Quanto maior o limite, menos frases.\n",
        "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
        "\n",
        "# Maneira mais rápida de obter uma frase batida como um trigrama/bigrama\n",
        "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "trigram_mod = gensim.models.phrases.Phraser(trigram)"
      ],
      "metadata": {
        "id": "xgt4Rvu_YOIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removendo as stopwords\n",
        "data_words_nostops = remove_stopwords(data_words)\n",
        "\n",
        "# Formando os bigramas\n",
        "data_words_bigrams = make_bigrams(data_words_nostops)\n",
        "\n",
        "# Inicializando o modelo spacy 'pt', mantendo apenas o componente tagger (para eficiência)\n",
        "nlp = spacy.load(\"pt_core_news_lg\", disable=['parser', 'ner'])\n",
        "\n",
        "# Fazendo lematização mantendo apenas substantivo, adjetivo, verbo e advérbio\n",
        "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
        "\n",
        "print(data_lemmatized[:1][0][:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54524e0b-2362-48ee-ac62-6d71688a2ea9",
        "id": "WU6VHj0jYOIh"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['simpsons', 'episodio', 'Manifestacoes']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Criando dicionário\n",
        "id2word = corpora.Dictionary(data_lemmatized)\n",
        "\n",
        "# Criando Corpus\n",
        "texts = data_lemmatized\n",
        "\n",
        "# Term Document Frequency\n",
        "corpus = [id2word.doc2bow(text) for text in texts]\n",
        "bow = corpus"
      ],
      "metadata": {
        "id": "Hv8eXbXOYOIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Achando o valor de coerência com diferentes números de tópicos\n",
        "for i in range(4,21):\n",
        "    lsi = LsiModel(bow, num_topics=i, id2word=id2word)\n",
        "    coherence_model = CoherenceModel(model=lsi, texts=texts, dictionary=id2word, coherence='c_v')\n",
        "    coherence_score = coherence_model.get_coherence()\n",
        "    print('Coherence score with {} clusters: {}'.format(i, coherence_score))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc5b7236-2d22-4641-cc02-4ae8b2bffdf2",
        "id": "mAWF7fpwYOIi"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coherence score with 4 clusters: 0.44730350330226704\n",
            "Coherence score with 5 clusters: 0.4065365337191934\n",
            "Coherence score with 6 clusters: 0.4056437966988479\n",
            "Coherence score with 7 clusters: 0.38839957162518196\n",
            "Coherence score with 8 clusters: 0.41994172473346913\n",
            "Coherence score with 9 clusters: 0.4122892557449871\n",
            "Coherence score with 10 clusters: 0.4205928187837372\n",
            "Coherence score with 11 clusters: 0.45206853500926447\n",
            "Coherence score with 12 clusters: 0.42862940968349\n",
            "Coherence score with 13 clusters: 0.4472102128438412\n",
            "Coherence score with 14 clusters: 0.3999109795559025\n",
            "Coherence score with 15 clusters: 0.42222136204088134\n",
            "Coherence score with 16 clusters: 0.41770937909065853\n",
            "Coherence score with 17 clusters: 0.41858797569427314\n",
            "Coherence score with 18 clusters: 0.42342848963523666\n",
            "Coherence score with 19 clusters: 0.430847797771955\n",
            "Coherence score with 20 clusters: 0.4340849681832187\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perfomando SVD na bag of words com o LsiModel para extrair o número ótimo de tópicos\n",
        "lsi = LsiModel(bow, num_topics=11, id2word=id2word)"
      ],
      "metadata": {
        "id": "7qAajDBxYOIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Achando as 10 palavras com maior associação com os tópicos derivados\n",
        "for topic_num, words in lsi.print_topics(num_words=10):\n",
        "    print('Words in {}: {}.'.format(topic_num, words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b6b56d9-e594-43b3-bfe7-711af4029f46",
        "id": "aBZ3kPwHYOIj"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words in 0: -0.212*\"querer\" + -0.188*\"pessoa\" + -0.177*\"morrer\" + -0.175*\"globo\" + -0.171*\"ano\" + -0.169*\"ficar\" + -0.165*\"vida\" + -0.160*\"hoje\" + -0.142*\"programa\" + -0.141*\"deixar\".\n",
            "Words in 1: 0.411*\"globo\" + 0.270*\"programa\" + -0.192*\"vida\" + -0.186*\"hoje\" + 0.170*\"bolsonaro\" + -0.166*\"aprender\" + -0.161*\"dizer\" + -0.142*\"tempo\" + 0.133*\"rede\" + -0.130*\"pensar\".\n",
            "Words in 2: 0.303*\"gente\" + 0.266*\"cara\" + 0.240*\"mundo\" + 0.208*\"pessoa\" + -0.172*\"luciano_huck\" + -0.147*\"investimento\" + -0.146*\"chegar\" + -0.144*\"globo\" + 0.143*\"querer\" + 0.127*\"discutir\".\n",
            "Words in 3: 0.310*\"luciano_huck\" + 0.269*\"investimento\" + 0.226*\"produto\" + 0.220*\"formular\" + -0.206*\"globo\" + 0.176*\"amostra\" + 0.149*\"pessoa\" + 0.132*\"tornar\" + 0.129*\"pesquisa\" + -0.128*\"ano\".\n",
            "Words in 4: 0.343*\"ano\" + 0.325*\"morrer\" + -0.220*\"globo\" + 0.190*\"ator\" + 0.170*\"cantor\" + -0.153*\"dizer\" + -0.142*\"bolsonaro\" + -0.141*\"programa\" + -0.136*\"deixar\" + 0.133*\"falar\".\n",
            "Words in 5: 0.319*\"programa\" + -0.318*\"presidente\" + 0.299*\"ratinho\" + -0.229*\"globo\" + -0.225*\"podiar\" + 0.221*\"ficar\" + 0.197*\"apresentador\" + 0.186*\"querer\" + -0.170*\"falar\" + 0.148*\"plateio\".\n",
            "Words in 6: -0.309*\"falar\" + -0.299*\"podiar\" + 0.296*\"globo\" + -0.255*\"presidente\" + -0.183*\"ficar\" + 0.157*\"ator\" + -0.143*\"ratinho\" + -0.133*\"programa\" + -0.120*\"saber\" + 0.119*\"morrer\".\n",
            "Words in 7: 0.303*\"pedir\" + -0.205*\"falar\" + -0.172*\"podiar\" + 0.163*\"homem\" + 0.163*\"capitao\" + 0.158*\"confederar\" + 0.158*\"soldado\" + -0.133*\"presidente\" + 0.123*\"filho\" + 0.112*\"cantor\".\n",
            "Words in 8: 0.226*\"cantor\" + 0.158*\"brasileiro\" + -0.141*\"querer\" + 0.134*\"deixar\" + -0.129*\"vida\" + 0.116*\"show\" + -0.113*\"presidente\" + 0.113*\"viagem\" + -0.111*\"capitao\" + 0.110*\"dinheiro\".\n",
            "Words in 9: 0.173*\"falar\" + 0.167*\"podiar\" + -0.161*\"viagem\" + -0.156*\"ficar\" + 0.156*\"pedir\" + 0.133*\"cantor\" + -0.120*\"pai\" + -0.120*\"levar\" + 0.117*\"mensagem\" + -0.116*\"filho\".\n",
            "Words in 10: -0.293*\"querer\" + -0.202*\"achar\" + -0.192*\"virtude\" + -0.190*\"capacidade\" + -0.137*\"vida\" + 0.122*\"ano\" + 0.122*\"pessoa\" + -0.119*\"sonho\" + -0.112*\"momento\" + 0.111*\"bolsonaro\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Método LSA - APLICAÇÃO NO DATASET DE TECNOLOGIA"
      ],
      "metadata": {
        "id": "LFfQUWOWYOfQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = stopwords.words('portuguese')\n",
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        # deacc=True removes punctuations\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
        "data = tm_tecnologia.values.tolist()\n",
        "data_words = list(sent_to_words(data))\n",
        "print(data_words[:1][0][:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ed21a13-6ed3-4713-fcb3-47388fda130e",
        "id": "rT5syrCeYOfQ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['virus', 'homem', 'teste', 'fidelidade', 'descobre', 'mulher', 'lesbica']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cria os modelos de bigrama e trigrama\n",
        "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # Quanto maior o limite, menos frases.\n",
        "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
        "\n",
        "# Maneira mais rápida de obter uma frase batida como um trigrama/bigrama\n",
        "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "trigram_mod = gensim.models.phrases.Phraser(trigram)"
      ],
      "metadata": {
        "id": "hXftgU7qYOfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removendo as stopwords\n",
        "data_words_nostops = remove_stopwords(data_words)\n",
        "\n",
        "# Formando os bigramas\n",
        "data_words_bigrams = make_bigrams(data_words_nostops)\n",
        "\n",
        "# Inicializando o modelo spacy 'pt', mantendo apenas o componente tagger (para eficiência)\n",
        "nlp = spacy.load(\"pt_core_news_lg\", disable=['parser', 'ner'])\n",
        "\n",
        "# Fazendo lematização mantendo apenas substantivo, adjetivo, verbo e advérbio\n",
        "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
        "\n",
        "print(data_lemmatized[:1][0][:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7951125-7c41-4384-8578-c179bf8ca4c3",
        "id": "15_mmK15YOfR"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['virus', 'homem', 'teste', 'fidelidade', 'descobrir', 'mulher', 'lesbico']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Criando dicionário\n",
        "id2word = corpora.Dictionary(data_lemmatized)\n",
        "\n",
        "# Criando Corpus\n",
        "texts = data_lemmatized\n",
        "\n",
        "# Term Document Frequency\n",
        "corpus = [id2word.doc2bow(text) for text in texts]\n",
        "bow = corpus"
      ],
      "metadata": {
        "id": "cD2409AVYOfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Achando o valor de coerência com diferentes números de tópicos\n",
        "for i in range(4,21):\n",
        "    lsi = LsiModel(bow, num_topics=i, id2word=id2word)\n",
        "    coherence_model = CoherenceModel(model=lsi, texts=texts, dictionary=id2word, coherence='c_v')\n",
        "    coherence_score = coherence_model.get_coherence()\n",
        "    print('Coherence score with {} clusters: {}'.format(i, coherence_score))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a8c6842-7762-4ce0-8b51-ae64a86159be",
        "id": "Ps3GjFHeYOfT"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coherence score with 4 clusters: 0.5643663406857131\n",
            "Coherence score with 5 clusters: 0.47919837328647086\n",
            "Coherence score with 6 clusters: 0.6435346468230966\n",
            "Coherence score with 7 clusters: 0.5609296287818822\n",
            "Coherence score with 8 clusters: 0.6338460075907174\n",
            "Coherence score with 9 clusters: 0.5771555313169994\n",
            "Coherence score with 10 clusters: 0.5318536580266799\n",
            "Coherence score with 11 clusters: 0.47316419548053407\n",
            "Coherence score with 12 clusters: 0.44963546253225584\n",
            "Coherence score with 13 clusters: 0.5356931596664187\n",
            "Coherence score with 14 clusters: 0.49530808754377553\n",
            "Coherence score with 15 clusters: 0.46686346374819876\n",
            "Coherence score with 16 clusters: 0.4428594963445109\n",
            "Coherence score with 17 clusters: 0.4387840482565547\n",
            "Coherence score with 18 clusters: 0.4665403613500561\n",
            "Coherence score with 19 clusters: 0.4309641828689344\n",
            "Coherence score with 20 clusters: 0.4038824897865164\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perfomando SVD na bag of words com o LsiModel para extrair o número ótimo de tópicos\n",
        "lsi = LsiModel(bow, num_topics=6, id2word=id2word)"
      ],
      "metadata": {
        "id": "nKSfs-C1YOfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Achando as 10 palavras com maior associação com os tópicos derivados\n",
        "for topic_num, words in lsi.print_topics(num_words=10):\n",
        "    print('Words in {}: {}.'.format(topic_num, words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c23393ab-2f36-47cc-b735-5c2bc9b243f0",
        "id": "NzaLzwzfYOfT"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words in 0: 0.310*\"mensagem\" + 0.285*\"site\" + 0.255*\"receber\" + 0.215*\"compartilhar\" + 0.213*\"contato\" + 0.212*\"amigo\" + 0.199*\"abaixo\" + 0.184*\"ganhar\" + 0.162*\"presente\" + 0.158*\"novo\".\n",
            "Words in 1: -0.479*\"mensagem\" + 0.291*\"site\" + -0.252*\"contato\" + -0.214*\"enviar\" + 0.197*\"abaixo\" + 0.166*\"compartilhar\" + 0.160*\"ganhar\" + 0.155*\"presente\" + 0.151*\"cadastro\" + -0.118*\"celular\".\n",
            "Words in 2: -0.372*\"cadastro\" + 0.346*\"presente\" + -0.268*\"informar\" + -0.264*\"site\" + -0.251*\"beneficio\" + 0.188*\"premio\" + 0.177*\"abaixo\" + 0.167*\"compartilhar\" + -0.153*\"programa\" + 0.145*\"novo\".\n",
            "Words in 3: -0.419*\"atualizar\" + 0.243*\"presente\" + -0.203*\"bloquear\" + 0.189*\"contato\" + 0.189*\"mensagem\" + -0.157*\"repassar\" + -0.148*\"amigo\" + -0.145*\"deixar\" + -0.137*\"pedir\" + -0.135*\"ficar\".\n",
            "Words in 4: 0.405*\"presente\" + 0.311*\"atualizar\" + -0.221*\"compartilhar\" + 0.196*\"novo\" + -0.193*\"site\" + -0.187*\"mensagem\" + -0.162*\"abaixo\" + 0.161*\"cadastro\" + 0.153*\"premio\" + -0.139*\"retirar\".\n",
            "Words in 5: 0.325*\"atualizar\" + -0.288*\"informacoes\" + -0.220*\"numero\" + -0.214*\"chave\" + 0.186*\"mensagem\" + -0.184*\"hotel\" + -0.162*\"casa\" + 0.157*\"site\" + -0.153*\"magnetico\" + -0.152*\"poder\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Método LSA - APLICAÇÃO NO DATASET DE CIÊNCIA"
      ],
      "metadata": {
        "id": "jflaAkJaYO1Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = stopwords.words('portuguese')\n",
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        # deacc=True removes punctuations\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
        "data = tm_ciencia.values.tolist()\n",
        "data_words = list(sent_to_words(data))\n",
        "print(data_words[:1][0][:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7365dd0-8bdc-4a0f-a032-eb69aef0095e",
        "id": "OuLmyRBwYO1R"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['teoria', 'falsa', 'masaru', 'emoto', 'palavras', 'alterar', 'substancias']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cria os modelos de bigrama e trigrama\n",
        "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # Quanto maior o limite, menos frases.\n",
        "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
        "\n",
        "# Maneira mais rápida de obter uma frase batida como um trigrama/bigrama\n",
        "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "trigram_mod = gensim.models.phrases.Phraser(trigram)"
      ],
      "metadata": {
        "id": "XabgHJN3YO1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removendo as stopwords\n",
        "data_words_nostops = remove_stopwords(data_words)\n",
        "\n",
        "# Formando os bigramas\n",
        "data_words_bigrams = make_bigrams(data_words_nostops)\n",
        "\n",
        "# Inicializando o modelo spacy 'pt', mantendo apenas o componente tagger (para eficiência)\n",
        "nlp = spacy.load(\"pt_core_news_lg\", disable=['parser', 'ner'])\n",
        "\n",
        "# Fazendo lematização mantendo apenas substantivo, adjetivo, verbo e advérbio\n",
        "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
        "\n",
        "print(data_lemmatized[:1][0][:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f955341c-1e75-4321-9168-017e8e5145fd",
        "id": "nM_ptMWzYO1S"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['teoria', 'falso', 'Masaru', 'emoto', 'palavra', 'alterar', 'substancia']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Criando dicionário\n",
        "id2word = corpora.Dictionary(data_lemmatized)\n",
        "\n",
        "# Criando Corpus\n",
        "texts = data_lemmatized\n",
        "\n",
        "# Term Document Frequency\n",
        "corpus = [id2word.doc2bow(text) for text in texts]\n",
        "bow = corpus"
      ],
      "metadata": {
        "id": "0Xuv_7VKYO1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Achando o valor de coerência com diferentes números de tópicos\n",
        "for i in range(4,21):\n",
        "    lsi = LsiModel(bow, num_topics=i, id2word=id2word)\n",
        "    coherence_model = CoherenceModel(model=lsi, texts=texts, dictionary=id2word, coherence='c_v')\n",
        "    coherence_score = coherence_model.get_coherence()\n",
        "    print('Coherence score with {} clusters: {}'.format(i, coherence_score))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6934bd67-0f39-4cc4-a91c-d9ac48d6e90a",
        "id": "um7G-rKdYO1T"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coherence score with 4 clusters: 0.7016859169633893\n",
            "Coherence score with 5 clusters: 0.6611656309758628\n",
            "Coherence score with 6 clusters: 0.786900325945144\n",
            "Coherence score with 7 clusters: 0.7411912482677465\n",
            "Coherence score with 8 clusters: 0.7345264111578247\n",
            "Coherence score with 9 clusters: 0.6771686291311144\n",
            "Coherence score with 10 clusters: 0.6156056335724427\n",
            "Coherence score with 11 clusters: 0.6090114855753326\n",
            "Coherence score with 12 clusters: 0.6290677457029797\n",
            "Coherence score with 13 clusters: 0.6644855918007239\n",
            "Coherence score with 14 clusters: 0.6824607119297613\n",
            "Coherence score with 15 clusters: 0.6548464670434727\n",
            "Coherence score with 16 clusters: 0.6026125182596254\n",
            "Coherence score with 17 clusters: 0.6122118318133627\n",
            "Coherence score with 18 clusters: 0.5906994419218977\n",
            "Coherence score with 19 clusters: 0.615101994829478\n",
            "Coherence score with 20 clusters: 0.6536620908126173\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perfomando SVD na bag of words com o LsiModel para extrair o número ótimo de tópicos\n",
        "lsi = LsiModel(bow, num_topics=6, id2word=id2word)"
      ],
      "metadata": {
        "id": "cs6ZjWTZYO1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Achando as 10 palavras com maior associação com os tópicos derivados\n",
        "for topic_num, words in lsi.print_topics(num_words=10):\n",
        "    print('Words in {}: {}.'.format(topic_num, words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6f2f6c9-6954-48e9-c08d-33449c2f22cb",
        "id": "7unACRvBYO1T"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words in 0: 0.343*\"espelho\" + 0.206*\"dia\" + 0.194*\"insolacao\" + 0.160*\"ficar\" + 0.156*\"casa\" + 0.152*\"poder\" + 0.145*\"dentro\" + 0.142*\"fenomeno\" + 0.136*\"temperatura\" + 0.131*\"perigoso\".\n",
            "Words in 1: 0.534*\"espelho\" + 0.166*\"parte\" + 0.165*\"espaco\" + 0.164*\"refletir\" + 0.164*\"unha\" + -0.154*\"dia\" + -0.146*\"insolacao\" + 0.129*\"mulher\" + 0.126*\"imagem\" + 0.123*\"teste\".\n",
            "Words in 2: 0.478*\"terra\" + 0.297*\"climatico\" + 0.231*\"natural\" + 0.161*\"causar\" + 0.149*\"orbitar\" + 0.149*\"mudanca\" + 0.126*\"hoje\" + 0.126*\"nasa\" + 0.120*\"depender\" + 0.119*\"ano\".\n",
            "Words in 3: -0.422*\"natural\" + 0.257*\"climatico\" + 0.237*\"terra\" + -0.168*\"corona\" + -0.167*\"espalhar\" + -0.166*\"professor\" + -0.163*\"dizer\" + 0.129*\"orbitar\" + 0.129*\"mudanca\" + -0.127*\"afetar\".\n",
            "Words in 4: 0.316*\"pulso\" + 0.316*\"sangue\" + 0.241*\"cientista\" + 0.237*\"frasco\" + 0.168*\"conseguir\" + 0.166*\"morte\" + 0.163*\"soro\" + 0.158*\"fracassar\" + 0.158*\"valvula\" + 0.158*\"vasilha\".\n",
            "Words in 5: -0.203*\"catastrofe\" + -0.173*\"ilha\" + 0.140*\"corona\" + 0.139*\"trabalho\" + 0.139*\"espalhar\" + -0.138*\"deslizamento\" + 0.137*\"professor\" + -0.137*\"metro\" + 0.134*\"dizer\" + -0.125*\"dentro\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wqPWeJ4ur7NU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Método LSA - APLICAÇÃO NO DATASET DE ESPORTE"
      ],
      "metadata": {
        "id": "AyWdUNjwYPMB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = stopwords.words('portuguese')\n",
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        # deacc=True removes punctuations\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
        "data = tm_esporte.values.tolist()\n",
        "data_words = list(sent_to_words(data))\n",
        "print(data_words[:1][0][:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "094b732d-2a8d-4d55-d445-e41c1ea87b65",
        "id": "hYLUxcw7YPMB"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['virus', 'logotipo', 'copa', 'mundo', 'mensagem', 'subliminar']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cria os modelos de bigrama e trigrama\n",
        "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # Quanto maior o limite, menos frases.\n",
        "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
        "\n",
        "# Maneira mais rápida de obter uma frase batida como um trigrama/bigrama\n",
        "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "trigram_mod = gensim.models.phrases.Phraser(trigram)"
      ],
      "metadata": {
        "id": "kDjf3-1nYPMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removendo as stopwords\n",
        "data_words_nostops = remove_stopwords(data_words)\n",
        "\n",
        "# Formando os bigramas\n",
        "data_words_bigrams = make_bigrams(data_words_nostops)\n",
        "\n",
        "# Inicializando o modelo spacy 'pt', mantendo apenas o componente tagger (para eficiência)\n",
        "nlp = spacy.load(\"pt_core_news_lg\", disable=['parser', 'ner'])\n",
        "\n",
        "# Fazendo lematização mantendo apenas substantivo, adjetivo, verbo e advérbio\n",
        "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
        "\n",
        "print(data_lemmatized[:1][0][:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae58b467-0773-4d59-bb75-84f11e4afe09",
        "id": "ygsfbCi8YPMC"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['virus', 'logotipo', 'Copa', 'mundo', 'mensagem', 'subliminar']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Criando dicionário\n",
        "id2word = corpora.Dictionary(data_lemmatized)\n",
        "\n",
        "# Criando Corpus\n",
        "texts = data_lemmatized\n",
        "\n",
        "# Term Document Frequency\n",
        "corpus = [id2word.doc2bow(text) for text in texts]\n",
        "bow = corpus"
      ],
      "metadata": {
        "id": "tPuPtxHCYPMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Achando o valor de coerência com diferentes números de tópicos\n",
        "for i in range(4,21):\n",
        "    lsi = LsiModel(bow, num_topics=i, id2word=id2word)\n",
        "    coherence_model = CoherenceModel(model=lsi, texts=texts, dictionary=id2word, coherence='c_v')\n",
        "    coherence_score = coherence_model.get_coherence()\n",
        "    print('Coherence score with {} clusters: {}'.format(i, coherence_score))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e438ba5c-3e4e-4823-d6fd-0c8de65754c9",
        "id": "S6mbLqOSYPMD"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coherence score with 4 clusters: 0.6926411294178936\n",
            "Coherence score with 5 clusters: 0.5439153705821173\n",
            "Coherence score with 6 clusters: 0.5379491345949124\n",
            "Coherence score with 7 clusters: 0.5044488841617132\n",
            "Coherence score with 8 clusters: 0.5105002127864703\n",
            "Coherence score with 9 clusters: 0.612562662639462\n",
            "Coherence score with 10 clusters: 0.508479674690671\n",
            "Coherence score with 11 clusters: 0.5978102621842779\n",
            "Coherence score with 12 clusters: 0.5572864681383634\n",
            "Coherence score with 13 clusters: 0.5133071106719774\n",
            "Coherence score with 14 clusters: 0.5100816159411895\n",
            "Coherence score with 15 clusters: 0.5958632511992246\n",
            "Coherence score with 16 clusters: 0.5106851336775186\n",
            "Coherence score with 17 clusters: 0.4925257037450193\n",
            "Coherence score with 18 clusters: 0.5454918387552564\n",
            "Coherence score with 19 clusters: 0.4453075259766949\n",
            "Coherence score with 20 clusters: 0.49287418540867955\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perfomando SVD na bag of words com o LsiModel para extrair o número ótimo de tópicos\n",
        "lsi = LsiModel(bow, num_topics=4, id2word=id2word)"
      ],
      "metadata": {
        "id": "twf7NJKCYPMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Achando as 10 palavras com maior associação com os tópicos derivados\n",
        "for topic_num, words in lsi.print_topics(num_words=10):\n",
        "    print('Words in {}: {}.'.format(topic_num, words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "910a39ab-d8a9-4a90-c86a-1648ae42cd63",
        "id": "hDYW4fwwYPME"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words in 0: -0.346*\"jogador\" + -0.230*\"atleta\" + -0.205*\"jogo\" + -0.201*\"flamengo\" + -0.197*\"presidente\" + -0.189*\"querer\" + -0.178*\"falar\" + -0.160*\"brasileiro\" + -0.154*\"ficar\" + -0.151*\"americano\".\n",
            "Words in 1: -0.498*\"jogador\" + 0.229*\"falar\" + 0.187*\"atleta\" + 0.169*\"querer\" + 0.161*\"americano\" + -0.142*\"flamengo\" + -0.137*\"mundo\" + -0.137*\"tecnico\" + 0.124*\"ficar\" + 0.123*\"achar\".\n",
            "Words in 2: 0.321*\"atleta\" + -0.291*\"falar\" + -0.220*\"jogo\" + 0.199*\"americano\" + -0.169*\"terrorista\" + -0.165*\"gente\" + 0.156*\"hotel\" + -0.150*\"casa\" + -0.131*\"morar\" + -0.127*\"mineirao\".\n",
            "Words in 3: 0.257*\"flamengo\" + -0.203*\"presidente\" + -0.198*\"gesto\" + 0.179*\"americano\" + -0.148*\"brasileiro\" + 0.143*\"hotel\" + 0.139*\"jovem\" + -0.136*\"falar\" + 0.125*\"jogar\" + 0.121*\"saber\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Método LSA - APLICAÇÃO NO DATASET DE MUNDO"
      ],
      "metadata": {
        "id": "s67s7WN7YPgo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = stopwords.words('portuguese')\n",
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        # deacc=True removes punctuations\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
        "data = tm_mundo.values.tolist()\n",
        "data_words = list(sent_to_words(data))\n",
        "print(data_words[:1][0][:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4524f26-b127-49dd-b7d6-4622408bb9dd",
        "id": "z-c-7A0OYPgp"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['mentira', 'chineses', 'comem', 'bebes', 'aumentar', 'desempenho']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cria os modelos de bigrama e trigrama\n",
        "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # Quanto maior o limite, menos frases.\n",
        "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
        "\n",
        "# Maneira mais rápida de obter uma frase batida como um trigrama/bigrama\n",
        "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "trigram_mod = gensim.models.phrases.Phraser(trigram)"
      ],
      "metadata": {
        "id": "ECFamcApYPgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removendo as stopwords\n",
        "data_words_nostops = remove_stopwords(data_words)\n",
        "\n",
        "# Formando os bigramas\n",
        "data_words_bigrams = make_bigrams(data_words_nostops)\n",
        "\n",
        "# Inicializando o modelo spacy 'pt', mantendo apenas o componente tagger (para eficiência)\n",
        "nlp = spacy.load(\"pt_core_news_lg\", disable=['parser', 'ner'])\n",
        "\n",
        "# Fazendo lematização mantendo apenas substantivo, adjetivo, verbo e advérbio\n",
        "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
        "\n",
        "print(data_lemmatized[:1][0][:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52b8b3fc-69be-4319-e26e-6755d10a7433",
        "id": "7aiecJCLYPgq"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['mentira', 'chinês', 'comer', 'bebe', 'aumentar', 'desempenho']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Criando dicionário\n",
        "id2word = corpora.Dictionary(data_lemmatized)\n",
        "\n",
        "# Criando Corpus\n",
        "texts = data_lemmatized\n",
        "\n",
        "# Term Document Frequency\n",
        "corpus = [id2word.doc2bow(text) for text in texts]\n",
        "bow = corpus"
      ],
      "metadata": {
        "id": "X4I4i4rLYPgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Achando o valor de coerência com diferentes números de tópicos\n",
        "for i in range(4,21):\n",
        "    lsi = LsiModel(bow, num_topics=i, id2word=id2word)\n",
        "    coherence_model = CoherenceModel(model=lsi, texts=texts, dictionary=id2word, coherence='c_v')\n",
        "    coherence_score = coherence_model.get_coherence()\n",
        "    print('Coherence score with {} clusters: {}'.format(i, coherence_score))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c244f901-deee-4ff4-85d9-0d358709f965",
        "id": "jt51ihn4YPgr"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coherence score with 4 clusters: 0.5302977201330468\n",
            "Coherence score with 5 clusters: 0.44448496718102176\n",
            "Coherence score with 6 clusters: 0.4446914225836162\n",
            "Coherence score with 7 clusters: 0.4242830938919813\n",
            "Coherence score with 8 clusters: 0.3946384456220431\n",
            "Coherence score with 9 clusters: 0.4067097367546177\n",
            "Coherence score with 10 clusters: 0.39065709294480977\n",
            "Coherence score with 11 clusters: 0.47517214395953306\n",
            "Coherence score with 12 clusters: 0.4473155785080758\n",
            "Coherence score with 13 clusters: 0.4616761143658505\n",
            "Coherence score with 14 clusters: 0.4139656808732341\n",
            "Coherence score with 15 clusters: 0.4612763066068053\n",
            "Coherence score with 16 clusters: 0.48788183251148165\n",
            "Coherence score with 17 clusters: 0.5280219867952474\n",
            "Coherence score with 18 clusters: 0.46075469891939075\n",
            "Coherence score with 19 clusters: 0.4469459933103468\n",
            "Coherence score with 20 clusters: 0.5084582806013785\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perfomando SVD na bag of words com o LsiModel para extrair o número ótimo de tópicos\n",
        "lsi = LsiModel(bow, num_topics=4, id2word=id2word)"
      ],
      "metadata": {
        "id": "iAQI6AT1YPgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Achando as 10 palavras com maior associação com os tópicos derivados\n",
        "for topic_num, words in lsi.print_topics(num_words=10):\n",
        "    print('Words in {}: {}.'.format(topic_num, words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "603adf2e-e501-46ad-8799-2ef865ed93d9",
        "id": "eqX4cdfeYPgr"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words in 0: 0.289*\"pessoa\" + 0.246*\"mundo\" + 0.181*\"pai\" + 0.164*\"ano\" + 0.135*\"homem\" + 0.127*\"governo\" + 0.123*\"virus\" + 0.116*\"mulher\" + 0.113*\"apenas\" + 0.107*\"direito\".\n",
            "Words in 1: -0.345*\"mulher\" + 0.247*\"virus\" + -0.213*\"direito\" + -0.199*\"pessoa\" + -0.155*\"considerar\" + 0.155*\"chinês\" + 0.134*\"morcego\" + 0.129*\"riqueza\" + 0.127*\"rato\" + 0.124*\"mundial\".\n",
            "Words in 2: -0.471*\"mulher\" + -0.218*\"direito\" + 0.169*\"pessoa\" + -0.168*\"considerar\" + -0.144*\"especie\" + 0.142*\"pai\" + -0.130*\"veredicto\" + -0.127*\"virus\" + 0.122*\"muculmano\" + -0.111*\"especialista\".\n",
            "Words in 3: 0.526*\"pessoa\" + -0.215*\"muculmano\" + -0.171*\"pai\" + -0.162*\"canada\" + -0.154*\"mulher\" + -0.152*\"prefeito\" + 0.143*\"golfinho\" + -0.133*\"entender\" + -0.118*\"vida\" + -0.109*\"falso\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Método LSA - APLICAÇÃO NO DATASET DE RELIGIÃO"
      ],
      "metadata": {
        "id": "syjSFhZ-YP2Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = stopwords.words('portuguese')\n",
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        # deacc=True removes punctuations\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
        "data = tm_religiao.values.tolist()\n",
        "data_words = list(sent_to_words(data))\n",
        "print(data_words[:1][0][:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42f7df3a-f57d-40e2-ebde-1d5a42d2a6be",
        "id": "W_sBvercYP2R"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['pastor', 'sergio', 'helder', 'chutou', 'santa', 'virou', 'catolico']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cria os modelos de bigrama e trigrama\n",
        "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # Quanto maior o limite, menos frases.\n",
        "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
        "\n",
        "# Maneira mais rápida de obter uma frase batida como um trigrama/bigrama\n",
        "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "trigram_mod = gensim.models.phrases.Phraser(trigram)"
      ],
      "metadata": {
        "id": "65vp1ZA_YP2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removendo as stopwords\n",
        "data_words_nostops = remove_stopwords(data_words)\n",
        "\n",
        "# Formando os bigramas\n",
        "data_words_bigrams = make_bigrams(data_words_nostops)\n",
        "\n",
        "# Inicializando o modelo spacy 'pt', mantendo apenas o componente tagger (para eficiência)\n",
        "nlp = spacy.load(\"pt_core_news_lg\", disable=['parser', 'ner'])\n",
        "\n",
        "# Fazendo lematização mantendo apenas substantivo, adjetivo, verbo e advérbio\n",
        "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
        "\n",
        "print(data_lemmatized[:1][0][:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "941eceab-5540-497a-aa2a-4dbf2cac2faa",
        "id": "vuj2KYsFYP2S"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['pastor', 'chutar', 'virar', 'catolico']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Criando dicionário\n",
        "id2word = corpora.Dictionary(data_lemmatized)\n",
        "\n",
        "# Criando Corpus\n",
        "texts = data_lemmatized\n",
        "\n",
        "# Term Document Frequency\n",
        "corpus = [id2word.doc2bow(text) for text in texts]\n",
        "bow = corpus"
      ],
      "metadata": {
        "id": "ZY-0xpCkYP2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Achando o valor de coerência com diferentes números de tópicos\n",
        "for i in range(4,21):\n",
        "    lsi = LsiModel(bow, num_topics=i, id2word=id2word)\n",
        "    coherence_model = CoherenceModel(model=lsi, texts=texts, dictionary=id2word, coherence='c_v')\n",
        "    coherence_score = coherence_model.get_coherence()\n",
        "    print('Coherence score with {} clusters: {}'.format(i, coherence_score))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6df83ecc-029c-405b-b735-ab108f6c11b4",
        "id": "CdrJBpOaYP2T"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coherence score with 4 clusters: 0.5476353881334544\n",
            "Coherence score with 5 clusters: 0.6038388392219703\n",
            "Coherence score with 6 clusters: 0.521625755765372\n",
            "Coherence score with 7 clusters: 0.5074387433451472\n",
            "Coherence score with 8 clusters: 0.4958741509612396\n",
            "Coherence score with 9 clusters: 0.4368844024347227\n",
            "Coherence score with 10 clusters: 0.49974577985976526\n",
            "Coherence score with 11 clusters: 0.4116575206344999\n",
            "Coherence score with 12 clusters: 0.4438987174527791\n",
            "Coherence score with 13 clusters: 0.4174249799860984\n",
            "Coherence score with 14 clusters: 0.4106505154465569\n",
            "Coherence score with 15 clusters: 0.4447962318518909\n",
            "Coherence score with 16 clusters: 0.4047565894442384\n",
            "Coherence score with 17 clusters: 0.40465878903180624\n",
            "Coherence score with 18 clusters: 0.40826220286784987\n",
            "Coherence score with 19 clusters: 0.36292574088485297\n",
            "Coherence score with 20 clusters: 0.3881659468451935\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perfomando SVD na bag of words com o LsiModel para extrair o número ótimo de tópicos\n",
        "lsi = LsiModel(bow, num_topics=5, id2word=id2word)"
      ],
      "metadata": {
        "id": "W4Qac7ARYP2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Achando as 10 palavras com maior associação com os tópicos derivados\n",
        "for topic_num, words in lsi.print_topics(num_words=10):\n",
        "    print('Words in {}: {}.'.format(topic_num, words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c609564c-9ff7-43e3-8679-1beed7f3a827",
        "id": "FaK9xom1YP2U"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words in 0: 0.256*\"pessoa\" + 0.253*\"falar\" + 0.172*\"querer\" + 0.170*\"deus\" + 0.168*\"igreja\" + 0.157*\"saber\" + 0.156*\"mensagem\" + 0.146*\"senhor\" + 0.143*\"orar\" + 0.131*\"pedir\".\n",
            "Words in 1: -0.398*\"falar\" + -0.300*\"bolsonaro\" + -0.286*\"mudanca\" + -0.180*\"querer\" + -0.174*\"saber\" + -0.154*\"usar\" + 0.153*\"mensagem\" + -0.152*\"deus\" + -0.144*\"enviar\" + 0.124*\"ajudar\".\n",
            "Words in 2: 0.350*\"cama\" + 0.271*\"gazin\" + 0.231*\"terra\" + 0.210*\"comprar\" + -0.210*\"mensagem\" + 0.193*\"saquinho\" + 0.155*\"cemiterio\" + 0.136*\"tirar\" + 0.131*\"video\" + -0.122*\"povo\".\n",
            "Words in 3: 0.220*\"ajudar\" + -0.187*\"irmao\" + -0.180*\"povo\" + 0.173*\"colocar\" + 0.171*\"gente\" + -0.167*\"poder\" + 0.163*\"vida\" + -0.160*\"mensagem\" + -0.156*\"senhor\" + 0.151*\"crianca\".\n",
            "Words in 4: -0.244*\"senhor\" + -0.235*\"igreja\" + 0.200*\"encher\" + 0.198*\"cama\" + 0.181*\"jejum\" + 0.169*\"mensagem\" + -0.160*\"foto\" + 0.153*\"gazin\" + 0.140*\"pessoa\" + -0.138*\"orar\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Método LSA - APLICAÇÃO NO DATASET GERAL"
      ],
      "metadata": {
        "id": "fYbEP1U8YQM6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = stopwords.words('portuguese')\n",
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        # deacc=True removes punctuations\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
        "data = tm_geral.values.tolist()\n",
        "data_words = list(sent_to_words(data))\n",
        "print(data_words[:1][0][:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c2bb6df-9509-4293-e881-2e560b35bff1",
        "id": "xDN_cARBYQM7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['simpsons', 'episodio', 'manifestacoes', 'brasil']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cria os modelos de bigrama e trigrama\n",
        "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # Quanto maior o limite, menos frases.\n",
        "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
        "\n",
        "# Maneira mais rápida de obter uma frase batida como um trigrama/bigrama\n",
        "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "trigram_mod = gensim.models.phrases.Phraser(trigram)"
      ],
      "metadata": {
        "id": "PbDZT5m3YQM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removendo as stopwords\n",
        "data_words_nostops = remove_stopwords(data_words)\n",
        "\n",
        "# Formando os bigramas\n",
        "data_words_bigrams = make_bigrams(data_words_nostops)\n",
        "\n",
        "# Inicializando o modelo spacy 'pt', mantendo apenas o componente tagger (para eficiência)\n",
        "nlp = spacy.load(\"pt_core_news_lg\", disable=['parser', 'ner'])\n",
        "\n",
        "# Fazendo lematização mantendo apenas substantivo, adjetivo, verbo e advérbio\n",
        "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
        "\n",
        "print(data_lemmatized[:1][0][:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17a81011-fb3d-4761-ba87-21e966f97acb",
        "id": "zYIw5qnFYQM8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['simpsons', 'episodio', 'Manifestacoes']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Criando dicionário\n",
        "id2word = corpora.Dictionary(data_lemmatized)\n",
        "\n",
        "# Criando Corpus\n",
        "texts = data_lemmatized\n",
        "\n",
        "# Term Document Frequency\n",
        "corpus = [id2word.doc2bow(text) for text in texts]\n",
        "bow = corpus"
      ],
      "metadata": {
        "id": "iqqDsMrHYQM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Achando o valor de coerência com diferentes números de tópicos\n",
        "for i in range(4,21):\n",
        "    lsi = LsiModel(bow, num_topics=i, id2word=id2word)\n",
        "    coherence_model = CoherenceModel(model=lsi, texts=texts, dictionary=id2word, coherence='c_v')\n",
        "    coherence_score = coherence_model.get_coherence()\n",
        "    print('Coherence score with {} clusters: {}'.format(i, coherence_score))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93e7dc52-52ef-4ef2-d17c-67bfa876e01f",
        "id": "VwieNnKNYQM-"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coherence score with 4 clusters: 0.45916256326687405\n",
            "Coherence score with 5 clusters: 0.4667496906793943\n",
            "Coherence score with 6 clusters: 0.44434570128737577\n",
            "Coherence score with 7 clusters: 0.4555948407139419\n",
            "Coherence score with 8 clusters: 0.4992460775590711\n",
            "Coherence score with 9 clusters: 0.4293939572740871\n",
            "Coherence score with 10 clusters: 0.4654567232964867\n",
            "Coherence score with 11 clusters: 0.4147429902820432\n",
            "Coherence score with 12 clusters: 0.43012300830513434\n",
            "Coherence score with 13 clusters: 0.43299585917401023\n",
            "Coherence score with 14 clusters: 0.4126778388797588\n",
            "Coherence score with 15 clusters: 0.43229668438944646\n",
            "Coherence score with 16 clusters: 0.4106512479286109\n",
            "Coherence score with 17 clusters: 0.406729690972846\n",
            "Coherence score with 18 clusters: 0.4290681593248755\n",
            "Coherence score with 19 clusters: 0.4057014273617947\n",
            "Coherence score with 20 clusters: 0.40327662578207935\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perfomando SVD na bag of words com o LsiModel para extrair o número ótimo de tópicos\n",
        "lsi = LsiModel(bow, num_topics=8, id2word=id2word)"
      ],
      "metadata": {
        "id": "pWdTbPILYQM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Achando as 10 palavras com maior associação com os tópicos derivados\n",
        "for topic_num, words in lsi.print_topics(num_words=10):\n",
        "    print('Words in {}: {}.'.format(topic_num, words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4403def-febf-40df-b556-367cd7bbd1bb",
        "id": "Pqy_tNS7YQM-"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words in 0: 0.294*\"pessoa\" + 0.228*\"falar\" + 0.170*\"querer\" + 0.158*\"saber\" + 0.155*\"gente\" + 0.155*\"ficar\" + 0.138*\"passar\" + 0.131*\"bolsonaro\" + 0.130*\"presidente\" + 0.129*\"ano\".\n",
            "Words in 1: 0.491*\"bolsonaro\" + -0.358*\"pessoa\" + 0.355*\"presidente\" + -0.231*\"falar\" + 0.210*\"governo\" + 0.202*\"brasileiro\" + -0.145*\"tomar\" + -0.135*\"gente\" + -0.126*\"agua\" + -0.113*\"virus\".\n",
            "Words in 2: -0.553*\"falar\" + -0.298*\"bolsonaro\" + 0.201*\"pessoa\" + -0.189*\"gente\" + -0.175*\"tomar\" + 0.169*\"receber\" + 0.161*\"mensagem\" + -0.154*\"querer\" + 0.121*\"ano\" + -0.115*\"presidente\".\n",
            "Words in 3: 0.545*\"pessoa\" + 0.447*\"bolsonaro\" + -0.246*\"ano\" + 0.127*\"virus\" + -0.126*\"querer\" + 0.125*\"presidente\" + -0.124*\"ficar\" + -0.124*\"mulher\" + -0.122*\"filho\" + 0.122*\"mensagem\".\n",
            "Words in 4: 0.369*\"mensagem\" + 0.283*\"bolsonaro\" + -0.228*\"governo\" + -0.219*\"presidente\" + -0.181*\"ano\" + 0.170*\"amigo\" + 0.154*\"contato\" + -0.151*\"brasileiro\" + -0.149*\"vacino\" + 0.138*\"compartilhar\".\n",
            "Words in 5: 0.414*\"falar\" + -0.259*\"bolsonaro\" + 0.225*\"governo\" + 0.219*\"mensagem\" + -0.216*\"ano\" + 0.160*\"receber\" + -0.160*\"morrer\" + -0.158*\"ficar\" + 0.143*\"site\" + -0.136*\"vida\".\n",
            "Words in 6: -0.338*\"querer\" + 0.312*\"ano\" + 0.265*\"bolsonaro\" + 0.207*\"falar\" + -0.195*\"governo\" + -0.192*\"pegar\" + -0.180*\"presidente\" + -0.178*\"gente\" + -0.159*\"pessoa\" + -0.139*\"olhar\".\n",
            "Words in 7: -0.384*\"acabar\" + -0.362*\"ano\" + -0.323*\"pessoa\" + 0.193*\"agua\" + 0.173*\"passar\" + -0.159*\"falar\" + 0.152*\"mensagem\" + -0.143*\"receber\" + -0.131*\"aumento\" + -0.121*\"partido\".\n"
          ]
        }
      ]
    }
  ]
}