{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qyGp0UeAKxer"
   },
   "source": [
    "## Algoritmo para adição da coluna de data no dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Após a execução do tratamento textual é realizado a etapa de aperfeiçoamento dos dados via Web Scraping utilizando o dataset resultante do tratamento textual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Realizando leitura do dataset fakepedia\n",
    "df = pd.read_csv(\"./datasetnew/treated_dataset.csv\", header=0, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "xRw1Eb7YLKZM"
   },
   "outputs": [],
   "source": [
    "#Importando biblioteca para fazer requisições \n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DPqgoX_ILKkC",
    "outputId": "e357eca7-4a06-4fd1-866e-eadb915b0dbc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in e:\\anaconda\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in e:\\anaconda\\lib\\site-packages (from bs4) (4.9.3)\n",
      "Requirement already satisfied: soupsieve>1.2; python_version >= \"3.0\" in e:\\anaconda\\lib\\site-packages (from beautifulsoup4->bs4) (1.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "JxzXRRtILKt8"
   },
   "outputs": [],
   "source": [
    "#Importando bs4\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D5Ml8_8YLK3a"
   },
   "outputs": [],
   "source": [
    "#Criando coluna que vai receber as datas das notícias\n",
    "df['datetime'] = pd.Series() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Resetando os índices do dataset\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOVA SOLUÇÃO FUNCIONAL REQUEST DATA\n",
    "import urllib\n",
    "from urllib.error import HTTPError\n",
    "for i in range(0, len(df)):\n",
    "    try:\n",
    "        req = urllib.request.Request(df['url_review'][i])\n",
    "        req.add_header('User-Agent', 'Mozilla/5.0 (Platform; Security; OS-or-CPU; Localization; rv:1.4) Gecko/20030624 Netscape/7.1 (ax)')\n",
    "        response = urllib.request.urlopen(req)\n",
    "        if response.getcode()==200:\n",
    "            try:\n",
    "                data = response.read()    \n",
    "            except (http.client.IncompleteRead) as e:\n",
    "                data = e.partial\n",
    "            site = BeautifulSoup(data, 'html.parser')\n",
    "            data = site.find('time', attrs={'class':'entry-date published'})\n",
    "            if(data is None):\n",
    "                data = site.find('time', attrs={'class':'entry-date published updated'})\n",
    "                if(data is None):\n",
    "                    print(i)\n",
    "                    print(df['message'][i])\n",
    "                else:\n",
    "                    df['datetime'][i] = data.get('datetime')  \n",
    "            else:\n",
    "                print(i)\n",
    "                df['datetime'][i] = data.get('datetime')\n",
    "        else:\n",
    "            print(response.getcode())\n",
    "            print(i)\n",
    "            df['datetime'][i] = 'URL indisponível'\n",
    "    except HTTPError as e:\n",
    "        data = e.read()\n",
    "        site = BeautifulSoup(data, 'html.parser')\n",
    "        data = site.find('time', attrs={'class':'entry-date published'})\n",
    "        if(data is None):\n",
    "            data = site.find('time', attrs={'class':'entry-date published updated'})\n",
    "            if(data is None):\n",
    "                print(i)\n",
    "                print(df['message'][i])\n",
    "            else:\n",
    "                df['datetime'][i] = data.get('datetime')  \n",
    "        else:\n",
    "            print(i)\n",
    "            df['datetime'][i] = data.get('datetime')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparação final arquivo csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verificando tamanho do dataset\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SALVANDO CSV FINAL DO TRATAMENTO\n",
    "#df.to_csv(\"./datasetnew/treated_dataset2.csv\", sep=';')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Tratamento_Inicial.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
